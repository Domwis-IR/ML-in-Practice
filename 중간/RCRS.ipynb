{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RCRS","provenance":[{"file_id":"1gQCfIS5M0cYmQi14ZAxSfmyEDdTUChZr","timestamp":1620662345141},{"file_id":"1NBZijnKI-JSQWMRa_ie6TIGgiCotY-oF","timestamp":1620412557373},{"file_id":"18b80wpeQD1Wj6NjwZFFhZGDuMWWW1TIZ","timestamp":1619286623952},{"file_id":"1pSPNcOLDPSKONSeGALl1Jplu9ET9z-1q","timestamp":1619081668342}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"bb1f7df8930247429af5a006940f6519":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_29b9a6e7942a4b49bf1cc26bbfe55a94","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_bba7942cee18473d940616418ce2e727","IPY_MODEL_33bbc9c960054eadb53d3b6b24900073"]}},"29b9a6e7942a4b49bf1cc26bbfe55a94":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bba7942cee18473d940616418ce2e727":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_543742a2e83a4de1a21733546eed45a2","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":169001437,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":169001437,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3a1ce876fde948caad275f1cfd0633d3"}},"33bbc9c960054eadb53d3b6b24900073":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_875d6f446cf84b4da27cc5f665d81d56","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 169001984/? [00:06&lt;00:00, 26530953.97it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_af4164ff4e57438091dee6622453251b"}},"543742a2e83a4de1a21733546eed45a2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"3a1ce876fde948caad275f1cfd0633d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"875d6f446cf84b4da27cc5f665d81d56":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"af4164ff4e57438091dee6622453251b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"OSFGYaIDG6f0"},"source":["Cutout Data Augmentation.\n","\n","This code is implmented by following the official code (https://github.com/uoguelph-mlrg/Cutout)\n"]},{"cell_type":"markdown","metadata":{"id":"vCVSE5-UboYl"},"source":["##**Import all neceassary packages**"]},{"cell_type":"code","metadata":{"id":"5YBMwPsubsbX","executionInfo":{"status":"ok","timestamp":1620938852335,"user_tz":-540,"elapsed":955,"user":{"displayName":"류지혜","photoUrl":"","userId":"05844261191654838045"}}},"source":["import numpy as np\n","import time\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","import torch.backends.cudnn as cudnn\n","from torch.optim.lr_scheduler import MultiStepLR\n","\n","from torchvision import datasets, transforms\n","\n","from tqdm.notebook import tqdm as tqdm"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L88afYXKMSdL"},"source":["##**Model - Define ResNet Model**\n"]},{"cell_type":"code","metadata":{"id":"eMFSLTnkMQdq","executionInfo":{"status":"ok","timestamp":1620938854787,"user_tz":-540,"elapsed":921,"user":{"displayName":"류지혜","photoUrl":"","userId":"05844261191654838045"}}},"source":["'''ResNet18/34/50/101/152 in Pytorch.'''\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = conv3x3(3,64)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = F.avg_pool2d(out, 4)\n","        out = out.view(out.size(0), -1)\n","        out = self.linear(out)\n","        return out\n","\n","\n","def ResNet18(num_classes=10):\n","    return ResNet(BasicBlock, [2,2,2,2], num_classes)\n","\n","def ResNet34(num_classes=10):\n","    return ResNet(BasicBlock, [3,4,6,3], num_classes)\n","\n","def ResNet50(num_classes=10):\n","    return ResNet(Bottleneck, [3,4,6,3], num_classes)\n","\n","def ResNet101(num_classes=10):\n","    return ResNet(Bottleneck, [3,4,23,3], num_classes)\n","\n","def ResNet152(num_classes=10):\n","    return ResNet(Bottleneck, [3,8,36,3], num_classes)\n","\n","def test_resnet():\n","    net = ResNet50()\n","    y = net(Variable(torch.randn(1,3,32,32)))\n","    print(y.size())\n","\n","# test_resnet()"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qjM3cl279Lvg"},"source":["##**Utils**"]},{"cell_type":"code","metadata":{"id":"gIvuSgE49Kvu","executionInfo":{"status":"ok","timestamp":1620938855267,"user_tz":-540,"elapsed":776,"user":{"displayName":"류지혜","photoUrl":"","userId":"05844261191654838045"}}},"source":["class AverageMeter(object):\n","    r\"\"\"Computes and stores the average and current value\n","    \"\"\"\n","    def __init__(self, name, fmt=':f'):\n","        self.name = name\n","        self.fmt = fmt\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","    def __str__(self):\n","        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n","        return fmtstr.format(**self.__dict__)\n","\n","\n","class ProgressMeter(object):\n","    def __init__(self, num_batches, *meters, prefix=\"\"):\n","        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n","        self.meters = meters\n","        self.prefix = prefix\n","\n","    def print(self, batch):\n","        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n","        entries += [str(meter) for meter in self.meters]\n","        print('\\t'.join(entries))\n","\n","    def _get_batch_fmtstr(self, num_batches):\n","        num_digits = len(str(num_batches // 1))\n","        fmt = '{:' + str(num_digits) + 'd}'\n","        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n","\n","\n","def accuracy(output, target, topk=(1,)):\n","    r\"\"\"Computes the accuracy over the $k$ top predictions for the specified values of k\n","    \"\"\"\n","    with torch.no_grad():\n","        maxk = max(topk)\n","        batch_size = target.size(0)\n","\n","        # _, pred = output.topk(maxk, 1, True, True)\n","        # pred = pred.t()\n","        # correct = pred.eq(target.view(1, -1).expand_as(pred))\n","\n","        # faster topk (ref: https://github.com/pytorch/pytorch/issues/22812)\n","        _, idx = output.sort(descending=True)\n","        pred = idx[:,:maxk]\n","        pred = pred.t()\n","        correct = pred.eq(target.view(1, -1).expand_as(pred))\n","\n","        res = []\n","        for k in topk:\n","            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n","            res.append(correct_k.mul_(100.0 / batch_size))\n","        return res"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o6y3zhSfMbdC"},"source":["##**Cutout: Main Code for Applying Cutout data augmentation**"]},{"cell_type":"code","metadata":{"id":"iMQI2K4AMopg","executionInfo":{"status":"ok","timestamp":1620938856064,"user_tz":-540,"elapsed":1021,"user":{"displayName":"류지혜","photoUrl":"","userId":"05844261191654838045"}}},"source":["class Cutout(object):\n","    \"\"\"Randomly mask out one or more patches from an image.\n","\n","    Args:\n","        n_holes (int): Number of patches to cut out of each image.\n","        length (int): The length (in pixels) of each square patch.\n","    \"\"\"\n","    def __init__(self, n_holes, length):\n","        self.n_holes = n_holes\n","        self.length = length\n","\n","    def __call__(self, img):\n","        \"\"\"\n","        Args:\n","            img (Tensor): Tensor image of size (C, H, W).\n","        Returns:\n","            Tensor: Image with n_holes of dimension length x length cut out of it.\n","        \"\"\"\n","        h = img.size(1)\n","        w = img.size(2)\n","\n","        mask = np.ones((h, w), np.float32)\n","\n","        for n in range(self.n_holes):\n","            y = np.random.randint(h)\n","            x = np.random.randint(w)\n","\n","            y1 = np.clip(y - self.length // 2, 0, h)\n","            y2 = np.clip(y + self.length // 2, 0, h)\n","            x1 = np.clip(x - self.length // 2, 0, w)\n","            x2 = np.clip(x + self.length // 2, 0, w)\n","\n","            mask[y1: y2, x1: x2] = 0.\n","\n","        mask = torch.from_numpy(mask)\n","        mask = mask.expand_as(img)\n","        img = img * mask\n","\n","        return img"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o1zGEvEi9W_g"},"source":["##**RCRS(Random Color Random Shadows)**"]},{"cell_type":"code","metadata":{"id":"kElVGH4g9WoB","executionInfo":{"status":"ok","timestamp":1620938857456,"user_tz":-540,"elapsed":1663,"user":{"displayName":"류지혜","photoUrl":"","userId":"05844261191654838045"}}},"source":["import random\n","import numpy as np\n","import cv2\n","from PIL import Image, ImageChops\n","import torchvision.transforms.functional as TF\n","\n","class RandomShadows(object):\n","    def __init__(self, p=0.5, high_ratio=(1,2), low_ratio=(0.01, 0.5), left_low_ratio=(0.4,0.6), \\\n","    left_high_ratio=(0,0.2), right_low_ratio=(0.4,0.6), right_high_ratio = (0,0.2)):\n","        self.p = p\n","        self.high_ratio = high_ratio\n","        self.low_ratio = low_ratio\n","        self.left_low_ratio = left_low_ratio\n","        self.left_high_ratio = left_high_ratio\n","        self.right_low_ratio = right_low_ratio\n","        self.right_high_ratio = right_high_ratio\n","\n","    @staticmethod\n","    def process(img, high_ratio, low_ratio, left_low_ratio, left_high_ratio, \\\n","            right_low_ratio, right_high_ratio):\n","\n","        w, h = img.size\n","        high_bright_factor = random.uniform(high_ratio[0], high_ratio[1])\n","        low_bright_factor = random.uniform(low_ratio[0], low_ratio[1])\n","\n","        left_low_factor = random.uniform(left_low_ratio[0]*h, left_low_ratio[1]*h)\n","        left_high_factor = random.uniform(left_high_ratio[0]*h, left_high_ratio[1]*h)\n","        right_low_factor = random.uniform(right_low_ratio[0]*h, right_low_ratio[1]*h)\n","        right_high_factor = random.uniform(right_high_ratio[0]*h, right_high_ratio[1]*h)\n","\n","        tl = (0, left_high_factor)\n","        bl = (0, left_high_factor+left_low_factor)\n","\n","        tr = (w, right_high_factor)\n","        br = (w, right_high_factor+right_low_factor)\n","\n","        contour = np.array([tl, tr, br, bl], dtype=np.int32)\n","\n","        mask = np.zeros([h, w, 3],np.uint8)\n","        # 변형된 부분: mask 칠하는 색을 랜덤하게 준다.\n","        cv2.fillPoly(mask,[contour],(random.randint(0,255),random.randint(0,255),random.randint(0,255)))\n","\n","        inverted_mask = cv2.bitwise_not(mask)\n","        mask_pil = Image.fromarray(mask)\n","        inverted_mask_pil = Image.fromarray(inverted_mask)\n","\n","        low_brightness = TF.adjust_brightness(img, low_bright_factor)\n","        low_brightness_masked = ImageChops.multiply(low_brightness, mask_pil)\n","        high_brightness = TF.adjust_brightness(img, high_bright_factor)\n","        high_brightness_masked = ImageChops.multiply(high_brightness, inverted_mask_pil)\n","\n","        return ImageChops.add(low_brightness_masked, high_brightness_masked)\n","\n","    def __call__(self, img):\n","        if random.uniform(0, 1) < self.p:\n","            img = self.process(img, self.high_ratio, self.low_ratio, \\\n","            self.left_low_ratio, self.left_high_ratio, self.right_low_ratio, \\\n","            self.right_high_ratio)\n","            return img\n","        else:\n","            return img\n"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mn0_Uk9FfqNv"},"source":["** random shadow 논문에 포함된 추가적인 함수 **\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"smUnhknvfplB","executionInfo":{"status":"ok","timestamp":1620938857952,"user_tz":-540,"elapsed":1093,"user":{"displayName":"류지혜","photoUrl":"","userId":"05844261191654838045"}}},"source":["import operator\n","import numpy as np\n","import cv2\n","import random\n","from PIL import Image\n","\n","class DiskAugmenter(object):\n","    def __init__(self, local_mask=(120, 160), global_mask=(40, 80),\n","                 flip_and_noise=False, augmenting_prob=0.67):\n","\n","        self.augmenting_prob = augmenting_prob\n","        self.local_mask = local_mask\n","        self.global_mask = global_mask\n","        self.flip_and_noise = flip_and_noise\n","        self.augment_illumination = any(x > 0 for x in list(local_mask) + list(global_mask))\n","\n","    def __call__(self, img):\n","        if random.uniform(0, 1) < self.augmenting_prob:\n","            img = illumination_augmenter(img, self.global_mask, self.local_mask)\n","            return img\n","        else:\n","            return img"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"iUkFQM-VKEXn","executionInfo":{"status":"ok","timestamp":1620938857952,"user_tz":-540,"elapsed":680,"user":{"displayName":"류지혜","photoUrl":"","userId":"05844261191654838045"}}},"source":["import random\n","import torchvision.transforms.functional as TF\n","\n","class RandomGamma(object):\n","    def __init__(self, gamma_p = 0.5, gamma_ratio=(0,1.5)):\n","        self.gamma_p = gamma_p\n","        self.gamma_ratio = gamma_ratio\n","\n","    def __call__(self,img):\n","        if random.uniform(0, 1) < self.gamma_p:\n","            gamma = random.uniform(self.gamma_ratio[0], self.gamma_ratio[1])\n","            img = TF.adjust_gamma(img, gamma, gain=1)\n","            return img\n","        else:\n","            return img\n","\n","class RandomColorJitter(object):\n","    def __init__(self, p = 0.5, brightness_ratio=(0,2), contrast_ratio=(0,2), \\\n","                saturation_ratio=(0,2), hue_ratio=(-0.5,0.5)):\n","        self.p = p\n","        self.brightness_ratio = brightness_ratio\n","        self.contrast_ratio = contrast_ratio\n","        self.saturation_ratio = saturation_ratio\n","        self.hue_ratio = hue_ratio\n","\n","    @staticmethod\n","    def process(img, brightness_ratio, contrast_ratio, saturation_ratio, hue_ratio):\n","        brightness = random.uniform(brightness_ratio[0], brightness_ratio[1])\n","        contrast = random.uniform(contrast_ratio[0], contrast_ratio[1])\n","        saturation = random.uniform(saturation_ratio[0], saturation_ratio[1])\n","        hue = random.uniform(hue_ratio[0], hue_ratio[1])\n","\n","        img = TF.adjust_brightness(img, brightness)\n","        img = TF.adjust_contrast(img, contrast)\n","        img = TF.adjust_saturation(img, saturation)\n","        img = TF.adjust_hue(img, hue)\n","\n","        return img\n","\n","    def __call__(self,img):\n","        if random.uniform(0, 1) < self.p:\n","            img = self.process(img, self.brightness_ratio, self.contrast_ratio, \\\n","                                self.saturation_ratio, self.hue_ratio)\n","            return img\n","        else:\n","            return img"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9s8oXpzdMvol"},"source":["##**Parameter Settings**"]},{"cell_type":"code","metadata":{"id":"Pjeqawi9cNK6","executionInfo":{"status":"ok","timestamp":1620938872975,"user_tz":-540,"elapsed":1514,"user":{"displayName":"류지혜","photoUrl":"","userId":"05844261191654838045"}}},"source":["dataset = 'cifar100' # cifar10 or cifar100\n","model = 'resnet34' # resnet18, resnet50, resnet101\n","batch_size = 128  # Input batch size for training (default: 128)\n","epochs = 150 # Number of epochs to train (default: 200)\n","learning_rate = 0.1 # Learning rate\n","data_augmentation = True # Traditional data augmentation such as augmantation by flipping and cropping?\n","cutout = True  # Apply Cutout?\n","n_holes = 1 # Number of holes to cut out from image\n","length = 16 # Length of the holes\n","seed = 0 # Random seed (default: 0)\n","print_freq = 30\n","cuda = torch.cuda.is_available()\n","cudnn.benchmark = True  # Should make training should go faster for large models\n","\n","# What we need for our data augmentation\n","randomshadows = True\n","\n","torch.manual_seed(seed)\n","if cuda:\n","    torch.cuda.manual_seed(seed)\n","\n","test_id = dataset + '_' + model"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eXL_PBj6cVoe"},"source":["##**Load and preprocess data**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":313,"referenced_widgets":["bb1f7df8930247429af5a006940f6519","29b9a6e7942a4b49bf1cc26bbfe55a94","bba7942cee18473d940616418ce2e727","33bbc9c960054eadb53d3b6b24900073","543742a2e83a4de1a21733546eed45a2","3a1ce876fde948caad275f1cfd0633d3","875d6f446cf84b4da27cc5f665d81d56","af4164ff4e57438091dee6622453251b"]},"id":"dvQjH3T9caYs","executionInfo":{"status":"ok","timestamp":1620938916718,"user_tz":-540,"elapsed":8190,"user":{"displayName":"류지혜","photoUrl":"","userId":"05844261191654838045"}},"outputId":"0cf4fee5-4259-4991-b060-cb27247faf97"},"source":["# Image Preprocessing\n","normalize = transforms.Normalize(mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n","                                     std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n","\n","train_transform = transforms.Compose([])\n","if data_augmentation:\n","    train_transform.transforms.append(transforms.RandomCrop(32, padding=4))\n","    train_transform.transforms.append(transforms.RandomHorizontalFlip())\n","    \n","if randomshadows:\n","    p = np.round(np.arange(0, 1.1, 0.1), 2)\n","    for i_p in p:\n","        print('RSH p value: ', i_p)\n","        data_transforms = {\n","            'train': transforms.Compose([\n","                # For CIFAR-10 and CIFAR100, either change the model or resize images to 64x64 (uncomment the transform below)\n","                # transforms.Resize(64),\n","                DiskAugmenter(local_mask=(120, 160), global_mask=(40, 80), augmenting_prob=0),\n","                RandomShadows(p=i_p, high_ratio=(1,2), low_ratio=(0,1), \\\n","                left_low_ratio=(0.4,0.8), left_high_ratio=(0,0.3), right_low_ratio=(0.4,0.8),\n","                right_high_ratio = (0,0.3)), ## high means from top of image, low means from top to bottom low\n","                #RandomGamma(gamma_p = 0, gamma_ratio=(0, 1.5)),\n","                #RandomColorJitter(p = 0, brightness_ratio=(0,2), contrast_ratio=(0,2), \\\n","                #           saturation_ratio=(0,2), hue_ratio=(-0.5,0.5)),\n","                \n","                #transforms.ToTensor(),\n","                #transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","                # transforms.RandomErasing(p=i_p)\n","            ]),\n","            'val': transforms.Compose([\n","                # For CIFAR-10 and CIFAR100, either change the model or resize images to 64x64 (uncomment the transform below)\n","                # transforms.Resize(64),\n","                RandomShadows(p=1, high_ratio=(1,2), low_ratio=(0,1), \\\n","                left_low_ratio=(0.4,0.8), left_high_ratio=(0,0.3), right_low_ratio=(0.4,0.8),\n","                right_high_ratio = (0,0.3)), ## high means from top of image, low means from top to bottom low\n","                #transforms.ToTensor(),\n","                #transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","            ]),\n","            'test': transforms.Compose([\n","                #transforms.ToTensor(),\n","            ])\n","        }\n","\n","train_transform.transforms.append(transforms.ToTensor())\n","train_transform.transforms.append(normalize)\n","\n","if cutout:\n","    train_transform.transforms.append(Cutout(n_holes=n_holes, length=length))\n","\n","\n","test_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    normalize])\n","\n","if dataset == 'cifar10':\n","    num_classes = 10\n","    train_dataset = datasets.CIFAR10(root='data/',\n","                                     train=True,\n","                                     transform=train_transform,\n","                                     download=True)\n","\n","    test_dataset = datasets.CIFAR10(root='data/',\n","                                    train=False,\n","                                    transform=test_transform,\n","                                    download=True)\n","elif dataset == 'cifar100':\n","    num_classes = 100\n","    train_dataset = datasets.CIFAR100(root='data/',\n","                                      train=True,\n","                                      transform=train_transform,\n","                                      download=True)\n","\n","    test_dataset = datasets.CIFAR100(root='data/',\n","                                     train=False,\n","                                     transform=test_transform,\n","                                     download=True)\n","\n","\n","# Data Loader (Input Pipeline)\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                           batch_size=batch_size,\n","                                           shuffle=True,\n","                                           pin_memory=True,\n","                                           num_workers=2)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n","                                          batch_size=batch_size,\n","                                          shuffle=False,\n","                                          pin_memory=True,\n","                                          num_workers=2)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["RSH p value:  0.0\n","RSH p value:  0.1\n","RSH p value:  0.2\n","RSH p value:  0.3\n","RSH p value:  0.4\n","RSH p value:  0.5\n","RSH p value:  0.6\n","RSH p value:  0.7\n","RSH p value:  0.8\n","RSH p value:  0.9\n","RSH p value:  1.0\n","Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to data/cifar-100-python.tar.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bb1f7df8930247429af5a006940f6519","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=169001437.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Extracting data/cifar-100-python.tar.gz to data/\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gITLQIAr9lAZ"},"source":["##**Main Training**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s0-lYvAp9oHA","executionInfo":{"status":"ok","timestamp":1620944774865,"user_tz":-540,"elapsed":5859942,"user":{"displayName":"류지혜","photoUrl":"","userId":"05844261191654838045"}},"outputId":"4cc0bb8a-4b4b-469a-8165-de50d2e521d7"},"source":["def train(train_loader, epoch, model, optimizer, criterion):\n","    batch_time = AverageMeter('Time', ':6.3f')\n","    losses = AverageMeter('Loss', ':.4e')\n","    top1 = AverageMeter('Acc@1', ':6.2f')\n","    top5 = AverageMeter('Acc@5', ':6.2f')\n","    progress = ProgressMeter(len(train_loader), batch_time, losses,\n","                             top1, top5, prefix=\"Epoch: [{}]\".format(epoch))\n","    # switch to train mode\n","    model.train()\n","\n","    end = time.time()\n","    for i, (input, target) in enumerate(train_loader):\n","        # measure data loading time\n","        input = input.cuda()\n","        target = target.cuda()\n","\n","        # compute output\n","        output = model(input)\n","        loss = criterion(output, target)\n","\n","        # measure accuracy and record loss, accuracy \n","        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n","        losses.update(loss.item(), input.size(0))\n","        top1.update(acc1[0].item(), input.size(0))\n","        top5.update(acc5[0].item(), input.size(0))\n","\n","        # compute gradient and do SGD step\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # measure elapsed time\n","        batch_time.update(time.time() - end)\n","        end = time.time()\n","\n","        if i % print_freq == 0:\n","            progress.print(i)\n","\n","    print('==> Train Accuracy: Acc@1 {top1.avg:.3f} || Acc@5 {top5.avg:.3f}'.format(top1=top1, top5=top5))\n","    return top1.avg\n","\n","def test(test_loader,epoch, model):\n","    top1 = AverageMeter('Acc@1', ':6.2f')\n","    top5 = AverageMeter('Acc@5', ':6.2f')\n","    model.eval()\n","    for i,(input,target) in enumerate(test_loader):\n","        input = input.cuda()\n","        target = target.cuda()\n","\n","        output = model(input)\n","        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n","        top1.update(acc1[0].item(), input.size(0))\n","        top5.update(acc5[0].item(), input.size(0))\n","    print('==> Test Accuracy:  Acc@1 {top1.avg:.3f} || Acc@5 {top5.avg:.3f}'.format(top1=top1, top5=top5))\n","    return top1.avg\n","\n","model = ResNet34(num_classes=num_classes).cuda()\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate,momentum=0.9, nesterov=True, weight_decay=5e-4)\n","\n","scheduler = MultiStepLR(optimizer, milestones=[60, 90, 120], gamma=0.2)\n","\n","criterion = torch.nn.CrossEntropyLoss().cuda()\n","###########################################################\n","best_acc = 0\n","for epoch in range(epochs):\n","    print(\"\\n----- epoch: {}, lr: {} -----\".format(\n","        epoch, optimizer.param_groups[0][\"lr\"]))\n","\n","    # train for one epoch\n","    start_time = time.time()\n","    train(train_loader, epoch, model, optimizer, criterion)\n","    test_acc = test(test_loader,epoch,model)\n","\n","    elapsed_time = time.time() - start_time\n","    print('==> {:.2f} seconds to train this epoch\\n'.format(elapsed_time))\n","    # learning rate scheduling\n","    scheduler.step()\n","    \n","    # Save model for best accuracy\n","    if best_acc < test_acc:\n","        best_acc = test_acc\n","        torch.save(model.state_dict(), 'model_best.pt')\n","\n","torch.save(model.state_dict(),'model_latest.pt')\n","print(f\"Best Top-1 Accuracy: {best_acc}\")"],"execution_count":17,"outputs":[{"output_type":"stream","text":["\n","----- epoch: 0, lr: 0.1 -----\n","Epoch: [0][  0/391]\tTime  1.046 ( 1.046)\tLoss 4.7496e+00 (4.7496e+00)\tAcc@1   2.34 (  2.34)\tAcc@5   6.25 (  6.25)\n","Epoch: [0][ 30/391]\tTime  0.092 ( 0.122)\tLoss 4.7248e+00 (5.4561e+00)\tAcc@1   1.56 (  0.98)\tAcc@5   4.69 (  4.74)\n","Epoch: [0][ 60/391]\tTime  0.088 ( 0.107)\tLoss 4.5975e+00 (5.0775e+00)\tAcc@1   0.78 (  1.04)\tAcc@5   6.25 (  4.78)\n","Epoch: [0][ 90/391]\tTime  0.095 ( 0.103)\tLoss 4.6052e+00 (4.9242e+00)\tAcc@1   1.56 (  1.18)\tAcc@5   5.47 (  5.31)\n","Epoch: [0][120/391]\tTime  0.091 ( 0.100)\tLoss 4.4923e+00 (4.8221e+00)\tAcc@1   3.12 (  1.50)\tAcc@5  12.50 (  6.33)\n","Epoch: [0][150/391]\tTime  0.096 ( 0.099)\tLoss 4.3540e+00 (4.7393e+00)\tAcc@1   3.12 (  1.80)\tAcc@5  12.50 (  7.63)\n","Epoch: [0][180/391]\tTime  0.095 ( 0.098)\tLoss 4.3175e+00 (4.6685e+00)\tAcc@1   1.56 (  2.05)\tAcc@5  10.94 (  8.80)\n","Epoch: [0][210/391]\tTime  0.085 ( 0.097)\tLoss 4.2360e+00 (4.6098e+00)\tAcc@1   4.69 (  2.38)\tAcc@5  16.41 ( 10.01)\n","Epoch: [0][240/391]\tTime  0.091 ( 0.096)\tLoss 4.2740e+00 (4.5577e+00)\tAcc@1   3.12 (  2.68)\tAcc@5  19.53 ( 11.22)\n","Epoch: [0][270/391]\tTime  0.091 ( 0.096)\tLoss 4.1565e+00 (4.5109e+00)\tAcc@1   7.03 (  2.95)\tAcc@5  14.84 ( 12.32)\n","Epoch: [0][300/391]\tTime  0.092 ( 0.096)\tLoss 4.0413e+00 (4.4701e+00)\tAcc@1   7.81 (  3.24)\tAcc@5  28.12 ( 13.31)\n","Epoch: [0][330/391]\tTime  0.087 ( 0.095)\tLoss 4.1336e+00 (4.4356e+00)\tAcc@1   4.69 (  3.53)\tAcc@5  13.28 ( 14.10)\n","Epoch: [0][360/391]\tTime  0.083 ( 0.095)\tLoss 4.0679e+00 (4.4034e+00)\tAcc@1   5.47 (  3.79)\tAcc@5  22.66 ( 14.92)\n","Epoch: [0][390/391]\tTime  0.536 ( 0.096)\tLoss 4.0147e+00 (4.3754e+00)\tAcc@1  10.00 (  4.03)\tAcc@5  26.25 ( 15.72)\n","==> Train Accuracy: Acc@1 4.026 || Acc@5 15.720\n","==> Test Accuracy:  Acc@1 8.390 || Acc@5 28.170\n","==> 40.34 seconds to train this epoch\n","\n","\n","----- epoch: 1, lr: 0.1 -----\n","Epoch: [1][  0/391]\tTime  0.261 ( 0.261)\tLoss 4.0730e+00 (4.0730e+00)\tAcc@1   5.47 (  5.47)\tAcc@5  21.09 ( 21.09)\n","Epoch: [1][ 30/391]\tTime  0.092 ( 0.099)\tLoss 3.9185e+00 (3.9553e+00)\tAcc@1   7.81 (  7.96)\tAcc@5  24.22 ( 27.72)\n","Epoch: [1][ 60/391]\tTime  0.096 ( 0.096)\tLoss 3.9181e+00 (3.9267e+00)\tAcc@1   4.69 (  8.16)\tAcc@5  26.56 ( 28.04)\n","Epoch: [1][ 90/391]\tTime  0.103 ( 0.095)\tLoss 3.8553e+00 (3.9149e+00)\tAcc@1  14.06 (  8.50)\tAcc@5  32.81 ( 28.25)\n","Epoch: [1][120/391]\tTime  0.088 ( 0.094)\tLoss 3.8871e+00 (3.8980e+00)\tAcc@1   7.81 (  8.76)\tAcc@5  28.91 ( 28.96)\n","Epoch: [1][150/391]\tTime  0.099 ( 0.094)\tLoss 3.7689e+00 (3.8860e+00)\tAcc@1  10.16 (  8.94)\tAcc@5  34.38 ( 29.20)\n","Epoch: [1][180/391]\tTime  0.090 ( 0.094)\tLoss 3.8450e+00 (3.8792e+00)\tAcc@1  10.94 (  8.96)\tAcc@5  33.59 ( 29.49)\n","Epoch: [1][210/391]\tTime  0.092 ( 0.093)\tLoss 3.6723e+00 (3.8642e+00)\tAcc@1   9.38 (  9.18)\tAcc@5  32.03 ( 30.02)\n","Epoch: [1][240/391]\tTime  0.093 ( 0.093)\tLoss 3.6776e+00 (3.8524e+00)\tAcc@1  12.50 (  9.38)\tAcc@5  35.16 ( 30.44)\n","Epoch: [1][270/391]\tTime  0.093 ( 0.093)\tLoss 3.8056e+00 (3.8365e+00)\tAcc@1  10.16 (  9.57)\tAcc@5  32.81 ( 30.92)\n","Epoch: [1][300/391]\tTime  0.092 ( 0.093)\tLoss 3.6862e+00 (3.8244e+00)\tAcc@1  10.16 (  9.78)\tAcc@5  32.81 ( 31.22)\n","Epoch: [1][330/391]\tTime  0.089 ( 0.093)\tLoss 3.7679e+00 (3.8157e+00)\tAcc@1  16.41 (  9.92)\tAcc@5  33.59 ( 31.61)\n","Epoch: [1][360/391]\tTime  0.090 ( 0.093)\tLoss 3.7638e+00 (3.8052e+00)\tAcc@1   8.59 ( 10.09)\tAcc@5  34.38 ( 31.93)\n","Epoch: [1][390/391]\tTime  0.081 ( 0.093)\tLoss 3.7574e+00 (3.7930e+00)\tAcc@1   8.75 ( 10.37)\tAcc@5  36.25 ( 32.36)\n","==> Train Accuracy: Acc@1 10.370 || Acc@5 32.362\n","==> Test Accuracy:  Acc@1 14.460 || Acc@5 38.890\n","==> 38.98 seconds to train this epoch\n","\n","\n","----- epoch: 2, lr: 0.1 -----\n","Epoch: [2][  0/391]\tTime  0.248 ( 0.248)\tLoss 3.6276e+00 (3.6276e+00)\tAcc@1  15.62 ( 15.62)\tAcc@5  40.62 ( 40.62)\n","Epoch: [2][ 30/391]\tTime  0.091 ( 0.099)\tLoss 3.5721e+00 (3.5799e+00)\tAcc@1  10.94 ( 13.58)\tAcc@5  41.41 ( 39.01)\n","Epoch: [2][ 60/391]\tTime  0.087 ( 0.096)\tLoss 3.4153e+00 (3.5704e+00)\tAcc@1  20.31 ( 14.23)\tAcc@5  44.53 ( 39.42)\n","Epoch: [2][ 90/391]\tTime  0.088 ( 0.095)\tLoss 3.7068e+00 (3.5537e+00)\tAcc@1  14.84 ( 14.46)\tAcc@5  35.94 ( 39.90)\n","Epoch: [2][120/391]\tTime  0.094 ( 0.094)\tLoss 3.6342e+00 (3.5396e+00)\tAcc@1  14.06 ( 14.90)\tAcc@5  32.81 ( 40.19)\n","Epoch: [2][150/391]\tTime  0.092 ( 0.094)\tLoss 3.3144e+00 (3.5253e+00)\tAcc@1  17.97 ( 15.14)\tAcc@5  50.78 ( 40.72)\n","Epoch: [2][180/391]\tTime  0.089 ( 0.093)\tLoss 3.4390e+00 (3.5171e+00)\tAcc@1  17.19 ( 15.35)\tAcc@5  42.97 ( 40.93)\n","Epoch: [2][210/391]\tTime  0.085 ( 0.093)\tLoss 3.4456e+00 (3.5071e+00)\tAcc@1  21.09 ( 15.56)\tAcc@5  46.09 ( 41.24)\n","Epoch: [2][240/391]\tTime  0.092 ( 0.093)\tLoss 3.3314e+00 (3.4894e+00)\tAcc@1  16.41 ( 15.78)\tAcc@5  50.78 ( 41.88)\n","Epoch: [2][270/391]\tTime  0.090 ( 0.093)\tLoss 3.1843e+00 (3.4717e+00)\tAcc@1  22.66 ( 16.11)\tAcc@5  50.00 ( 42.37)\n","Epoch: [2][300/391]\tTime  0.092 ( 0.093)\tLoss 3.3737e+00 (3.4566e+00)\tAcc@1  17.97 ( 16.40)\tAcc@5  42.97 ( 42.83)\n","Epoch: [2][330/391]\tTime  0.098 ( 0.093)\tLoss 3.3044e+00 (3.4447e+00)\tAcc@1  21.88 ( 16.60)\tAcc@5  51.56 ( 43.23)\n","Epoch: [2][360/391]\tTime  0.097 ( 0.093)\tLoss 3.1247e+00 (3.4322e+00)\tAcc@1  24.22 ( 16.78)\tAcc@5  58.59 ( 43.65)\n","Epoch: [2][390/391]\tTime  0.082 ( 0.093)\tLoss 3.1274e+00 (3.4175e+00)\tAcc@1  27.50 ( 17.01)\tAcc@5  56.25 ( 44.04)\n","==> Train Accuracy: Acc@1 17.012 || Acc@5 44.042\n","==> Test Accuracy:  Acc@1 20.850 || Acc@5 49.430\n","==> 38.95 seconds to train this epoch\n","\n","\n","----- epoch: 3, lr: 0.1 -----\n","Epoch: [3][  0/391]\tTime  0.276 ( 0.276)\tLoss 3.0506e+00 (3.0506e+00)\tAcc@1  17.97 ( 17.97)\tAcc@5  56.25 ( 56.25)\n","Epoch: [3][ 30/391]\tTime  0.091 ( 0.098)\tLoss 3.1191e+00 (3.2038e+00)\tAcc@1  21.09 ( 20.01)\tAcc@5  50.78 ( 50.30)\n","Epoch: [3][ 60/391]\tTime  0.090 ( 0.095)\tLoss 3.1118e+00 (3.1817e+00)\tAcc@1  19.53 ( 20.76)\tAcc@5  52.34 ( 50.76)\n","Epoch: [3][ 90/391]\tTime  0.091 ( 0.094)\tLoss 3.1827e+00 (3.1763e+00)\tAcc@1  23.44 ( 21.22)\tAcc@5  51.56 ( 50.94)\n","Epoch: [3][120/391]\tTime  0.093 ( 0.094)\tLoss 3.4827e+00 (3.1750e+00)\tAcc@1  10.94 ( 21.18)\tAcc@5  39.06 ( 50.89)\n","Epoch: [3][150/391]\tTime  0.093 ( 0.094)\tLoss 3.1058e+00 (3.1725e+00)\tAcc@1  21.88 ( 21.32)\tAcc@5  57.03 ( 50.93)\n","Epoch: [3][180/391]\tTime  0.092 ( 0.093)\tLoss 3.2775e+00 (3.1673e+00)\tAcc@1  22.66 ( 21.50)\tAcc@5  45.31 ( 51.14)\n","Epoch: [3][210/391]\tTime  0.093 ( 0.093)\tLoss 3.2060e+00 (3.1628e+00)\tAcc@1  25.00 ( 21.75)\tAcc@5  49.22 ( 51.25)\n","Epoch: [3][240/391]\tTime  0.093 ( 0.093)\tLoss 3.3815e+00 (3.1532e+00)\tAcc@1  22.66 ( 21.82)\tAcc@5  45.31 ( 51.55)\n","Epoch: [3][270/391]\tTime  0.091 ( 0.093)\tLoss 3.2495e+00 (3.1397e+00)\tAcc@1  17.97 ( 22.04)\tAcc@5  49.22 ( 51.91)\n","Epoch: [3][300/391]\tTime  0.090 ( 0.093)\tLoss 2.9827e+00 (3.1295e+00)\tAcc@1  25.78 ( 22.17)\tAcc@5  56.25 ( 52.23)\n","Epoch: [3][330/391]\tTime  0.089 ( 0.093)\tLoss 2.9878e+00 (3.1180e+00)\tAcc@1  24.22 ( 22.32)\tAcc@5  62.50 ( 52.52)\n","Epoch: [3][360/391]\tTime  0.087 ( 0.093)\tLoss 3.0774e+00 (3.1082e+00)\tAcc@1  26.56 ( 22.46)\tAcc@5  51.56 ( 52.77)\n","Epoch: [3][390/391]\tTime  0.083 ( 0.093)\tLoss 2.9468e+00 (3.0951e+00)\tAcc@1  30.00 ( 22.76)\tAcc@5  58.75 ( 53.10)\n","==> Train Accuracy: Acc@1 22.758 || Acc@5 53.102\n","==> Test Accuracy:  Acc@1 23.930 || Acc@5 53.610\n","==> 38.99 seconds to train this epoch\n","\n","\n","----- epoch: 4, lr: 0.1 -----\n","Epoch: [4][  0/391]\tTime  0.271 ( 0.271)\tLoss 2.8767e+00 (2.8767e+00)\tAcc@1  25.78 ( 25.78)\tAcc@5  57.81 ( 57.81)\n","Epoch: [4][ 30/391]\tTime  0.091 ( 0.099)\tLoss 2.9530e+00 (2.9016e+00)\tAcc@1  22.66 ( 25.66)\tAcc@5  59.38 ( 58.24)\n","Epoch: [4][ 60/391]\tTime  0.088 ( 0.096)\tLoss 2.7144e+00 (2.9004e+00)\tAcc@1  27.34 ( 26.26)\tAcc@5  66.41 ( 58.43)\n","Epoch: [4][ 90/391]\tTime  0.095 ( 0.095)\tLoss 2.5553e+00 (2.8850e+00)\tAcc@1  37.50 ( 26.67)\tAcc@5  66.41 ( 58.41)\n","Epoch: [4][120/391]\tTime  0.092 ( 0.094)\tLoss 2.7273e+00 (2.8765e+00)\tAcc@1  30.47 ( 26.91)\tAcc@5  60.94 ( 58.65)\n","Epoch: [4][150/391]\tTime  0.099 ( 0.094)\tLoss 2.8842e+00 (2.8693e+00)\tAcc@1  27.34 ( 27.21)\tAcc@5  56.25 ( 58.66)\n","Epoch: [4][180/391]\tTime  0.091 ( 0.093)\tLoss 2.9250e+00 (2.8623e+00)\tAcc@1  26.56 ( 27.31)\tAcc@5  56.25 ( 58.87)\n","Epoch: [4][210/391]\tTime  0.093 ( 0.093)\tLoss 2.8132e+00 (2.8573e+00)\tAcc@1  29.69 ( 27.38)\tAcc@5  55.47 ( 59.03)\n","Epoch: [4][240/391]\tTime  0.090 ( 0.093)\tLoss 2.7607e+00 (2.8530e+00)\tAcc@1  27.34 ( 27.52)\tAcc@5  64.84 ( 59.14)\n","Epoch: [4][270/391]\tTime  0.089 ( 0.093)\tLoss 2.7970e+00 (2.8488e+00)\tAcc@1  30.47 ( 27.60)\tAcc@5  65.62 ( 59.28)\n","Epoch: [4][300/391]\tTime  0.092 ( 0.093)\tLoss 2.8447e+00 (2.8440e+00)\tAcc@1  31.25 ( 27.70)\tAcc@5  57.81 ( 59.40)\n","Epoch: [4][330/391]\tTime  0.092 ( 0.093)\tLoss 2.4059e+00 (2.8346e+00)\tAcc@1  34.38 ( 27.89)\tAcc@5  68.75 ( 59.73)\n","Epoch: [4][360/391]\tTime  0.094 ( 0.093)\tLoss 2.9249e+00 (2.8311e+00)\tAcc@1  29.69 ( 28.01)\tAcc@5  58.59 ( 59.79)\n","Epoch: [4][390/391]\tTime  0.082 ( 0.093)\tLoss 2.7423e+00 (2.8209e+00)\tAcc@1  37.50 ( 28.19)\tAcc@5  57.50 ( 60.01)\n","==> Train Accuracy: Acc@1 28.186 || Acc@5 60.010\n","==> Test Accuracy:  Acc@1 28.610 || Acc@5 59.560\n","==> 38.95 seconds to train this epoch\n","\n","\n","----- epoch: 5, lr: 0.1 -----\n","Epoch: [5][  0/391]\tTime  0.264 ( 0.264)\tLoss 2.6821e+00 (2.6821e+00)\tAcc@1  27.34 ( 27.34)\tAcc@5  60.16 ( 60.16)\n","Epoch: [5][ 30/391]\tTime  0.089 ( 0.099)\tLoss 2.4797e+00 (2.6543e+00)\tAcc@1  39.06 ( 31.38)\tAcc@5  67.97 ( 63.26)\n","Epoch: [5][ 60/391]\tTime  0.089 ( 0.096)\tLoss 2.6406e+00 (2.6510e+00)\tAcc@1  30.47 ( 32.06)\tAcc@5  64.06 ( 63.64)\n","Epoch: [5][ 90/391]\tTime  0.092 ( 0.095)\tLoss 2.6216e+00 (2.6535e+00)\tAcc@1  32.03 ( 31.86)\tAcc@5  66.41 ( 63.74)\n","Epoch: [5][120/391]\tTime  0.091 ( 0.094)\tLoss 2.9230e+00 (2.6514e+00)\tAcc@1  24.22 ( 31.67)\tAcc@5  58.59 ( 64.02)\n","Epoch: [5][150/391]\tTime  0.094 ( 0.093)\tLoss 2.2346e+00 (2.6441e+00)\tAcc@1  37.50 ( 31.63)\tAcc@5  73.44 ( 64.27)\n","Epoch: [5][180/391]\tTime  0.094 ( 0.093)\tLoss 2.5571e+00 (2.6364e+00)\tAcc@1  36.72 ( 31.67)\tAcc@5  67.97 ( 64.41)\n","Epoch: [5][210/391]\tTime  0.095 ( 0.093)\tLoss 2.5201e+00 (2.6350e+00)\tAcc@1  35.94 ( 31.75)\tAcc@5  64.84 ( 64.43)\n","Epoch: [5][240/391]\tTime  0.089 ( 0.093)\tLoss 2.6953e+00 (2.6345e+00)\tAcc@1  29.69 ( 31.67)\tAcc@5  62.50 ( 64.49)\n","Epoch: [5][270/391]\tTime  0.091 ( 0.093)\tLoss 2.7291e+00 (2.6543e+00)\tAcc@1  26.56 ( 31.29)\tAcc@5  61.72 ( 64.05)\n","Epoch: [5][300/391]\tTime  0.093 ( 0.093)\tLoss 2.6952e+00 (2.6573e+00)\tAcc@1  29.69 ( 31.18)\tAcc@5  64.84 ( 63.95)\n","Epoch: [5][330/391]\tTime  0.094 ( 0.093)\tLoss 2.5915e+00 (2.6572e+00)\tAcc@1  32.81 ( 31.24)\tAcc@5  62.50 ( 63.97)\n","Epoch: [5][360/391]\tTime  0.093 ( 0.093)\tLoss 2.5853e+00 (2.6541e+00)\tAcc@1  30.47 ( 31.37)\tAcc@5  64.06 ( 64.02)\n","Epoch: [5][390/391]\tTime  0.081 ( 0.093)\tLoss 2.6694e+00 (2.6492e+00)\tAcc@1  32.50 ( 31.54)\tAcc@5  60.00 ( 64.13)\n","==> Train Accuracy: Acc@1 31.542 || Acc@5 64.126\n","==> Test Accuracy:  Acc@1 31.440 || Acc@5 63.950\n","==> 38.98 seconds to train this epoch\n","\n","\n","----- epoch: 6, lr: 0.1 -----\n","Epoch: [6][  0/391]\tTime  0.259 ( 0.259)\tLoss 2.6292e+00 (2.6292e+00)\tAcc@1  24.22 ( 24.22)\tAcc@5  66.41 ( 66.41)\n","Epoch: [6][ 30/391]\tTime  0.096 ( 0.099)\tLoss 2.4218e+00 (2.5591e+00)\tAcc@1  37.50 ( 32.94)\tAcc@5  70.31 ( 66.56)\n","Epoch: [6][ 60/391]\tTime  0.090 ( 0.095)\tLoss 2.4329e+00 (2.5221e+00)\tAcc@1  34.38 ( 33.86)\tAcc@5  70.31 ( 67.09)\n","Epoch: [6][ 90/391]\tTime  0.096 ( 0.094)\tLoss 2.7900e+00 (2.5208e+00)\tAcc@1  23.44 ( 33.72)\tAcc@5  61.72 ( 67.19)\n","Epoch: [6][120/391]\tTime  0.092 ( 0.094)\tLoss 2.4493e+00 (2.5003e+00)\tAcc@1  33.59 ( 34.15)\tAcc@5  73.44 ( 67.70)\n","Epoch: [6][150/391]\tTime  0.094 ( 0.093)\tLoss 2.3189e+00 (2.4825e+00)\tAcc@1  36.72 ( 34.62)\tAcc@5  67.97 ( 68.03)\n","Epoch: [6][180/391]\tTime  0.092 ( 0.093)\tLoss 2.4670e+00 (2.4769e+00)\tAcc@1  35.94 ( 34.79)\tAcc@5  64.06 ( 68.03)\n","Epoch: [6][210/391]\tTime  0.091 ( 0.093)\tLoss 2.5136e+00 (2.4733e+00)\tAcc@1  32.81 ( 35.08)\tAcc@5  64.84 ( 68.08)\n","Epoch: [6][240/391]\tTime  0.092 ( 0.093)\tLoss 2.2610e+00 (2.4727e+00)\tAcc@1  38.28 ( 35.13)\tAcc@5  72.66 ( 68.14)\n","Epoch: [6][270/391]\tTime  0.091 ( 0.093)\tLoss 2.3466e+00 (2.4660e+00)\tAcc@1  41.41 ( 35.35)\tAcc@5  71.09 ( 68.25)\n","Epoch: [6][300/391]\tTime  0.091 ( 0.093)\tLoss 2.3653e+00 (2.4620e+00)\tAcc@1  43.75 ( 35.52)\tAcc@5  69.53 ( 68.28)\n","Epoch: [6][330/391]\tTime  0.093 ( 0.093)\tLoss 2.7748e+00 (2.4557e+00)\tAcc@1  27.34 ( 35.63)\tAcc@5  60.16 ( 68.40)\n","Epoch: [6][360/391]\tTime  0.095 ( 0.093)\tLoss 2.5721e+00 (2.4545e+00)\tAcc@1  34.38 ( 35.66)\tAcc@5  68.75 ( 68.32)\n","Epoch: [6][390/391]\tTime  0.082 ( 0.092)\tLoss 2.5217e+00 (2.4484e+00)\tAcc@1  32.50 ( 35.76)\tAcc@5  65.00 ( 68.52)\n","==> Train Accuracy: Acc@1 35.764 || Acc@5 68.522\n","==> Test Accuracy:  Acc@1 34.390 || Acc@5 66.860\n","==> 38.80 seconds to train this epoch\n","\n","\n","----- epoch: 7, lr: 0.1 -----\n","Epoch: [7][  0/391]\tTime  0.278 ( 0.278)\tLoss 2.0498e+00 (2.0498e+00)\tAcc@1  41.41 ( 41.41)\tAcc@5  78.12 ( 78.12)\n","Epoch: [7][ 30/391]\tTime  0.091 ( 0.099)\tLoss 2.5757e+00 (2.2198e+00)\tAcc@1  31.25 ( 40.22)\tAcc@5  70.31 ( 73.99)\n","Epoch: [7][ 60/391]\tTime  0.094 ( 0.096)\tLoss 2.5379e+00 (2.2384e+00)\tAcc@1  29.69 ( 39.86)\tAcc@5  67.97 ( 73.16)\n","Epoch: [7][ 90/391]\tTime  0.092 ( 0.095)\tLoss 2.2214e+00 (2.2675e+00)\tAcc@1  39.06 ( 39.51)\tAcc@5  75.00 ( 72.45)\n","Epoch: [7][120/391]\tTime  0.092 ( 0.094)\tLoss 2.1820e+00 (2.2726e+00)\tAcc@1  47.66 ( 39.22)\tAcc@5  74.22 ( 72.23)\n","Epoch: [7][150/391]\tTime  0.093 ( 0.094)\tLoss 2.5108e+00 (2.2757e+00)\tAcc@1  34.38 ( 39.12)\tAcc@5  68.75 ( 72.51)\n","Epoch: [7][180/391]\tTime  0.092 ( 0.093)\tLoss 2.2250e+00 (2.2706e+00)\tAcc@1  38.28 ( 39.27)\tAcc@5  73.44 ( 72.43)\n","Epoch: [7][210/391]\tTime  0.092 ( 0.093)\tLoss 2.2133e+00 (2.2741e+00)\tAcc@1  42.97 ( 39.26)\tAcc@5  71.09 ( 72.36)\n","Epoch: [7][240/391]\tTime  0.091 ( 0.093)\tLoss 2.4854e+00 (2.2681e+00)\tAcc@1  27.34 ( 39.31)\tAcc@5  70.31 ( 72.44)\n","Epoch: [7][270/391]\tTime  0.092 ( 0.093)\tLoss 2.1061e+00 (2.2685e+00)\tAcc@1  40.62 ( 39.37)\tAcc@5  80.47 ( 72.48)\n","Epoch: [7][300/391]\tTime  0.092 ( 0.093)\tLoss 2.2517e+00 (2.2714e+00)\tAcc@1  38.28 ( 39.34)\tAcc@5  69.53 ( 72.37)\n","Epoch: [7][330/391]\tTime  0.092 ( 0.093)\tLoss 2.1554e+00 (2.2731e+00)\tAcc@1  40.62 ( 39.34)\tAcc@5  73.44 ( 72.36)\n","Epoch: [7][360/391]\tTime  0.110 ( 0.093)\tLoss 2.4516e+00 (2.2714e+00)\tAcc@1  35.94 ( 39.48)\tAcc@5  68.75 ( 72.33)\n","Epoch: [7][390/391]\tTime  0.082 ( 0.093)\tLoss 1.9386e+00 (2.2691e+00)\tAcc@1  47.50 ( 39.54)\tAcc@5  81.25 ( 72.39)\n","==> Train Accuracy: Acc@1 39.544 || Acc@5 72.392\n","==> Test Accuracy:  Acc@1 41.270 || Acc@5 73.070\n","==> 38.91 seconds to train this epoch\n","\n","\n","----- epoch: 8, lr: 0.1 -----\n","Epoch: [8][  0/391]\tTime  0.286 ( 0.286)\tLoss 2.2000e+00 (2.2000e+00)\tAcc@1  35.94 ( 35.94)\tAcc@5  71.88 ( 71.88)\n","Epoch: [8][ 30/391]\tTime  0.084 ( 0.100)\tLoss 2.4231e+00 (2.2064e+00)\tAcc@1  35.94 ( 40.65)\tAcc@5  71.88 ( 73.77)\n","Epoch: [8][ 60/391]\tTime  0.091 ( 0.096)\tLoss 2.1736e+00 (2.1712e+00)\tAcc@1  39.84 ( 42.01)\tAcc@5  73.44 ( 74.30)\n","Epoch: [8][ 90/391]\tTime  0.089 ( 0.095)\tLoss 1.8872e+00 (2.1534e+00)\tAcc@1  46.09 ( 41.89)\tAcc@5  82.03 ( 74.86)\n","Epoch: [8][120/391]\tTime  0.088 ( 0.094)\tLoss 2.1739e+00 (2.1563e+00)\tAcc@1  41.41 ( 41.91)\tAcc@5  79.69 ( 74.75)\n","Epoch: [8][150/391]\tTime  0.087 ( 0.094)\tLoss 2.0841e+00 (2.1514e+00)\tAcc@1  46.88 ( 42.25)\tAcc@5  77.34 ( 74.73)\n","Epoch: [8][180/391]\tTime  0.093 ( 0.093)\tLoss 2.0128e+00 (2.1460e+00)\tAcc@1  47.66 ( 42.59)\tAcc@5  76.56 ( 74.79)\n","Epoch: [8][210/391]\tTime  0.095 ( 0.093)\tLoss 2.0402e+00 (2.1553e+00)\tAcc@1  48.44 ( 42.39)\tAcc@5  76.56 ( 74.65)\n","Epoch: [8][240/391]\tTime  0.091 ( 0.093)\tLoss 2.1248e+00 (2.1537e+00)\tAcc@1  35.16 ( 42.38)\tAcc@5  79.69 ( 74.81)\n","Epoch: [8][270/391]\tTime  0.090 ( 0.093)\tLoss 2.3652e+00 (2.1577e+00)\tAcc@1  35.94 ( 42.15)\tAcc@5  71.09 ( 74.67)\n","Epoch: [8][300/391]\tTime  0.094 ( 0.093)\tLoss 1.9505e+00 (2.1500e+00)\tAcc@1  46.09 ( 42.24)\tAcc@5  78.12 ( 74.80)\n","Epoch: [8][330/391]\tTime  0.093 ( 0.093)\tLoss 2.2404e+00 (2.1559e+00)\tAcc@1  41.41 ( 42.13)\tAcc@5  71.09 ( 74.69)\n","Epoch: [8][360/391]\tTime  0.092 ( 0.093)\tLoss 2.3524e+00 (2.1536e+00)\tAcc@1  36.72 ( 42.24)\tAcc@5  68.75 ( 74.74)\n","Epoch: [8][390/391]\tTime  0.083 ( 0.093)\tLoss 1.8277e+00 (2.1498e+00)\tAcc@1  48.75 ( 42.36)\tAcc@5  85.00 ( 74.83)\n","==> Train Accuracy: Acc@1 42.364 || Acc@5 74.834\n","==> Test Accuracy:  Acc@1 44.090 || Acc@5 75.990\n","==> 38.91 seconds to train this epoch\n","\n","\n","----- epoch: 9, lr: 0.1 -----\n","Epoch: [9][  0/391]\tTime  0.283 ( 0.283)\tLoss 2.1006e+00 (2.1006e+00)\tAcc@1  43.75 ( 43.75)\tAcc@5  80.47 ( 80.47)\n","Epoch: [9][ 30/391]\tTime  0.094 ( 0.100)\tLoss 1.7747e+00 (2.0536e+00)\tAcc@1  55.47 ( 43.62)\tAcc@5  82.03 ( 77.24)\n","Epoch: [9][ 60/391]\tTime  0.098 ( 0.096)\tLoss 2.1906e+00 (2.0603e+00)\tAcc@1  41.41 ( 43.43)\tAcc@5  72.66 ( 76.68)\n","Epoch: [9][ 90/391]\tTime  0.097 ( 0.095)\tLoss 2.1306e+00 (2.0570e+00)\tAcc@1  38.28 ( 43.64)\tAcc@5  79.69 ( 76.95)\n","Epoch: [9][120/391]\tTime  0.081 ( 0.094)\tLoss 1.9361e+00 (2.0577e+00)\tAcc@1  42.19 ( 43.78)\tAcc@5  80.47 ( 76.83)\n","Epoch: [9][150/391]\tTime  0.092 ( 0.094)\tLoss 2.0226e+00 (2.0586e+00)\tAcc@1  42.97 ( 43.84)\tAcc@5  77.34 ( 76.77)\n","Epoch: [9][180/391]\tTime  0.087 ( 0.094)\tLoss 2.1358e+00 (2.0516e+00)\tAcc@1  43.75 ( 44.18)\tAcc@5  71.09 ( 76.95)\n","Epoch: [9][210/391]\tTime  0.092 ( 0.093)\tLoss 2.3046e+00 (2.0488e+00)\tAcc@1  40.62 ( 44.34)\tAcc@5  69.53 ( 76.94)\n","Epoch: [9][240/391]\tTime  0.094 ( 0.093)\tLoss 2.1118e+00 (2.0501e+00)\tAcc@1  44.53 ( 44.40)\tAcc@5  72.66 ( 76.81)\n","Epoch: [9][270/391]\tTime  0.094 ( 0.093)\tLoss 1.9617e+00 (2.0512e+00)\tAcc@1  44.53 ( 44.36)\tAcc@5  78.12 ( 76.75)\n","Epoch: [9][300/391]\tTime  0.091 ( 0.093)\tLoss 2.0253e+00 (2.0474e+00)\tAcc@1  46.09 ( 44.49)\tAcc@5  75.00 ( 76.79)\n","Epoch: [9][330/391]\tTime  0.090 ( 0.093)\tLoss 2.0485e+00 (2.0480e+00)\tAcc@1  47.66 ( 44.48)\tAcc@5  75.78 ( 76.74)\n","Epoch: [9][360/391]\tTime  0.089 ( 0.093)\tLoss 2.0244e+00 (2.0462e+00)\tAcc@1  45.31 ( 44.55)\tAcc@5  80.47 ( 76.75)\n","Epoch: [9][390/391]\tTime  0.082 ( 0.093)\tLoss 2.2514e+00 (2.0443e+00)\tAcc@1  37.50 ( 44.54)\tAcc@5  72.50 ( 76.75)\n","==> Train Accuracy: Acc@1 44.544 || Acc@5 76.748\n","==> Test Accuracy:  Acc@1 47.610 || Acc@5 80.310\n","==> 38.98 seconds to train this epoch\n","\n","\n","----- epoch: 10, lr: 0.1 -----\n","Epoch: [10][  0/391]\tTime  0.275 ( 0.275)\tLoss 1.7466e+00 (1.7466e+00)\tAcc@1  48.44 ( 48.44)\tAcc@5  82.81 ( 82.81)\n","Epoch: [10][ 30/391]\tTime  0.090 ( 0.098)\tLoss 2.1633e+00 (1.9727e+00)\tAcc@1  37.50 ( 46.47)\tAcc@5  77.34 ( 78.43)\n","Epoch: [10][ 60/391]\tTime  0.087 ( 0.095)\tLoss 1.9092e+00 (1.9476e+00)\tAcc@1  44.53 ( 47.28)\tAcc@5  83.59 ( 78.93)\n","Epoch: [10][ 90/391]\tTime  0.095 ( 0.094)\tLoss 1.8208e+00 (1.9559e+00)\tAcc@1  48.44 ( 46.68)\tAcc@5  84.38 ( 78.75)\n","Epoch: [10][120/391]\tTime  0.093 ( 0.094)\tLoss 1.9281e+00 (1.9574e+00)\tAcc@1  47.66 ( 46.42)\tAcc@5  76.56 ( 78.67)\n","Epoch: [10][150/391]\tTime  0.097 ( 0.093)\tLoss 2.3744e+00 (1.9546e+00)\tAcc@1  37.50 ( 46.53)\tAcc@5  66.41 ( 78.57)\n","Epoch: [10][180/391]\tTime  0.094 ( 0.093)\tLoss 1.8589e+00 (1.9469e+00)\tAcc@1  56.25 ( 46.83)\tAcc@5  77.34 ( 78.66)\n","Epoch: [10][210/391]\tTime  0.094 ( 0.093)\tLoss 2.0031e+00 (1.9509e+00)\tAcc@1  49.22 ( 46.88)\tAcc@5  78.12 ( 78.45)\n","Epoch: [10][240/391]\tTime  0.091 ( 0.093)\tLoss 1.9956e+00 (1.9528e+00)\tAcc@1  44.53 ( 46.76)\tAcc@5  76.56 ( 78.45)\n","Epoch: [10][270/391]\tTime  0.095 ( 0.093)\tLoss 1.8610e+00 (1.9537e+00)\tAcc@1  49.22 ( 46.73)\tAcc@5  76.56 ( 78.46)\n","Epoch: [10][300/391]\tTime  0.092 ( 0.093)\tLoss 1.9259e+00 (1.9545e+00)\tAcc@1  49.22 ( 46.72)\tAcc@5  82.81 ( 78.47)\n","Epoch: [10][330/391]\tTime  0.088 ( 0.093)\tLoss 1.9615e+00 (1.9564e+00)\tAcc@1  45.31 ( 46.64)\tAcc@5  78.91 ( 78.46)\n","Epoch: [10][360/391]\tTime  0.094 ( 0.093)\tLoss 2.0424e+00 (1.9548e+00)\tAcc@1  40.62 ( 46.58)\tAcc@5  80.47 ( 78.53)\n","Epoch: [10][390/391]\tTime  0.082 ( 0.093)\tLoss 2.0560e+00 (1.9562e+00)\tAcc@1  42.50 ( 46.51)\tAcc@5  82.50 ( 78.58)\n","==> Train Accuracy: Acc@1 46.506 || Acc@5 78.580\n","==> Test Accuracy:  Acc@1 46.690 || Acc@5 77.040\n","==> 38.90 seconds to train this epoch\n","\n","\n","----- epoch: 11, lr: 0.1 -----\n","Epoch: [11][  0/391]\tTime  0.243 ( 0.243)\tLoss 2.0238e+00 (2.0238e+00)\tAcc@1  37.50 ( 37.50)\tAcc@5  81.25 ( 81.25)\n","Epoch: [11][ 30/391]\tTime  0.087 ( 0.098)\tLoss 1.8909e+00 (1.8493e+00)\tAcc@1  46.09 ( 49.09)\tAcc@5  77.34 ( 80.17)\n","Epoch: [11][ 60/391]\tTime  0.092 ( 0.095)\tLoss 1.8228e+00 (1.8386e+00)\tAcc@1  49.22 ( 49.01)\tAcc@5  82.03 ( 80.65)\n","Epoch: [11][ 90/391]\tTime  0.092 ( 0.094)\tLoss 2.0264e+00 (1.8463e+00)\tAcc@1  42.19 ( 49.07)\tAcc@5  78.12 ( 80.53)\n","Epoch: [11][120/391]\tTime  0.092 ( 0.094)\tLoss 2.2838e+00 (1.8661e+00)\tAcc@1  38.28 ( 48.72)\tAcc@5  71.09 ( 80.06)\n","Epoch: [11][150/391]\tTime  0.092 ( 0.093)\tLoss 2.0116e+00 (1.8602e+00)\tAcc@1  46.88 ( 48.65)\tAcc@5  75.78 ( 80.28)\n","Epoch: [11][180/391]\tTime  0.096 ( 0.093)\tLoss 1.8744e+00 (1.8639e+00)\tAcc@1  44.53 ( 48.55)\tAcc@5  82.03 ( 80.17)\n","Epoch: [11][210/391]\tTime  0.091 ( 0.093)\tLoss 1.8196e+00 (1.8664e+00)\tAcc@1  53.12 ( 48.56)\tAcc@5  78.12 ( 80.19)\n","Epoch: [11][240/391]\tTime  0.089 ( 0.093)\tLoss 1.9963e+00 (1.8681e+00)\tAcc@1  49.22 ( 48.54)\tAcc@5  77.34 ( 80.07)\n","Epoch: [11][270/391]\tTime  0.090 ( 0.093)\tLoss 1.8752e+00 (1.8699e+00)\tAcc@1  46.88 ( 48.52)\tAcc@5  79.69 ( 80.06)\n","Epoch: [11][300/391]\tTime  0.093 ( 0.093)\tLoss 1.7426e+00 (1.8706e+00)\tAcc@1  56.25 ( 48.50)\tAcc@5  81.25 ( 80.07)\n","Epoch: [11][330/391]\tTime  0.092 ( 0.093)\tLoss 1.9126e+00 (1.8731e+00)\tAcc@1  46.09 ( 48.52)\tAcc@5  78.12 ( 80.02)\n","Epoch: [11][360/391]\tTime  0.091 ( 0.093)\tLoss 2.1719e+00 (1.8750e+00)\tAcc@1  35.94 ( 48.46)\tAcc@5  78.12 ( 79.96)\n","Epoch: [11][390/391]\tTime  0.082 ( 0.093)\tLoss 1.9136e+00 (1.8775e+00)\tAcc@1  45.00 ( 48.43)\tAcc@5  83.75 ( 79.89)\n","==> Train Accuracy: Acc@1 48.426 || Acc@5 79.894\n","==> Test Accuracy:  Acc@1 48.640 || Acc@5 80.170\n","==> 38.86 seconds to train this epoch\n","\n","\n","----- epoch: 12, lr: 0.1 -----\n","Epoch: [12][  0/391]\tTime  0.289 ( 0.289)\tLoss 1.6264e+00 (1.6264e+00)\tAcc@1  52.34 ( 52.34)\tAcc@5  87.50 ( 87.50)\n","Epoch: [12][ 30/391]\tTime  0.091 ( 0.099)\tLoss 2.0338e+00 (1.8135e+00)\tAcc@1  44.53 ( 50.28)\tAcc@5  76.56 ( 80.59)\n","Epoch: [12][ 60/391]\tTime  0.091 ( 0.096)\tLoss 1.8816e+00 (1.8208e+00)\tAcc@1  47.66 ( 50.08)\tAcc@5  76.56 ( 80.72)\n","Epoch: [12][ 90/391]\tTime  0.087 ( 0.095)\tLoss 1.7163e+00 (1.8180e+00)\tAcc@1  57.81 ( 50.37)\tAcc@5  81.25 ( 80.61)\n","Epoch: [12][120/391]\tTime  0.090 ( 0.094)\tLoss 1.8980e+00 (1.8204e+00)\tAcc@1  46.09 ( 50.24)\tAcc@5  77.34 ( 80.48)\n","Epoch: [12][150/391]\tTime  0.090 ( 0.094)\tLoss 1.9029e+00 (1.8116e+00)\tAcc@1  48.44 ( 50.29)\tAcc@5  81.25 ( 80.76)\n","Epoch: [12][180/391]\tTime  0.092 ( 0.094)\tLoss 1.6032e+00 (1.8031e+00)\tAcc@1  53.12 ( 50.56)\tAcc@5  83.59 ( 80.82)\n","Epoch: [12][210/391]\tTime  0.087 ( 0.093)\tLoss 1.7869e+00 (1.8078e+00)\tAcc@1  51.56 ( 50.40)\tAcc@5  82.81 ( 80.78)\n","Epoch: [12][240/391]\tTime  0.092 ( 0.093)\tLoss 1.7374e+00 (1.8081e+00)\tAcc@1  54.69 ( 50.33)\tAcc@5  81.25 ( 80.83)\n","Epoch: [12][270/391]\tTime  0.088 ( 0.093)\tLoss 1.7019e+00 (1.8154e+00)\tAcc@1  55.47 ( 50.10)\tAcc@5  81.25 ( 80.77)\n","Epoch: [12][300/391]\tTime  0.092 ( 0.093)\tLoss 1.8939e+00 (1.8182e+00)\tAcc@1  46.88 ( 49.97)\tAcc@5  76.56 ( 80.79)\n","Epoch: [12][330/391]\tTime  0.091 ( 0.093)\tLoss 1.7335e+00 (1.8206e+00)\tAcc@1  57.03 ( 49.88)\tAcc@5  80.47 ( 80.81)\n","Epoch: [12][360/391]\tTime  0.090 ( 0.093)\tLoss 1.8948e+00 (1.8263e+00)\tAcc@1  47.66 ( 49.76)\tAcc@5  79.69 ( 80.74)\n","Epoch: [12][390/391]\tTime  0.082 ( 0.093)\tLoss 1.8153e+00 (1.8261e+00)\tAcc@1  45.00 ( 49.75)\tAcc@5  83.75 ( 80.76)\n","==> Train Accuracy: Acc@1 49.746 || Acc@5 80.760\n","==> Test Accuracy:  Acc@1 48.850 || Acc@5 80.180\n","==> 38.97 seconds to train this epoch\n","\n","\n","----- epoch: 13, lr: 0.1 -----\n","Epoch: [13][  0/391]\tTime  0.242 ( 0.242)\tLoss 1.7793e+00 (1.7793e+00)\tAcc@1  53.12 ( 53.12)\tAcc@5  82.81 ( 82.81)\n","Epoch: [13][ 30/391]\tTime  0.093 ( 0.099)\tLoss 1.7918e+00 (1.7508e+00)\tAcc@1  52.34 ( 52.07)\tAcc@5  85.94 ( 81.91)\n","Epoch: [13][ 60/391]\tTime  0.093 ( 0.095)\tLoss 1.7502e+00 (1.7477e+00)\tAcc@1  60.16 ( 51.74)\tAcc@5  82.03 ( 82.15)\n","Epoch: [13][ 90/391]\tTime  0.090 ( 0.094)\tLoss 1.9504e+00 (1.7473e+00)\tAcc@1  46.88 ( 51.69)\tAcc@5  76.56 ( 82.16)\n","Epoch: [13][120/391]\tTime  0.095 ( 0.094)\tLoss 1.9699e+00 (1.7523e+00)\tAcc@1  46.88 ( 51.56)\tAcc@5  78.91 ( 82.15)\n","Epoch: [13][150/391]\tTime  0.094 ( 0.094)\tLoss 1.5628e+00 (1.7588e+00)\tAcc@1  57.81 ( 51.10)\tAcc@5  85.16 ( 82.16)\n","Epoch: [13][180/391]\tTime  0.091 ( 0.093)\tLoss 1.9079e+00 (1.7598e+00)\tAcc@1  49.22 ( 51.17)\tAcc@5  82.81 ( 82.01)\n","Epoch: [13][210/391]\tTime  0.093 ( 0.093)\tLoss 1.5716e+00 (1.7662e+00)\tAcc@1  57.03 ( 51.08)\tAcc@5  89.06 ( 81.91)\n","Epoch: [13][240/391]\tTime  0.091 ( 0.093)\tLoss 1.3951e+00 (1.7663e+00)\tAcc@1  60.94 ( 51.05)\tAcc@5  89.84 ( 81.91)\n","Epoch: [13][270/391]\tTime  0.094 ( 0.093)\tLoss 2.1006e+00 (1.7667e+00)\tAcc@1  46.09 ( 51.08)\tAcc@5  78.12 ( 81.88)\n","Epoch: [13][300/391]\tTime  0.092 ( 0.093)\tLoss 1.8811e+00 (1.7653e+00)\tAcc@1  42.97 ( 51.13)\tAcc@5  77.34 ( 81.91)\n","Epoch: [13][330/391]\tTime  0.094 ( 0.093)\tLoss 1.8145e+00 (1.7701e+00)\tAcc@1  54.69 ( 51.01)\tAcc@5  79.69 ( 81.78)\n","Epoch: [13][360/391]\tTime  0.090 ( 0.093)\tLoss 1.7506e+00 (1.7709e+00)\tAcc@1  53.91 ( 51.01)\tAcc@5  85.16 ( 81.78)\n","Epoch: [13][390/391]\tTime  0.083 ( 0.093)\tLoss 1.5138e+00 (1.7725e+00)\tAcc@1  58.75 ( 51.01)\tAcc@5  81.25 ( 81.82)\n","==> Train Accuracy: Acc@1 51.006 || Acc@5 81.816\n","==> Test Accuracy:  Acc@1 53.630 || Acc@5 82.690\n","==> 38.86 seconds to train this epoch\n","\n","\n","----- epoch: 14, lr: 0.1 -----\n","Epoch: [14][  0/391]\tTime  0.260 ( 0.260)\tLoss 1.7902e+00 (1.7902e+00)\tAcc@1  46.88 ( 46.88)\tAcc@5  83.59 ( 83.59)\n","Epoch: [14][ 30/391]\tTime  0.090 ( 0.098)\tLoss 1.7783e+00 (1.7173e+00)\tAcc@1  50.00 ( 51.71)\tAcc@5  80.47 ( 82.61)\n","Epoch: [14][ 60/391]\tTime  0.093 ( 0.095)\tLoss 1.6715e+00 (1.7148e+00)\tAcc@1  53.12 ( 52.06)\tAcc@5  83.59 ( 82.65)\n","Epoch: [14][ 90/391]\tTime  0.090 ( 0.094)\tLoss 1.8706e+00 (1.7113e+00)\tAcc@1  49.22 ( 51.97)\tAcc@5  78.12 ( 82.73)\n","Epoch: [14][120/391]\tTime  0.092 ( 0.094)\tLoss 1.5355e+00 (1.7141e+00)\tAcc@1  54.69 ( 52.07)\tAcc@5  89.06 ( 82.77)\n","Epoch: [14][150/391]\tTime  0.093 ( 0.093)\tLoss 1.6723e+00 (1.7212e+00)\tAcc@1  50.00 ( 51.87)\tAcc@5  85.16 ( 82.80)\n","Epoch: [14][180/391]\tTime  0.094 ( 0.093)\tLoss 1.6583e+00 (1.7267e+00)\tAcc@1  55.47 ( 51.74)\tAcc@5  82.03 ( 82.58)\n","Epoch: [14][210/391]\tTime  0.091 ( 0.093)\tLoss 1.8210e+00 (1.7238e+00)\tAcc@1  48.44 ( 51.80)\tAcc@5  79.69 ( 82.62)\n","Epoch: [14][240/391]\tTime  0.090 ( 0.093)\tLoss 2.0012e+00 (1.7286e+00)\tAcc@1  46.88 ( 51.75)\tAcc@5  75.78 ( 82.58)\n","Epoch: [14][270/391]\tTime  0.092 ( 0.093)\tLoss 1.7502e+00 (1.7289e+00)\tAcc@1  54.69 ( 51.76)\tAcc@5  79.69 ( 82.58)\n","Epoch: [14][300/391]\tTime  0.094 ( 0.093)\tLoss 1.7263e+00 (1.7273e+00)\tAcc@1  50.00 ( 51.81)\tAcc@5  86.72 ( 82.58)\n","Epoch: [14][330/391]\tTime  0.089 ( 0.093)\tLoss 1.8395e+00 (1.7282e+00)\tAcc@1  44.53 ( 51.77)\tAcc@5  83.59 ( 82.58)\n","Epoch: [14][360/391]\tTime  0.096 ( 0.093)\tLoss 1.8020e+00 (1.7285e+00)\tAcc@1  48.44 ( 51.75)\tAcc@5  78.91 ( 82.55)\n","Epoch: [14][390/391]\tTime  0.082 ( 0.092)\tLoss 1.7179e+00 (1.7291e+00)\tAcc@1  58.75 ( 51.75)\tAcc@5  78.75 ( 82.52)\n","==> Train Accuracy: Acc@1 51.748 || Acc@5 82.524\n","==> Test Accuracy:  Acc@1 52.250 || Acc@5 81.620\n","==> 38.79 seconds to train this epoch\n","\n","\n","----- epoch: 15, lr: 0.1 -----\n","Epoch: [15][  0/391]\tTime  0.263 ( 0.263)\tLoss 1.6429e+00 (1.6429e+00)\tAcc@1  55.47 ( 55.47)\tAcc@5  79.69 ( 79.69)\n","Epoch: [15][ 30/391]\tTime  0.094 ( 0.099)\tLoss 1.6548e+00 (1.6326e+00)\tAcc@1  53.12 ( 54.91)\tAcc@5  85.16 ( 84.12)\n","Epoch: [15][ 60/391]\tTime  0.092 ( 0.095)\tLoss 1.6722e+00 (1.6366e+00)\tAcc@1  55.47 ( 54.79)\tAcc@5  83.59 ( 84.32)\n","Epoch: [15][ 90/391]\tTime  0.091 ( 0.094)\tLoss 1.6546e+00 (1.6615e+00)\tAcc@1  54.69 ( 54.10)\tAcc@5  85.16 ( 83.93)\n","Epoch: [15][120/391]\tTime  0.090 ( 0.094)\tLoss 1.7362e+00 (1.6819e+00)\tAcc@1  53.12 ( 53.56)\tAcc@5  86.72 ( 83.57)\n","Epoch: [15][150/391]\tTime  0.095 ( 0.093)\tLoss 1.9219e+00 (1.6902e+00)\tAcc@1  49.22 ( 53.25)\tAcc@5  82.81 ( 83.47)\n","Epoch: [15][180/391]\tTime  0.088 ( 0.093)\tLoss 1.6523e+00 (1.6859e+00)\tAcc@1  53.91 ( 53.36)\tAcc@5  78.12 ( 83.43)\n","Epoch: [15][210/391]\tTime  0.096 ( 0.093)\tLoss 1.7130e+00 (1.6825e+00)\tAcc@1  48.44 ( 53.30)\tAcc@5  81.25 ( 83.55)\n","Epoch: [15][240/391]\tTime  0.090 ( 0.093)\tLoss 1.7261e+00 (1.6859e+00)\tAcc@1  51.56 ( 53.29)\tAcc@5  82.81 ( 83.47)\n","Epoch: [15][270/391]\tTime  0.093 ( 0.093)\tLoss 1.9215e+00 (1.6859e+00)\tAcc@1  52.34 ( 53.27)\tAcc@5  80.47 ( 83.50)\n","Epoch: [15][300/391]\tTime  0.094 ( 0.093)\tLoss 1.5793e+00 (1.6867e+00)\tAcc@1  53.91 ( 53.23)\tAcc@5  85.16 ( 83.46)\n","Epoch: [15][330/391]\tTime  0.095 ( 0.093)\tLoss 1.9805e+00 (1.6917e+00)\tAcc@1  49.22 ( 53.13)\tAcc@5  82.03 ( 83.32)\n","Epoch: [15][360/391]\tTime  0.093 ( 0.093)\tLoss 1.8278e+00 (1.6919e+00)\tAcc@1  57.81 ( 53.17)\tAcc@5  77.34 ( 83.33)\n","Epoch: [15][390/391]\tTime  0.082 ( 0.093)\tLoss 1.5709e+00 (1.6924e+00)\tAcc@1  52.50 ( 53.13)\tAcc@5  86.25 ( 83.29)\n","==> Train Accuracy: Acc@1 53.130 || Acc@5 83.294\n","==> Test Accuracy:  Acc@1 50.230 || Acc@5 80.600\n","==> 38.89 seconds to train this epoch\n","\n","\n","----- epoch: 16, lr: 0.1 -----\n","Epoch: [16][  0/391]\tTime  0.296 ( 0.296)\tLoss 1.7272e+00 (1.7272e+00)\tAcc@1  48.44 ( 48.44)\tAcc@5  86.72 ( 86.72)\n","Epoch: [16][ 30/391]\tTime  0.095 ( 0.100)\tLoss 1.5621e+00 (1.6486e+00)\tAcc@1  57.81 ( 53.12)\tAcc@5  87.50 ( 84.73)\n","Epoch: [16][ 60/391]\tTime  0.093 ( 0.096)\tLoss 1.8636e+00 (1.6343e+00)\tAcc@1  52.34 ( 53.78)\tAcc@5  76.56 ( 84.73)\n","Epoch: [16][ 90/391]\tTime  0.088 ( 0.095)\tLoss 1.7356e+00 (1.6430e+00)\tAcc@1  47.66 ( 53.66)\tAcc@5  83.59 ( 84.31)\n","Epoch: [16][120/391]\tTime  0.094 ( 0.094)\tLoss 1.5470e+00 (1.6494e+00)\tAcc@1  53.12 ( 53.68)\tAcc@5  85.94 ( 83.99)\n","Epoch: [16][150/391]\tTime  0.087 ( 0.094)\tLoss 1.5931e+00 (1.6581e+00)\tAcc@1  56.25 ( 53.64)\tAcc@5  86.72 ( 83.80)\n","Epoch: [16][180/391]\tTime  0.092 ( 0.093)\tLoss 1.4576e+00 (1.6567e+00)\tAcc@1  60.94 ( 53.79)\tAcc@5  86.72 ( 83.72)\n","Epoch: [16][210/391]\tTime  0.093 ( 0.093)\tLoss 1.6156e+00 (1.6569e+00)\tAcc@1  50.78 ( 53.89)\tAcc@5  89.06 ( 83.63)\n","Epoch: [16][240/391]\tTime  0.092 ( 0.093)\tLoss 1.5547e+00 (1.6563e+00)\tAcc@1  54.69 ( 53.86)\tAcc@5  85.16 ( 83.61)\n","Epoch: [16][270/391]\tTime  0.089 ( 0.093)\tLoss 1.6083e+00 (1.6545e+00)\tAcc@1  54.69 ( 53.81)\tAcc@5  80.47 ( 83.68)\n","Epoch: [16][300/391]\tTime  0.091 ( 0.093)\tLoss 1.6036e+00 (1.6625e+00)\tAcc@1  56.25 ( 53.64)\tAcc@5  85.16 ( 83.53)\n","Epoch: [16][330/391]\tTime  0.083 ( 0.093)\tLoss 1.8232e+00 (1.6690e+00)\tAcc@1  46.09 ( 53.45)\tAcc@5  82.03 ( 83.46)\n","Epoch: [16][360/391]\tTime  0.092 ( 0.093)\tLoss 1.6582e+00 (1.6719e+00)\tAcc@1  53.12 ( 53.39)\tAcc@5  81.25 ( 83.40)\n","Epoch: [16][390/391]\tTime  0.083 ( 0.093)\tLoss 1.6307e+00 (1.6705e+00)\tAcc@1  56.25 ( 53.47)\tAcc@5  83.75 ( 83.44)\n","==> Train Accuracy: Acc@1 53.468 || Acc@5 83.438\n","==> Test Accuracy:  Acc@1 54.210 || Acc@5 83.710\n","==> 38.88 seconds to train this epoch\n","\n","\n","----- epoch: 17, lr: 0.1 -----\n","Epoch: [17][  0/391]\tTime  0.262 ( 0.262)\tLoss 1.4066e+00 (1.4066e+00)\tAcc@1  60.16 ( 60.16)\tAcc@5  84.38 ( 84.38)\n","Epoch: [17][ 30/391]\tTime  0.094 ( 0.099)\tLoss 1.7323e+00 (1.5444e+00)\tAcc@1  51.56 ( 56.68)\tAcc@5  83.59 ( 86.14)\n","Epoch: [17][ 60/391]\tTime  0.098 ( 0.096)\tLoss 1.7613e+00 (1.6039e+00)\tAcc@1  54.69 ( 55.57)\tAcc@5  85.16 ( 84.58)\n","Epoch: [17][ 90/391]\tTime  0.099 ( 0.095)\tLoss 1.4748e+00 (1.6205e+00)\tAcc@1  58.59 ( 55.01)\tAcc@5  84.38 ( 84.23)\n","Epoch: [17][120/391]\tTime  0.093 ( 0.094)\tLoss 1.6007e+00 (1.6212e+00)\tAcc@1  55.47 ( 54.91)\tAcc@5  84.38 ( 84.21)\n","Epoch: [17][150/391]\tTime  0.093 ( 0.093)\tLoss 1.7429e+00 (1.6210e+00)\tAcc@1  51.56 ( 54.88)\tAcc@5  83.59 ( 84.25)\n","Epoch: [17][180/391]\tTime  0.092 ( 0.093)\tLoss 2.0465e+00 (1.6403e+00)\tAcc@1  46.09 ( 54.42)\tAcc@5  75.78 ( 84.05)\n","Epoch: [17][210/391]\tTime  0.092 ( 0.093)\tLoss 1.4646e+00 (1.6380e+00)\tAcc@1  54.69 ( 54.51)\tAcc@5  85.94 ( 84.12)\n","Epoch: [17][240/391]\tTime  0.095 ( 0.093)\tLoss 1.6934e+00 (1.6358e+00)\tAcc@1  52.34 ( 54.43)\tAcc@5  82.03 ( 84.12)\n","Epoch: [17][270/391]\tTime  0.090 ( 0.093)\tLoss 1.4128e+00 (1.6307e+00)\tAcc@1  57.03 ( 54.59)\tAcc@5  89.84 ( 84.23)\n","Epoch: [17][300/391]\tTime  0.098 ( 0.093)\tLoss 1.8257e+00 (1.6330e+00)\tAcc@1  53.12 ( 54.60)\tAcc@5  79.69 ( 84.19)\n","Epoch: [17][330/391]\tTime  0.094 ( 0.093)\tLoss 1.7490e+00 (1.6331e+00)\tAcc@1  47.66 ( 54.54)\tAcc@5  80.47 ( 84.16)\n","Epoch: [17][360/391]\tTime  0.091 ( 0.093)\tLoss 1.6848e+00 (1.6322e+00)\tAcc@1  52.34 ( 54.48)\tAcc@5  83.59 ( 84.21)\n","Epoch: [17][390/391]\tTime  0.081 ( 0.093)\tLoss 1.4695e+00 (1.6344e+00)\tAcc@1  57.50 ( 54.46)\tAcc@5  92.50 ( 84.16)\n","==> Train Accuracy: Acc@1 54.462 || Acc@5 84.158\n","==> Test Accuracy:  Acc@1 55.030 || Acc@5 84.210\n","==> 38.83 seconds to train this epoch\n","\n","\n","----- epoch: 18, lr: 0.1 -----\n","Epoch: [18][  0/391]\tTime  0.275 ( 0.275)\tLoss 1.5625e+00 (1.5625e+00)\tAcc@1  57.03 ( 57.03)\tAcc@5  85.16 ( 85.16)\n","Epoch: [18][ 30/391]\tTime  0.092 ( 0.099)\tLoss 1.5340e+00 (1.5911e+00)\tAcc@1  57.03 ( 55.75)\tAcc@5  85.94 ( 85.16)\n","Epoch: [18][ 60/391]\tTime  0.091 ( 0.096)\tLoss 1.6979e+00 (1.5700e+00)\tAcc@1  53.91 ( 56.06)\tAcc@5  80.47 ( 85.19)\n","Epoch: [18][ 90/391]\tTime  0.090 ( 0.095)\tLoss 1.5820e+00 (1.5694e+00)\tAcc@1  54.69 ( 55.92)\tAcc@5  85.16 ( 85.27)\n","Epoch: [18][120/391]\tTime  0.092 ( 0.094)\tLoss 1.4869e+00 (1.5862e+00)\tAcc@1  61.72 ( 55.68)\tAcc@5  85.16 ( 84.98)\n","Epoch: [18][150/391]\tTime  0.095 ( 0.094)\tLoss 1.3488e+00 (1.5911e+00)\tAcc@1  62.50 ( 55.77)\tAcc@5  86.72 ( 84.86)\n","Epoch: [18][180/391]\tTime  0.085 ( 0.094)\tLoss 1.5639e+00 (1.5928e+00)\tAcc@1  54.69 ( 55.67)\tAcc@5  88.28 ( 84.84)\n","Epoch: [18][210/391]\tTime  0.091 ( 0.093)\tLoss 1.9663e+00 (1.5958e+00)\tAcc@1  48.44 ( 55.67)\tAcc@5  77.34 ( 84.76)\n","Epoch: [18][240/391]\tTime  0.092 ( 0.093)\tLoss 1.7646e+00 (1.6001e+00)\tAcc@1  52.34 ( 55.43)\tAcc@5  81.25 ( 84.75)\n","Epoch: [18][270/391]\tTime  0.092 ( 0.093)\tLoss 1.3617e+00 (1.6030e+00)\tAcc@1  64.06 ( 55.31)\tAcc@5  89.06 ( 84.64)\n","Epoch: [18][300/391]\tTime  0.096 ( 0.093)\tLoss 1.6954e+00 (1.6028e+00)\tAcc@1  54.69 ( 55.35)\tAcc@5  82.03 ( 84.62)\n","Epoch: [18][330/391]\tTime  0.092 ( 0.093)\tLoss 1.6326e+00 (1.6058e+00)\tAcc@1  58.59 ( 55.24)\tAcc@5  85.16 ( 84.55)\n","Epoch: [18][360/391]\tTime  0.087 ( 0.093)\tLoss 1.6680e+00 (1.6107e+00)\tAcc@1  50.78 ( 55.15)\tAcc@5  83.59 ( 84.44)\n","Epoch: [18][390/391]\tTime  0.081 ( 0.093)\tLoss 1.7166e+00 (1.6118e+00)\tAcc@1  48.75 ( 55.17)\tAcc@5  83.75 ( 84.42)\n","==> Train Accuracy: Acc@1 55.170 || Acc@5 84.422\n","==> Test Accuracy:  Acc@1 48.960 || Acc@5 77.270\n","==> 38.95 seconds to train this epoch\n","\n","\n","----- epoch: 19, lr: 0.1 -----\n","Epoch: [19][  0/391]\tTime  0.275 ( 0.275)\tLoss 1.6239e+00 (1.6239e+00)\tAcc@1  53.12 ( 53.12)\tAcc@5  84.38 ( 84.38)\n","Epoch: [19][ 30/391]\tTime  0.089 ( 0.098)\tLoss 1.3479e+00 (1.5068e+00)\tAcc@1  62.50 ( 58.24)\tAcc@5  87.50 ( 86.39)\n","Epoch: [19][ 60/391]\tTime  0.092 ( 0.095)\tLoss 1.8399e+00 (1.5171e+00)\tAcc@1  50.00 ( 57.43)\tAcc@5  83.59 ( 86.35)\n","Epoch: [19][ 90/391]\tTime  0.094 ( 0.094)\tLoss 1.4648e+00 (1.5371e+00)\tAcc@1  55.47 ( 56.77)\tAcc@5  85.16 ( 85.80)\n","Epoch: [19][120/391]\tTime  0.092 ( 0.094)\tLoss 1.7954e+00 (1.5421e+00)\tAcc@1  53.91 ( 56.87)\tAcc@5  82.03 ( 85.66)\n","Epoch: [19][150/391]\tTime  0.087 ( 0.093)\tLoss 1.5744e+00 (1.5483e+00)\tAcc@1  57.03 ( 56.77)\tAcc@5  85.16 ( 85.67)\n","Epoch: [19][180/391]\tTime  0.092 ( 0.093)\tLoss 1.7460e+00 (1.5679e+00)\tAcc@1  50.78 ( 56.24)\tAcc@5  85.94 ( 85.38)\n","Epoch: [19][210/391]\tTime  0.093 ( 0.093)\tLoss 1.5413e+00 (1.5696e+00)\tAcc@1  54.69 ( 56.18)\tAcc@5  86.72 ( 85.29)\n","Epoch: [19][240/391]\tTime  0.092 ( 0.093)\tLoss 1.7151e+00 (1.5695e+00)\tAcc@1  53.12 ( 56.17)\tAcc@5  85.16 ( 85.29)\n","Epoch: [19][270/391]\tTime  0.087 ( 0.093)\tLoss 1.5806e+00 (1.5805e+00)\tAcc@1  57.03 ( 55.83)\tAcc@5  85.16 ( 85.09)\n","Epoch: [19][300/391]\tTime  0.090 ( 0.093)\tLoss 1.8111e+00 (1.5832e+00)\tAcc@1  53.91 ( 55.79)\tAcc@5  82.81 ( 85.11)\n","Epoch: [19][330/391]\tTime  0.094 ( 0.093)\tLoss 1.6315e+00 (1.5866e+00)\tAcc@1  60.94 ( 55.74)\tAcc@5  85.16 ( 85.10)\n","Epoch: [19][360/391]\tTime  0.091 ( 0.093)\tLoss 1.4366e+00 (1.5893e+00)\tAcc@1  53.12 ( 55.58)\tAcc@5  91.41 ( 85.12)\n","Epoch: [19][390/391]\tTime  0.081 ( 0.093)\tLoss 1.7710e+00 (1.5923e+00)\tAcc@1  43.75 ( 55.59)\tAcc@5  83.75 ( 85.07)\n","==> Train Accuracy: Acc@1 55.586 || Acc@5 85.074\n","==> Test Accuracy:  Acc@1 53.420 || Acc@5 83.390\n","==> 38.87 seconds to train this epoch\n","\n","\n","----- epoch: 20, lr: 0.1 -----\n","Epoch: [20][  0/391]\tTime  0.287 ( 0.287)\tLoss 1.6486e+00 (1.6486e+00)\tAcc@1  54.69 ( 54.69)\tAcc@5  83.59 ( 83.59)\n","Epoch: [20][ 30/391]\tTime  0.095 ( 0.099)\tLoss 1.6113e+00 (1.5418e+00)\tAcc@1  53.91 ( 56.45)\tAcc@5  84.38 ( 85.18)\n","Epoch: [20][ 60/391]\tTime  0.093 ( 0.096)\tLoss 1.5060e+00 (1.5403e+00)\tAcc@1  52.34 ( 56.51)\tAcc@5  82.03 ( 85.53)\n","Epoch: [20][ 90/391]\tTime  0.090 ( 0.095)\tLoss 1.5498e+00 (1.5419e+00)\tAcc@1  57.03 ( 56.77)\tAcc@5  82.03 ( 85.49)\n","Epoch: [20][120/391]\tTime  0.095 ( 0.094)\tLoss 1.4330e+00 (1.5445e+00)\tAcc@1  59.38 ( 56.53)\tAcc@5  86.72 ( 85.59)\n","Epoch: [20][150/391]\tTime  0.094 ( 0.094)\tLoss 1.5625e+00 (1.5375e+00)\tAcc@1  60.16 ( 56.88)\tAcc@5  86.72 ( 85.67)\n","Epoch: [20][180/391]\tTime  0.094 ( 0.093)\tLoss 1.5093e+00 (1.5404e+00)\tAcc@1  60.16 ( 56.79)\tAcc@5  84.38 ( 85.47)\n","Epoch: [20][210/391]\tTime  0.092 ( 0.093)\tLoss 1.9225e+00 (1.5452e+00)\tAcc@1  46.09 ( 56.66)\tAcc@5  78.91 ( 85.46)\n","Epoch: [20][240/391]\tTime  0.096 ( 0.093)\tLoss 1.5484e+00 (1.5518e+00)\tAcc@1  53.91 ( 56.41)\tAcc@5  87.50 ( 85.37)\n","Epoch: [20][270/391]\tTime  0.096 ( 0.093)\tLoss 1.5272e+00 (1.5519e+00)\tAcc@1  56.25 ( 56.39)\tAcc@5  84.38 ( 85.43)\n","Epoch: [20][300/391]\tTime  0.093 ( 0.093)\tLoss 1.4069e+00 (1.5524e+00)\tAcc@1  56.25 ( 56.38)\tAcc@5  89.06 ( 85.42)\n","Epoch: [20][330/391]\tTime  0.091 ( 0.093)\tLoss 1.2944e+00 (1.5583e+00)\tAcc@1  61.72 ( 56.33)\tAcc@5  88.28 ( 85.30)\n","Epoch: [20][360/391]\tTime  0.091 ( 0.093)\tLoss 1.6703e+00 (1.5611e+00)\tAcc@1  57.03 ( 56.30)\tAcc@5  85.16 ( 85.27)\n","Epoch: [20][390/391]\tTime  0.080 ( 0.093)\tLoss 1.6528e+00 (1.5652e+00)\tAcc@1  52.50 ( 56.18)\tAcc@5  88.75 ( 85.22)\n","==> Train Accuracy: Acc@1 56.182 || Acc@5 85.218\n","==> Test Accuracy:  Acc@1 54.510 || Acc@5 83.410\n","==> 38.96 seconds to train this epoch\n","\n","\n","----- epoch: 21, lr: 0.1 -----\n","Epoch: [21][  0/391]\tTime  0.270 ( 0.270)\tLoss 1.7550e+00 (1.7550e+00)\tAcc@1  51.56 ( 51.56)\tAcc@5  82.03 ( 82.03)\n","Epoch: [21][ 30/391]\tTime  0.092 ( 0.099)\tLoss 1.4904e+00 (1.4945e+00)\tAcc@1  57.81 ( 58.06)\tAcc@5  89.84 ( 87.00)\n","Epoch: [21][ 60/391]\tTime  0.091 ( 0.095)\tLoss 1.9030e+00 (1.4860e+00)\tAcc@1  46.88 ( 58.18)\tAcc@5  81.25 ( 87.05)\n","Epoch: [21][ 90/391]\tTime  0.090 ( 0.094)\tLoss 1.7393e+00 (1.5058e+00)\tAcc@1  51.56 ( 57.67)\tAcc@5  82.81 ( 86.44)\n","Epoch: [21][120/391]\tTime  0.092 ( 0.094)\tLoss 1.6842e+00 (1.5080e+00)\tAcc@1  54.69 ( 57.54)\tAcc@5  85.16 ( 86.35)\n","Epoch: [21][150/391]\tTime  0.097 ( 0.094)\tLoss 1.3413e+00 (1.5150e+00)\tAcc@1  60.94 ( 57.33)\tAcc@5  89.84 ( 86.27)\n","Epoch: [21][180/391]\tTime  0.090 ( 0.093)\tLoss 1.4348e+00 (1.5238e+00)\tAcc@1  56.25 ( 57.13)\tAcc@5  89.06 ( 86.18)\n","Epoch: [21][210/391]\tTime  0.092 ( 0.093)\tLoss 1.5997e+00 (1.5359e+00)\tAcc@1  50.00 ( 56.92)\tAcc@5  83.59 ( 85.93)\n","Epoch: [21][240/391]\tTime  0.092 ( 0.093)\tLoss 1.6613e+00 (1.5343e+00)\tAcc@1  53.12 ( 56.98)\tAcc@5  83.59 ( 85.97)\n","Epoch: [21][270/391]\tTime  0.098 ( 0.093)\tLoss 1.5355e+00 (1.5339e+00)\tAcc@1  58.59 ( 56.99)\tAcc@5  85.16 ( 85.94)\n","Epoch: [21][300/391]\tTime  0.092 ( 0.093)\tLoss 1.5949e+00 (1.5355e+00)\tAcc@1  50.00 ( 57.00)\tAcc@5  87.50 ( 85.92)\n","Epoch: [21][330/391]\tTime  0.093 ( 0.093)\tLoss 1.6149e+00 (1.5426e+00)\tAcc@1  57.81 ( 56.85)\tAcc@5  84.38 ( 85.78)\n","Epoch: [21][360/391]\tTime  0.089 ( 0.093)\tLoss 1.7837e+00 (1.5462e+00)\tAcc@1  51.56 ( 56.65)\tAcc@5  83.59 ( 85.74)\n","Epoch: [21][390/391]\tTime  0.081 ( 0.093)\tLoss 1.7213e+00 (1.5495e+00)\tAcc@1  52.50 ( 56.62)\tAcc@5  82.50 ( 85.63)\n","==> Train Accuracy: Acc@1 56.616 || Acc@5 85.630\n","==> Test Accuracy:  Acc@1 47.700 || Acc@5 77.060\n","==> 38.91 seconds to train this epoch\n","\n","\n","----- epoch: 22, lr: 0.1 -----\n","Epoch: [22][  0/391]\tTime  0.248 ( 0.248)\tLoss 1.5419e+00 (1.5419e+00)\tAcc@1  57.03 ( 57.03)\tAcc@5  86.72 ( 86.72)\n","Epoch: [22][ 30/391]\tTime  0.092 ( 0.098)\tLoss 1.4845e+00 (1.4554e+00)\tAcc@1  58.59 ( 58.85)\tAcc@5  87.50 ( 87.20)\n","Epoch: [22][ 60/391]\tTime  0.091 ( 0.095)\tLoss 1.7186e+00 (1.4954e+00)\tAcc@1  55.47 ( 58.04)\tAcc@5  82.81 ( 86.62)\n","Epoch: [22][ 90/391]\tTime  0.091 ( 0.094)\tLoss 1.5845e+00 (1.5078e+00)\tAcc@1  55.47 ( 57.44)\tAcc@5  85.16 ( 86.32)\n","Epoch: [22][120/391]\tTime  0.092 ( 0.094)\tLoss 1.1654e+00 (1.5076e+00)\tAcc@1  72.66 ( 57.88)\tAcc@5  89.06 ( 86.36)\n","Epoch: [22][150/391]\tTime  0.091 ( 0.093)\tLoss 1.5398e+00 (1.5102e+00)\tAcc@1  53.12 ( 57.81)\tAcc@5  86.72 ( 86.23)\n","Epoch: [22][180/391]\tTime  0.096 ( 0.093)\tLoss 1.6869e+00 (1.5205e+00)\tAcc@1  53.91 ( 57.49)\tAcc@5  81.25 ( 86.14)\n","Epoch: [22][210/391]\tTime  0.091 ( 0.093)\tLoss 1.3582e+00 (1.5243e+00)\tAcc@1  68.75 ( 57.56)\tAcc@5  87.50 ( 85.95)\n","Epoch: [22][240/391]\tTime  0.090 ( 0.093)\tLoss 1.6713e+00 (1.5338e+00)\tAcc@1  50.78 ( 57.36)\tAcc@5  84.38 ( 85.72)\n","Epoch: [22][270/391]\tTime  0.094 ( 0.093)\tLoss 1.6859e+00 (1.5333e+00)\tAcc@1  53.12 ( 57.39)\tAcc@5  84.38 ( 85.67)\n","Epoch: [22][300/391]\tTime  0.091 ( 0.093)\tLoss 1.4398e+00 (1.5306e+00)\tAcc@1  55.47 ( 57.43)\tAcc@5  88.28 ( 85.72)\n","Epoch: [22][330/391]\tTime  0.090 ( 0.093)\tLoss 1.5194e+00 (1.5283e+00)\tAcc@1  55.47 ( 57.52)\tAcc@5  88.28 ( 85.75)\n","Epoch: [22][360/391]\tTime  0.092 ( 0.093)\tLoss 1.4972e+00 (1.5295e+00)\tAcc@1  57.03 ( 57.52)\tAcc@5  88.28 ( 85.68)\n","Epoch: [22][390/391]\tTime  0.082 ( 0.093)\tLoss 1.6584e+00 (1.5317e+00)\tAcc@1  58.75 ( 57.43)\tAcc@5  82.50 ( 85.69)\n","==> Train Accuracy: Acc@1 57.434 || Acc@5 85.686\n","==> Test Accuracy:  Acc@1 50.090 || Acc@5 80.700\n","==> 38.86 seconds to train this epoch\n","\n","\n","----- epoch: 23, lr: 0.1 -----\n","Epoch: [23][  0/391]\tTime  0.264 ( 0.264)\tLoss 1.4112e+00 (1.4112e+00)\tAcc@1  60.16 ( 60.16)\tAcc@5  88.28 ( 88.28)\n","Epoch: [23][ 30/391]\tTime  0.090 ( 0.098)\tLoss 1.4573e+00 (1.4457e+00)\tAcc@1  57.81 ( 58.49)\tAcc@5  89.84 ( 88.03)\n","Epoch: [23][ 60/391]\tTime  0.091 ( 0.096)\tLoss 1.3243e+00 (1.4581e+00)\tAcc@1  59.38 ( 58.25)\tAcc@5  89.84 ( 87.72)\n","Epoch: [23][ 90/391]\tTime  0.092 ( 0.095)\tLoss 1.4370e+00 (1.4920e+00)\tAcc@1  64.06 ( 57.30)\tAcc@5  88.28 ( 87.15)\n","Epoch: [23][120/391]\tTime  0.093 ( 0.094)\tLoss 1.4284e+00 (1.4852e+00)\tAcc@1  57.81 ( 57.64)\tAcc@5  85.94 ( 86.98)\n","Epoch: [23][150/391]\tTime  0.091 ( 0.094)\tLoss 1.1921e+00 (1.4853e+00)\tAcc@1  69.53 ( 57.75)\tAcc@5  92.19 ( 86.91)\n","Epoch: [23][180/391]\tTime  0.093 ( 0.094)\tLoss 1.4289e+00 (1.4986e+00)\tAcc@1  57.81 ( 57.53)\tAcc@5  86.72 ( 86.67)\n","Epoch: [23][210/391]\tTime  0.095 ( 0.093)\tLoss 1.4566e+00 (1.4995e+00)\tAcc@1  61.72 ( 57.57)\tAcc@5  88.28 ( 86.65)\n","Epoch: [23][240/391]\tTime  0.093 ( 0.093)\tLoss 1.5608e+00 (1.5029e+00)\tAcc@1  57.81 ( 57.57)\tAcc@5  85.94 ( 86.54)\n","Epoch: [23][270/391]\tTime  0.092 ( 0.093)\tLoss 1.6771e+00 (1.5106e+00)\tAcc@1  55.47 ( 57.37)\tAcc@5  85.94 ( 86.38)\n","Epoch: [23][300/391]\tTime  0.096 ( 0.093)\tLoss 1.4594e+00 (1.5147e+00)\tAcc@1  55.47 ( 57.29)\tAcc@5  86.72 ( 86.28)\n","Epoch: [23][330/391]\tTime  0.089 ( 0.093)\tLoss 1.6378e+00 (1.5184e+00)\tAcc@1  55.47 ( 57.25)\tAcc@5  82.03 ( 86.18)\n","Epoch: [23][360/391]\tTime  0.093 ( 0.093)\tLoss 1.6067e+00 (1.5210e+00)\tAcc@1  50.78 ( 57.18)\tAcc@5  86.72 ( 86.11)\n","Epoch: [23][390/391]\tTime  0.082 ( 0.093)\tLoss 1.4212e+00 (1.5219e+00)\tAcc@1  52.50 ( 57.21)\tAcc@5  92.50 ( 86.07)\n","==> Train Accuracy: Acc@1 57.208 || Acc@5 86.074\n","==> Test Accuracy:  Acc@1 51.230 || Acc@5 80.570\n","==> 38.88 seconds to train this epoch\n","\n","\n","----- epoch: 24, lr: 0.1 -----\n","Epoch: [24][  0/391]\tTime  0.291 ( 0.291)\tLoss 1.4978e+00 (1.4978e+00)\tAcc@1  56.25 ( 56.25)\tAcc@5  82.81 ( 82.81)\n","Epoch: [24][ 30/391]\tTime  0.096 ( 0.100)\tLoss 1.4802e+00 (1.4458e+00)\tAcc@1  62.50 ( 59.30)\tAcc@5  82.03 ( 86.69)\n","Epoch: [24][ 60/391]\tTime  0.098 ( 0.096)\tLoss 1.4964e+00 (1.4343e+00)\tAcc@1  57.03 ( 59.67)\tAcc@5  88.28 ( 87.17)\n","Epoch: [24][ 90/391]\tTime  0.076 ( 0.095)\tLoss 1.3857e+00 (1.4655e+00)\tAcc@1  60.94 ( 58.89)\tAcc@5  86.72 ( 86.62)\n","Epoch: [24][120/391]\tTime  0.092 ( 0.094)\tLoss 1.2730e+00 (1.4740e+00)\tAcc@1  61.72 ( 58.81)\tAcc@5  91.41 ( 86.45)\n","Epoch: [24][150/391]\tTime  0.094 ( 0.094)\tLoss 1.4255e+00 (1.4758e+00)\tAcc@1  56.25 ( 58.54)\tAcc@5  89.06 ( 86.51)\n","Epoch: [24][180/391]\tTime  0.092 ( 0.093)\tLoss 1.6425e+00 (1.4831e+00)\tAcc@1  53.12 ( 58.30)\tAcc@5  82.81 ( 86.39)\n","Epoch: [24][210/391]\tTime  0.098 ( 0.093)\tLoss 1.4131e+00 (1.4884e+00)\tAcc@1  57.03 ( 58.12)\tAcc@5  87.50 ( 86.38)\n","Epoch: [24][240/391]\tTime  0.091 ( 0.093)\tLoss 1.6762e+00 (1.4937e+00)\tAcc@1  52.34 ( 57.99)\tAcc@5  85.16 ( 86.27)\n","Epoch: [24][270/391]\tTime  0.091 ( 0.093)\tLoss 1.4895e+00 (1.4956e+00)\tAcc@1  53.12 ( 57.98)\tAcc@5  89.84 ( 86.23)\n","Epoch: [24][300/391]\tTime  0.092 ( 0.093)\tLoss 1.2537e+00 (1.4984e+00)\tAcc@1  64.84 ( 57.90)\tAcc@5  90.62 ( 86.21)\n","Epoch: [24][330/391]\tTime  0.091 ( 0.093)\tLoss 1.4008e+00 (1.5000e+00)\tAcc@1  54.69 ( 57.81)\tAcc@5  87.50 ( 86.22)\n","Epoch: [24][360/391]\tTime  0.090 ( 0.093)\tLoss 1.7468e+00 (1.5027e+00)\tAcc@1  54.69 ( 57.77)\tAcc@5  82.81 ( 86.20)\n","Epoch: [24][390/391]\tTime  0.082 ( 0.093)\tLoss 1.4672e+00 (1.5045e+00)\tAcc@1  62.50 ( 57.74)\tAcc@5  86.25 ( 86.23)\n","==> Train Accuracy: Acc@1 57.742 || Acc@5 86.230\n","==> Test Accuracy:  Acc@1 55.030 || Acc@5 83.160\n","==> 38.89 seconds to train this epoch\n","\n","\n","----- epoch: 25, lr: 0.1 -----\n","Epoch: [25][  0/391]\tTime  0.252 ( 0.252)\tLoss 1.5077e+00 (1.5077e+00)\tAcc@1  64.06 ( 64.06)\tAcc@5  82.03 ( 82.03)\n","Epoch: [25][ 30/391]\tTime  0.092 ( 0.098)\tLoss 1.5084e+00 (1.4346e+00)\tAcc@1  61.72 ( 59.43)\tAcc@5  87.50 ( 87.35)\n","Epoch: [25][ 60/391]\tTime  0.095 ( 0.095)\tLoss 1.3538e+00 (1.4400e+00)\tAcc@1  65.62 ( 59.20)\tAcc@5  85.16 ( 87.06)\n","Epoch: [25][ 90/391]\tTime  0.091 ( 0.094)\tLoss 1.1957e+00 (1.4544e+00)\tAcc@1  65.62 ( 58.65)\tAcc@5  90.62 ( 87.11)\n","Epoch: [25][120/391]\tTime  0.093 ( 0.094)\tLoss 1.4988e+00 (1.4558e+00)\tAcc@1  58.59 ( 58.73)\tAcc@5  82.81 ( 86.96)\n","Epoch: [25][150/391]\tTime  0.090 ( 0.093)\tLoss 1.5965e+00 (1.4654e+00)\tAcc@1  57.81 ( 58.60)\tAcc@5  84.38 ( 86.76)\n","Epoch: [25][180/391]\tTime  0.092 ( 0.093)\tLoss 1.4191e+00 (1.4685e+00)\tAcc@1  64.06 ( 58.62)\tAcc@5  84.38 ( 86.73)\n","Epoch: [25][210/391]\tTime  0.091 ( 0.093)\tLoss 1.3415e+00 (1.4721e+00)\tAcc@1  64.84 ( 58.43)\tAcc@5  90.62 ( 86.68)\n","Epoch: [25][240/391]\tTime  0.092 ( 0.093)\tLoss 1.5010e+00 (1.4823e+00)\tAcc@1  64.84 ( 58.14)\tAcc@5  85.94 ( 86.52)\n","Epoch: [25][270/391]\tTime  0.090 ( 0.093)\tLoss 1.5598e+00 (1.4872e+00)\tAcc@1  55.47 ( 58.03)\tAcc@5  85.94 ( 86.43)\n","Epoch: [25][300/391]\tTime  0.091 ( 0.093)\tLoss 1.4829e+00 (1.4943e+00)\tAcc@1  58.59 ( 57.95)\tAcc@5  87.50 ( 86.29)\n","Epoch: [25][330/391]\tTime  0.091 ( 0.093)\tLoss 1.2796e+00 (1.4992e+00)\tAcc@1  64.84 ( 57.80)\tAcc@5  89.06 ( 86.20)\n","Epoch: [25][360/391]\tTime  0.092 ( 0.093)\tLoss 1.4503e+00 (1.4994e+00)\tAcc@1  64.84 ( 57.80)\tAcc@5  90.62 ( 86.20)\n","Epoch: [25][390/391]\tTime  0.082 ( 0.093)\tLoss 1.3990e+00 (1.4972e+00)\tAcc@1  58.75 ( 57.92)\tAcc@5  82.50 ( 86.19)\n","==> Train Accuracy: Acc@1 57.924 || Acc@5 86.186\n","==> Test Accuracy:  Acc@1 54.730 || Acc@5 84.110\n","==> 38.89 seconds to train this epoch\n","\n","\n","----- epoch: 26, lr: 0.1 -----\n","Epoch: [26][  0/391]\tTime  0.262 ( 0.262)\tLoss 1.4522e+00 (1.4522e+00)\tAcc@1  60.16 ( 60.16)\tAcc@5  87.50 ( 87.50)\n","Epoch: [26][ 30/391]\tTime  0.095 ( 0.099)\tLoss 1.3379e+00 (1.3924e+00)\tAcc@1  60.94 ( 60.41)\tAcc@5  92.19 ( 88.48)\n","Epoch: [26][ 60/391]\tTime  0.090 ( 0.096)\tLoss 1.5137e+00 (1.3965e+00)\tAcc@1  53.12 ( 60.09)\tAcc@5  83.59 ( 88.35)\n","Epoch: [26][ 90/391]\tTime  0.093 ( 0.095)\tLoss 1.3125e+00 (1.4154e+00)\tAcc@1  60.94 ( 59.63)\tAcc@5  90.62 ( 88.08)\n","Epoch: [26][120/391]\tTime  0.093 ( 0.094)\tLoss 1.4844e+00 (1.4243e+00)\tAcc@1  59.38 ( 59.59)\tAcc@5  87.50 ( 87.81)\n","Epoch: [26][150/391]\tTime  0.092 ( 0.094)\tLoss 1.3939e+00 (1.4352e+00)\tAcc@1  62.50 ( 59.30)\tAcc@5  87.50 ( 87.55)\n","Epoch: [26][180/391]\tTime  0.091 ( 0.093)\tLoss 1.5308e+00 (1.4517e+00)\tAcc@1  53.12 ( 58.87)\tAcc@5  84.38 ( 87.31)\n","Epoch: [26][210/391]\tTime  0.092 ( 0.093)\tLoss 1.5974e+00 (1.4538e+00)\tAcc@1  57.03 ( 58.93)\tAcc@5  84.38 ( 87.27)\n","Epoch: [26][240/391]\tTime  0.090 ( 0.093)\tLoss 1.5092e+00 (1.4600e+00)\tAcc@1  55.47 ( 58.79)\tAcc@5  86.72 ( 87.16)\n","Epoch: [26][270/391]\tTime  0.096 ( 0.093)\tLoss 1.3579e+00 (1.4646e+00)\tAcc@1  64.84 ( 58.71)\tAcc@5  86.72 ( 87.01)\n","Epoch: [26][300/391]\tTime  0.093 ( 0.093)\tLoss 1.8151e+00 (1.4666e+00)\tAcc@1  52.34 ( 58.67)\tAcc@5  83.59 ( 86.94)\n","Epoch: [26][330/391]\tTime  0.091 ( 0.093)\tLoss 1.3512e+00 (1.4704e+00)\tAcc@1  64.06 ( 58.60)\tAcc@5  85.94 ( 86.84)\n","Epoch: [26][360/391]\tTime  0.091 ( 0.093)\tLoss 1.3755e+00 (1.4728e+00)\tAcc@1  57.03 ( 58.50)\tAcc@5  91.41 ( 86.81)\n","Epoch: [26][390/391]\tTime  0.082 ( 0.093)\tLoss 1.4732e+00 (1.4722e+00)\tAcc@1  56.25 ( 58.48)\tAcc@5  95.00 ( 86.83)\n","==> Train Accuracy: Acc@1 58.478 || Acc@5 86.828\n","==> Test Accuracy:  Acc@1 53.720 || Acc@5 81.610\n","==> 38.83 seconds to train this epoch\n","\n","\n","----- epoch: 27, lr: 0.1 -----\n","Epoch: [27][  0/391]\tTime  0.252 ( 0.252)\tLoss 1.3197e+00 (1.3197e+00)\tAcc@1  65.62 ( 65.62)\tAcc@5  91.41 ( 91.41)\n","Epoch: [27][ 30/391]\tTime  0.097 ( 0.098)\tLoss 1.6264e+00 (1.4571e+00)\tAcc@1  54.69 ( 58.42)\tAcc@5  85.16 ( 86.11)\n","Epoch: [27][ 60/391]\tTime  0.090 ( 0.095)\tLoss 1.4398e+00 (1.4452e+00)\tAcc@1  59.38 ( 59.03)\tAcc@5  88.28 ( 86.80)\n","Epoch: [27][ 90/391]\tTime  0.094 ( 0.094)\tLoss 1.3336e+00 (1.4470e+00)\tAcc@1  60.16 ( 59.29)\tAcc@5  89.84 ( 86.91)\n","Epoch: [27][120/391]\tTime  0.093 ( 0.094)\tLoss 1.4279e+00 (1.4553e+00)\tAcc@1  58.59 ( 59.01)\tAcc@5  83.59 ( 86.67)\n","Epoch: [27][150/391]\tTime  0.094 ( 0.093)\tLoss 1.4895e+00 (1.4553e+00)\tAcc@1  57.81 ( 59.02)\tAcc@5  85.94 ( 86.93)\n","Epoch: [27][180/391]\tTime  0.093 ( 0.093)\tLoss 1.6030e+00 (1.4495e+00)\tAcc@1  52.34 ( 59.09)\tAcc@5  83.59 ( 87.00)\n","Epoch: [27][210/391]\tTime  0.090 ( 0.093)\tLoss 1.3131e+00 (1.4510e+00)\tAcc@1  65.62 ( 59.01)\tAcc@5  89.06 ( 87.02)\n","Epoch: [27][240/391]\tTime  0.088 ( 0.093)\tLoss 1.4031e+00 (1.4609e+00)\tAcc@1  57.03 ( 58.80)\tAcc@5  86.72 ( 86.87)\n","Epoch: [27][270/391]\tTime  0.094 ( 0.093)\tLoss 1.5781e+00 (1.4654e+00)\tAcc@1  57.03 ( 58.72)\tAcc@5  84.38 ( 86.77)\n","Epoch: [27][300/391]\tTime  0.094 ( 0.093)\tLoss 1.7166e+00 (1.4677e+00)\tAcc@1  51.56 ( 58.66)\tAcc@5  85.94 ( 86.81)\n","Epoch: [27][330/391]\tTime  0.093 ( 0.093)\tLoss 1.5878e+00 (1.4646e+00)\tAcc@1  57.03 ( 58.77)\tAcc@5  83.59 ( 86.84)\n","Epoch: [27][360/391]\tTime  0.094 ( 0.093)\tLoss 1.5630e+00 (1.4652e+00)\tAcc@1  57.03 ( 58.72)\tAcc@5  82.81 ( 86.87)\n","Epoch: [27][390/391]\tTime  0.081 ( 0.092)\tLoss 1.7953e+00 (1.4702e+00)\tAcc@1  55.00 ( 58.63)\tAcc@5  76.25 ( 86.78)\n","==> Train Accuracy: Acc@1 58.628 || Acc@5 86.776\n","==> Test Accuracy:  Acc@1 56.000 || Acc@5 84.760\n","==> 38.82 seconds to train this epoch\n","\n","\n","----- epoch: 28, lr: 0.1 -----\n","Epoch: [28][  0/391]\tTime  0.248 ( 0.248)\tLoss 1.5775e+00 (1.5775e+00)\tAcc@1  51.56 ( 51.56)\tAcc@5  85.16 ( 85.16)\n","Epoch: [28][ 30/391]\tTime  0.093 ( 0.099)\tLoss 1.6757e+00 (1.4101e+00)\tAcc@1  56.25 ( 60.16)\tAcc@5  82.03 ( 87.98)\n","Epoch: [28][ 60/391]\tTime  0.090 ( 0.096)\tLoss 1.7265e+00 (1.4074e+00)\tAcc@1  50.00 ( 60.35)\tAcc@5  85.16 ( 87.88)\n","Epoch: [28][ 90/391]\tTime  0.092 ( 0.095)\tLoss 1.3970e+00 (1.4136e+00)\tAcc@1  56.25 ( 59.98)\tAcc@5  87.50 ( 87.66)\n","Epoch: [28][120/391]\tTime  0.093 ( 0.094)\tLoss 1.4051e+00 (1.4212e+00)\tAcc@1  62.50 ( 59.99)\tAcc@5  85.94 ( 87.35)\n","Epoch: [28][150/391]\tTime  0.092 ( 0.094)\tLoss 1.4253e+00 (1.4383e+00)\tAcc@1  64.84 ( 59.46)\tAcc@5  88.28 ( 87.07)\n","Epoch: [28][180/391]\tTime  0.096 ( 0.093)\tLoss 1.4656e+00 (1.4465e+00)\tAcc@1  59.38 ( 59.31)\tAcc@5  83.59 ( 86.93)\n","Epoch: [28][210/391]\tTime  0.094 ( 0.093)\tLoss 1.2394e+00 (1.4465e+00)\tAcc@1  63.28 ( 59.29)\tAcc@5  87.50 ( 86.90)\n","Epoch: [28][240/391]\tTime  0.091 ( 0.093)\tLoss 1.2020e+00 (1.4464e+00)\tAcc@1  69.53 ( 59.24)\tAcc@5  89.06 ( 86.95)\n","Epoch: [28][270/391]\tTime  0.087 ( 0.093)\tLoss 1.3973e+00 (1.4463e+00)\tAcc@1  60.16 ( 59.26)\tAcc@5  84.38 ( 86.97)\n","Epoch: [28][300/391]\tTime  0.097 ( 0.093)\tLoss 1.3122e+00 (1.4492e+00)\tAcc@1  60.16 ( 59.19)\tAcc@5  89.06 ( 86.90)\n","Epoch: [28][330/391]\tTime  0.093 ( 0.093)\tLoss 1.4667e+00 (1.4506e+00)\tAcc@1  57.81 ( 59.12)\tAcc@5  85.94 ( 86.90)\n","Epoch: [28][360/391]\tTime  0.096 ( 0.093)\tLoss 1.6244e+00 (1.4545e+00)\tAcc@1  55.47 ( 58.99)\tAcc@5  87.50 ( 86.88)\n","Epoch: [28][390/391]\tTime  0.082 ( 0.093)\tLoss 1.4952e+00 (1.4560e+00)\tAcc@1  53.75 ( 58.95)\tAcc@5  85.00 ( 86.88)\n","==> Train Accuracy: Acc@1 58.946 || Acc@5 86.876\n","==> Test Accuracy:  Acc@1 56.440 || Acc@5 84.700\n","==> 38.93 seconds to train this epoch\n","\n","\n","----- epoch: 29, lr: 0.1 -----\n","Epoch: [29][  0/391]\tTime  0.257 ( 0.257)\tLoss 1.4868e+00 (1.4868e+00)\tAcc@1  60.94 ( 60.94)\tAcc@5  85.16 ( 85.16)\n","Epoch: [29][ 30/391]\tTime  0.094 ( 0.098)\tLoss 1.5701e+00 (1.4059e+00)\tAcc@1  56.25 ( 60.58)\tAcc@5  83.59 ( 87.53)\n","Epoch: [29][ 60/391]\tTime  0.095 ( 0.096)\tLoss 1.7755e+00 (1.4216e+00)\tAcc@1  51.56 ( 60.14)\tAcc@5  83.59 ( 87.28)\n","Epoch: [29][ 90/391]\tTime  0.092 ( 0.094)\tLoss 1.5757e+00 (1.4352e+00)\tAcc@1  57.81 ( 59.65)\tAcc@5  81.25 ( 87.30)\n","Epoch: [29][120/391]\tTime  0.091 ( 0.094)\tLoss 1.3257e+00 (1.4368e+00)\tAcc@1  63.28 ( 59.50)\tAcc@5  89.84 ( 87.22)\n","Epoch: [29][150/391]\tTime  0.103 ( 0.094)\tLoss 1.4529e+00 (1.4333e+00)\tAcc@1  57.81 ( 59.80)\tAcc@5  87.50 ( 87.24)\n","Epoch: [29][180/391]\tTime  0.093 ( 0.093)\tLoss 1.3779e+00 (1.4265e+00)\tAcc@1  57.81 ( 59.79)\tAcc@5  87.50 ( 87.36)\n","Epoch: [29][210/391]\tTime  0.092 ( 0.093)\tLoss 1.3152e+00 (1.4348e+00)\tAcc@1  60.16 ( 59.49)\tAcc@5  87.50 ( 87.23)\n","Epoch: [29][240/391]\tTime  0.089 ( 0.093)\tLoss 1.4713e+00 (1.4415e+00)\tAcc@1  55.47 ( 59.40)\tAcc@5  89.84 ( 87.11)\n","Epoch: [29][270/391]\tTime  0.097 ( 0.093)\tLoss 1.6873e+00 (1.4419e+00)\tAcc@1  60.16 ( 59.41)\tAcc@5  85.16 ( 87.19)\n","Epoch: [29][300/391]\tTime  0.092 ( 0.093)\tLoss 1.5921e+00 (1.4435e+00)\tAcc@1  61.72 ( 59.35)\tAcc@5  85.16 ( 87.13)\n","Epoch: [29][330/391]\tTime  0.091 ( 0.093)\tLoss 1.3622e+00 (1.4483e+00)\tAcc@1  58.59 ( 59.22)\tAcc@5  91.41 ( 87.03)\n","Epoch: [29][360/391]\tTime  0.098 ( 0.093)\tLoss 1.3527e+00 (1.4515e+00)\tAcc@1  63.28 ( 59.18)\tAcc@5  87.50 ( 87.01)\n","Epoch: [29][390/391]\tTime  0.081 ( 0.093)\tLoss 1.2041e+00 (1.4522e+00)\tAcc@1  63.75 ( 59.08)\tAcc@5  92.50 ( 87.02)\n","==> Train Accuracy: Acc@1 59.080 || Acc@5 87.022\n","==> Test Accuracy:  Acc@1 55.890 || Acc@5 84.540\n","==> 38.88 seconds to train this epoch\n","\n","\n","----- epoch: 30, lr: 0.1 -----\n","Epoch: [30][  0/391]\tTime  0.247 ( 0.247)\tLoss 1.4518e+00 (1.4518e+00)\tAcc@1  56.25 ( 56.25)\tAcc@5  85.94 ( 85.94)\n","Epoch: [30][ 30/391]\tTime  0.094 ( 0.098)\tLoss 1.3793e+00 (1.4080e+00)\tAcc@1  61.72 ( 60.89)\tAcc@5  84.38 ( 87.85)\n","Epoch: [30][ 60/391]\tTime  0.095 ( 0.095)\tLoss 1.5550e+00 (1.4018e+00)\tAcc@1  57.81 ( 61.05)\tAcc@5  83.59 ( 87.78)\n","Epoch: [30][ 90/391]\tTime  0.091 ( 0.094)\tLoss 1.4010e+00 (1.4080e+00)\tAcc@1  57.81 ( 60.69)\tAcc@5  87.50 ( 87.59)\n","Epoch: [30][120/391]\tTime  0.092 ( 0.093)\tLoss 1.2318e+00 (1.4150e+00)\tAcc@1  65.62 ( 60.45)\tAcc@5  89.06 ( 87.40)\n","Epoch: [30][150/391]\tTime  0.094 ( 0.093)\tLoss 1.3686e+00 (1.4209e+00)\tAcc@1  62.50 ( 60.31)\tAcc@5  89.06 ( 87.38)\n","Epoch: [30][180/391]\tTime  0.094 ( 0.093)\tLoss 1.5513e+00 (1.4225e+00)\tAcc@1  60.16 ( 60.20)\tAcc@5  85.94 ( 87.39)\n","Epoch: [30][210/391]\tTime  0.090 ( 0.093)\tLoss 1.5841e+00 (1.4321e+00)\tAcc@1  58.59 ( 59.86)\tAcc@5  85.16 ( 87.31)\n","Epoch: [30][240/391]\tTime  0.091 ( 0.093)\tLoss 1.6168e+00 (1.4401e+00)\tAcc@1  52.34 ( 59.55)\tAcc@5  88.28 ( 87.19)\n","Epoch: [30][270/391]\tTime  0.098 ( 0.093)\tLoss 1.4946e+00 (1.4434e+00)\tAcc@1  54.69 ( 59.40)\tAcc@5  86.72 ( 87.12)\n","Epoch: [30][300/391]\tTime  0.096 ( 0.093)\tLoss 1.3125e+00 (1.4442e+00)\tAcc@1  58.59 ( 59.41)\tAcc@5  89.06 ( 87.08)\n","Epoch: [30][330/391]\tTime  0.090 ( 0.093)\tLoss 1.5432e+00 (1.4468e+00)\tAcc@1  53.91 ( 59.37)\tAcc@5  89.06 ( 86.99)\n","Epoch: [30][360/391]\tTime  0.094 ( 0.093)\tLoss 1.4438e+00 (1.4472e+00)\tAcc@1  64.06 ( 59.42)\tAcc@5  81.25 ( 86.96)\n","Epoch: [30][390/391]\tTime  0.082 ( 0.093)\tLoss 1.4750e+00 (1.4497e+00)\tAcc@1  56.25 ( 59.34)\tAcc@5  82.50 ( 86.90)\n","==> Train Accuracy: Acc@1 59.344 || Acc@5 86.904\n","==> Test Accuracy:  Acc@1 54.050 || Acc@5 84.100\n","==> 38.95 seconds to train this epoch\n","\n","\n","----- epoch: 31, lr: 0.1 -----\n","Epoch: [31][  0/391]\tTime  0.269 ( 0.269)\tLoss 1.6305e+00 (1.6305e+00)\tAcc@1  52.34 ( 52.34)\tAcc@5  82.03 ( 82.03)\n","Epoch: [31][ 30/391]\tTime  0.091 ( 0.098)\tLoss 1.4023e+00 (1.3254e+00)\tAcc@1  58.59 ( 60.86)\tAcc@5  88.28 ( 89.09)\n","Epoch: [31][ 60/391]\tTime  0.092 ( 0.095)\tLoss 1.3439e+00 (1.3623e+00)\tAcc@1  60.94 ( 60.23)\tAcc@5  92.97 ( 89.00)\n","Epoch: [31][ 90/391]\tTime  0.089 ( 0.094)\tLoss 1.1190e+00 (1.3805e+00)\tAcc@1  64.84 ( 60.20)\tAcc@5  91.41 ( 88.36)\n","Epoch: [31][120/391]\tTime  0.091 ( 0.094)\tLoss 1.4096e+00 (1.3861e+00)\tAcc@1  57.81 ( 60.19)\tAcc@5  89.84 ( 88.27)\n","Epoch: [31][150/391]\tTime  0.091 ( 0.093)\tLoss 1.2495e+00 (1.3954e+00)\tAcc@1  64.06 ( 59.99)\tAcc@5  93.75 ( 88.03)\n","Epoch: [31][180/391]\tTime  0.091 ( 0.093)\tLoss 1.4236e+00 (1.4094e+00)\tAcc@1  61.72 ( 59.75)\tAcc@5  86.72 ( 87.74)\n","Epoch: [31][210/391]\tTime  0.092 ( 0.093)\tLoss 1.3826e+00 (1.4180e+00)\tAcc@1  60.16 ( 59.66)\tAcc@5  86.72 ( 87.62)\n","Epoch: [31][240/391]\tTime  0.092 ( 0.093)\tLoss 1.3188e+00 (1.4153e+00)\tAcc@1  63.28 ( 59.76)\tAcc@5  88.28 ( 87.62)\n","Epoch: [31][270/391]\tTime  0.092 ( 0.093)\tLoss 1.4557e+00 (1.4218e+00)\tAcc@1  57.03 ( 59.62)\tAcc@5  89.06 ( 87.57)\n","Epoch: [31][300/391]\tTime  0.105 ( 0.093)\tLoss 1.5510e+00 (1.4281e+00)\tAcc@1  59.38 ( 59.48)\tAcc@5  82.81 ( 87.45)\n","Epoch: [31][330/391]\tTime  0.088 ( 0.093)\tLoss 1.3761e+00 (1.4280e+00)\tAcc@1  61.72 ( 59.49)\tAcc@5  87.50 ( 87.37)\n","Epoch: [31][360/391]\tTime  0.091 ( 0.093)\tLoss 1.5496e+00 (1.4317e+00)\tAcc@1  58.59 ( 59.39)\tAcc@5  84.38 ( 87.28)\n","Epoch: [31][390/391]\tTime  0.082 ( 0.093)\tLoss 1.4806e+00 (1.4338e+00)\tAcc@1  61.25 ( 59.42)\tAcc@5  86.25 ( 87.25)\n","==> Train Accuracy: Acc@1 59.416 || Acc@5 87.248\n","==> Test Accuracy:  Acc@1 55.330 || Acc@5 84.210\n","==> 38.87 seconds to train this epoch\n","\n","\n","----- epoch: 32, lr: 0.1 -----\n","Epoch: [32][  0/391]\tTime  0.287 ( 0.287)\tLoss 1.2066e+00 (1.2066e+00)\tAcc@1  64.06 ( 64.06)\tAcc@5  93.75 ( 93.75)\n","Epoch: [32][ 30/391]\tTime  0.092 ( 0.099)\tLoss 1.2363e+00 (1.3276e+00)\tAcc@1  60.94 ( 62.42)\tAcc@5  93.75 ( 88.61)\n","Epoch: [32][ 60/391]\tTime  0.093 ( 0.095)\tLoss 1.3654e+00 (1.3445e+00)\tAcc@1  63.28 ( 61.86)\tAcc@5  89.06 ( 88.40)\n","Epoch: [32][ 90/391]\tTime  0.092 ( 0.094)\tLoss 1.3420e+00 (1.3596e+00)\tAcc@1  58.59 ( 61.52)\tAcc@5  86.72 ( 88.20)\n","Epoch: [32][120/391]\tTime  0.090 ( 0.094)\tLoss 1.4420e+00 (1.3621e+00)\tAcc@1  62.50 ( 61.44)\tAcc@5  88.28 ( 88.13)\n","Epoch: [32][150/391]\tTime  0.097 ( 0.094)\tLoss 1.6394e+00 (1.3890e+00)\tAcc@1  57.81 ( 60.62)\tAcc@5  85.16 ( 87.81)\n","Epoch: [32][180/391]\tTime  0.094 ( 0.093)\tLoss 1.4111e+00 (1.4033e+00)\tAcc@1  64.84 ( 60.42)\tAcc@5  87.50 ( 87.64)\n","Epoch: [32][210/391]\tTime  0.094 ( 0.093)\tLoss 1.3822e+00 (1.4123e+00)\tAcc@1  55.47 ( 60.17)\tAcc@5  89.84 ( 87.57)\n","Epoch: [32][240/391]\tTime  0.090 ( 0.093)\tLoss 1.4810e+00 (1.4191e+00)\tAcc@1  61.72 ( 59.93)\tAcc@5  89.06 ( 87.51)\n","Epoch: [32][270/391]\tTime  0.094 ( 0.093)\tLoss 1.3200e+00 (1.4206e+00)\tAcc@1  58.59 ( 59.82)\tAcc@5  90.62 ( 87.50)\n","Epoch: [32][300/391]\tTime  0.087 ( 0.093)\tLoss 1.2904e+00 (1.4205e+00)\tAcc@1  64.06 ( 59.90)\tAcc@5  89.06 ( 87.47)\n","Epoch: [32][330/391]\tTime  0.091 ( 0.093)\tLoss 1.4075e+00 (1.4224e+00)\tAcc@1  60.16 ( 59.92)\tAcc@5  89.06 ( 87.46)\n","Epoch: [32][360/391]\tTime  0.088 ( 0.093)\tLoss 1.4576e+00 (1.4267e+00)\tAcc@1  60.94 ( 59.87)\tAcc@5  88.28 ( 87.32)\n","Epoch: [32][390/391]\tTime  0.082 ( 0.093)\tLoss 1.3471e+00 (1.4306e+00)\tAcc@1  58.75 ( 59.73)\tAcc@5  92.50 ( 87.27)\n","==> Train Accuracy: Acc@1 59.728 || Acc@5 87.274\n","==> Test Accuracy:  Acc@1 58.820 || Acc@5 86.360\n","==> 38.82 seconds to train this epoch\n","\n","\n","----- epoch: 33, lr: 0.1 -----\n","Epoch: [33][  0/391]\tTime  0.281 ( 0.281)\tLoss 1.0743e+00 (1.0743e+00)\tAcc@1  68.75 ( 68.75)\tAcc@5  91.41 ( 91.41)\n","Epoch: [33][ 30/391]\tTime  0.096 ( 0.101)\tLoss 1.2581e+00 (1.3628e+00)\tAcc@1  63.28 ( 61.27)\tAcc@5  89.06 ( 88.31)\n","Epoch: [33][ 60/391]\tTime  0.091 ( 0.097)\tLoss 1.2829e+00 (1.3670e+00)\tAcc@1  64.84 ( 61.17)\tAcc@5  89.84 ( 87.96)\n","Epoch: [33][ 90/391]\tTime  0.088 ( 0.095)\tLoss 1.3894e+00 (1.3749e+00)\tAcc@1  59.38 ( 60.91)\tAcc@5  91.41 ( 87.77)\n","Epoch: [33][120/391]\tTime  0.093 ( 0.095)\tLoss 1.1197e+00 (1.3750e+00)\tAcc@1  64.84 ( 60.85)\tAcc@5  92.19 ( 87.89)\n","Epoch: [33][150/391]\tTime  0.094 ( 0.094)\tLoss 1.3402e+00 (1.3877e+00)\tAcc@1  63.28 ( 60.58)\tAcc@5  86.72 ( 87.85)\n","Epoch: [33][180/391]\tTime  0.093 ( 0.094)\tLoss 1.1920e+00 (1.3905e+00)\tAcc@1  66.41 ( 60.43)\tAcc@5  90.62 ( 87.84)\n","Epoch: [33][210/391]\tTime  0.096 ( 0.093)\tLoss 1.2146e+00 (1.4007e+00)\tAcc@1  65.62 ( 60.17)\tAcc@5  91.41 ( 87.68)\n","Epoch: [33][240/391]\tTime  0.086 ( 0.093)\tLoss 1.4543e+00 (1.4078e+00)\tAcc@1  52.34 ( 60.02)\tAcc@5  88.28 ( 87.59)\n","Epoch: [33][270/391]\tTime  0.094 ( 0.093)\tLoss 1.4127e+00 (1.4108e+00)\tAcc@1  61.72 ( 59.99)\tAcc@5  87.50 ( 87.61)\n","Epoch: [33][300/391]\tTime  0.095 ( 0.093)\tLoss 1.2505e+00 (1.4126e+00)\tAcc@1  62.50 ( 59.95)\tAcc@5  89.84 ( 87.59)\n","Epoch: [33][330/391]\tTime  0.091 ( 0.093)\tLoss 1.4988e+00 (1.4138e+00)\tAcc@1  60.94 ( 59.84)\tAcc@5  85.16 ( 87.67)\n","Epoch: [33][360/391]\tTime  0.091 ( 0.093)\tLoss 1.6053e+00 (1.4156e+00)\tAcc@1  58.59 ( 59.83)\tAcc@5  82.81 ( 87.66)\n","Epoch: [33][390/391]\tTime  0.083 ( 0.093)\tLoss 1.5352e+00 (1.4199e+00)\tAcc@1  55.00 ( 59.76)\tAcc@5  86.25 ( 87.58)\n","==> Train Accuracy: Acc@1 59.758 || Acc@5 87.582\n","==> Test Accuracy:  Acc@1 55.040 || Acc@5 82.990\n","==> 38.97 seconds to train this epoch\n","\n","\n","----- epoch: 34, lr: 0.1 -----\n","Epoch: [34][  0/391]\tTime  0.242 ( 0.242)\tLoss 1.3431e+00 (1.3431e+00)\tAcc@1  59.38 ( 59.38)\tAcc@5  89.06 ( 89.06)\n","Epoch: [34][ 30/391]\tTime  0.102 ( 0.098)\tLoss 1.3069e+00 (1.3164e+00)\tAcc@1  64.84 ( 62.25)\tAcc@5  89.06 ( 89.24)\n","Epoch: [34][ 60/391]\tTime  0.084 ( 0.095)\tLoss 1.4755e+00 (1.3433e+00)\tAcc@1  58.59 ( 61.59)\tAcc@5  85.16 ( 88.63)\n","Epoch: [34][ 90/391]\tTime  0.090 ( 0.094)\tLoss 1.2496e+00 (1.3500e+00)\tAcc@1  63.28 ( 61.44)\tAcc@5  86.72 ( 88.42)\n","Epoch: [34][120/391]\tTime  0.091 ( 0.094)\tLoss 1.3817e+00 (1.3672e+00)\tAcc@1  62.50 ( 61.01)\tAcc@5  89.06 ( 88.22)\n","Epoch: [34][150/391]\tTime  0.090 ( 0.093)\tLoss 1.4243e+00 (1.3772e+00)\tAcc@1  62.50 ( 61.04)\tAcc@5  85.16 ( 88.03)\n","Epoch: [34][180/391]\tTime  0.096 ( 0.093)\tLoss 1.5464e+00 (1.3813e+00)\tAcc@1  55.47 ( 60.91)\tAcc@5  86.72 ( 88.04)\n","Epoch: [34][210/391]\tTime  0.093 ( 0.093)\tLoss 1.1312e+00 (1.3889e+00)\tAcc@1  67.19 ( 60.75)\tAcc@5  91.41 ( 87.79)\n","Epoch: [34][240/391]\tTime  0.091 ( 0.093)\tLoss 1.3629e+00 (1.3979e+00)\tAcc@1  61.72 ( 60.48)\tAcc@5  88.28 ( 87.70)\n","Epoch: [34][270/391]\tTime  0.091 ( 0.093)\tLoss 1.5769e+00 (1.3990e+00)\tAcc@1  64.06 ( 60.47)\tAcc@5  81.25 ( 87.72)\n","Epoch: [34][300/391]\tTime  0.095 ( 0.093)\tLoss 1.4663e+00 (1.4061e+00)\tAcc@1  57.81 ( 60.35)\tAcc@5  82.81 ( 87.54)\n","Epoch: [34][330/391]\tTime  0.093 ( 0.093)\tLoss 1.2865e+00 (1.4096e+00)\tAcc@1  67.19 ( 60.18)\tAcc@5  89.84 ( 87.50)\n","Epoch: [34][360/391]\tTime  0.092 ( 0.093)\tLoss 1.5243e+00 (1.4121e+00)\tAcc@1  56.25 ( 60.10)\tAcc@5  87.50 ( 87.47)\n","Epoch: [34][390/391]\tTime  0.084 ( 0.093)\tLoss 1.5073e+00 (1.4120e+00)\tAcc@1  62.50 ( 60.04)\tAcc@5  85.00 ( 87.50)\n","==> Train Accuracy: Acc@1 60.038 || Acc@5 87.498\n","==> Test Accuracy:  Acc@1 57.000 || Acc@5 84.480\n","==> 38.87 seconds to train this epoch\n","\n","\n","----- epoch: 35, lr: 0.1 -----\n","Epoch: [35][  0/391]\tTime  0.245 ( 0.245)\tLoss 1.1988e+00 (1.1988e+00)\tAcc@1  60.16 ( 60.16)\tAcc@5  92.97 ( 92.97)\n","Epoch: [35][ 30/391]\tTime  0.091 ( 0.098)\tLoss 1.3118e+00 (1.3289e+00)\tAcc@1  62.50 ( 62.63)\tAcc@5  87.50 ( 88.96)\n","Epoch: [35][ 60/391]\tTime  0.094 ( 0.095)\tLoss 1.3915e+00 (1.3556e+00)\tAcc@1  61.72 ( 62.38)\tAcc@5  85.16 ( 88.19)\n","Epoch: [35][ 90/391]\tTime  0.094 ( 0.094)\tLoss 1.4082e+00 (1.3648e+00)\tAcc@1  58.59 ( 62.26)\tAcc@5  89.84 ( 88.03)\n","Epoch: [35][120/391]\tTime  0.092 ( 0.094)\tLoss 1.4914e+00 (1.3693e+00)\tAcc@1  62.50 ( 61.80)\tAcc@5  87.50 ( 88.13)\n","Epoch: [35][150/391]\tTime  0.091 ( 0.093)\tLoss 1.4041e+00 (1.3807e+00)\tAcc@1  54.69 ( 61.37)\tAcc@5  86.72 ( 87.93)\n","Epoch: [35][180/391]\tTime  0.096 ( 0.093)\tLoss 1.3123e+00 (1.3844e+00)\tAcc@1  62.50 ( 61.21)\tAcc@5  90.62 ( 87.90)\n","Epoch: [35][210/391]\tTime  0.093 ( 0.093)\tLoss 1.7505e+00 (1.3932e+00)\tAcc@1  55.47 ( 60.93)\tAcc@5  77.34 ( 87.83)\n","Epoch: [35][240/391]\tTime  0.089 ( 0.093)\tLoss 1.3985e+00 (1.3941e+00)\tAcc@1  62.50 ( 60.92)\tAcc@5  89.84 ( 87.79)\n","Epoch: [35][270/391]\tTime  0.088 ( 0.093)\tLoss 1.4330e+00 (1.3947e+00)\tAcc@1  61.72 ( 60.87)\tAcc@5  88.28 ( 87.81)\n","Epoch: [35][300/391]\tTime  0.093 ( 0.093)\tLoss 1.2676e+00 (1.3952e+00)\tAcc@1  60.94 ( 60.84)\tAcc@5  93.75 ( 87.80)\n","Epoch: [35][330/391]\tTime  0.095 ( 0.093)\tLoss 1.1712e+00 (1.3978e+00)\tAcc@1  65.62 ( 60.69)\tAcc@5  91.41 ( 87.83)\n","Epoch: [35][360/391]\tTime  0.092 ( 0.093)\tLoss 1.5123e+00 (1.4029e+00)\tAcc@1  56.25 ( 60.62)\tAcc@5  86.72 ( 87.73)\n","Epoch: [35][390/391]\tTime  0.082 ( 0.092)\tLoss 1.7325e+00 (1.4028e+00)\tAcc@1  53.75 ( 60.66)\tAcc@5  80.00 ( 87.67)\n","==> Train Accuracy: Acc@1 60.664 || Acc@5 87.670\n","==> Test Accuracy:  Acc@1 57.710 || Acc@5 85.230\n","==> 38.85 seconds to train this epoch\n","\n","\n","----- epoch: 36, lr: 0.1 -----\n","Epoch: [36][  0/391]\tTime  0.255 ( 0.255)\tLoss 1.2436e+00 (1.2436e+00)\tAcc@1  64.84 ( 64.84)\tAcc@5  92.19 ( 92.19)\n","Epoch: [36][ 30/391]\tTime  0.085 ( 0.098)\tLoss 1.3138e+00 (1.3447e+00)\tAcc@1  64.06 ( 61.62)\tAcc@5  89.84 ( 88.58)\n","Epoch: [36][ 60/391]\tTime  0.095 ( 0.095)\tLoss 1.4677e+00 (1.3720e+00)\tAcc@1  58.59 ( 61.05)\tAcc@5  87.50 ( 88.28)\n","Epoch: [36][ 90/391]\tTime  0.095 ( 0.094)\tLoss 1.3324e+00 (1.3680e+00)\tAcc@1  60.16 ( 60.96)\tAcc@5  89.84 ( 88.44)\n","Epoch: [36][120/391]\tTime  0.092 ( 0.094)\tLoss 1.1897e+00 (1.3659e+00)\tAcc@1  62.50 ( 60.97)\tAcc@5  93.75 ( 88.51)\n","Epoch: [36][150/391]\tTime  0.087 ( 0.094)\tLoss 1.4761e+00 (1.3711e+00)\tAcc@1  54.69 ( 60.96)\tAcc@5  89.84 ( 88.25)\n","Epoch: [36][180/391]\tTime  0.092 ( 0.093)\tLoss 1.5531e+00 (1.3747e+00)\tAcc@1  55.47 ( 60.82)\tAcc@5  82.81 ( 88.10)\n","Epoch: [36][210/391]\tTime  0.085 ( 0.093)\tLoss 1.4490e+00 (1.3796e+00)\tAcc@1  59.38 ( 60.79)\tAcc@5  85.94 ( 88.03)\n","Epoch: [36][240/391]\tTime  0.088 ( 0.093)\tLoss 1.1844e+00 (1.3820e+00)\tAcc@1  66.41 ( 60.74)\tAcc@5  89.84 ( 88.00)\n","Epoch: [36][270/391]\tTime  0.089 ( 0.093)\tLoss 1.5828e+00 (1.3889e+00)\tAcc@1  55.47 ( 60.60)\tAcc@5  84.38 ( 87.86)\n","Epoch: [36][300/391]\tTime  0.093 ( 0.093)\tLoss 1.3272e+00 (1.3909e+00)\tAcc@1  63.28 ( 60.56)\tAcc@5  89.06 ( 87.85)\n","Epoch: [36][330/391]\tTime  0.092 ( 0.093)\tLoss 1.4916e+00 (1.3962e+00)\tAcc@1  64.84 ( 60.45)\tAcc@5  82.81 ( 87.78)\n","Epoch: [36][360/391]\tTime  0.093 ( 0.093)\tLoss 1.3178e+00 (1.3958e+00)\tAcc@1  64.06 ( 60.50)\tAcc@5  85.94 ( 87.80)\n","Epoch: [36][390/391]\tTime  0.082 ( 0.093)\tLoss 1.4333e+00 (1.4004e+00)\tAcc@1  60.00 ( 60.40)\tAcc@5  88.75 ( 87.74)\n","==> Train Accuracy: Acc@1 60.404 || Acc@5 87.742\n","==> Test Accuracy:  Acc@1 57.040 || Acc@5 84.600\n","==> 38.92 seconds to train this epoch\n","\n","\n","----- epoch: 37, lr: 0.1 -----\n","Epoch: [37][  0/391]\tTime  0.276 ( 0.276)\tLoss 1.3996e+00 (1.3996e+00)\tAcc@1  56.25 ( 56.25)\tAcc@5  87.50 ( 87.50)\n","Epoch: [37][ 30/391]\tTime  0.092 ( 0.099)\tLoss 1.2479e+00 (1.3010e+00)\tAcc@1  62.50 ( 62.47)\tAcc@5  92.19 ( 89.21)\n","Epoch: [37][ 60/391]\tTime  0.092 ( 0.096)\tLoss 1.4550e+00 (1.3187e+00)\tAcc@1  65.62 ( 62.22)\tAcc@5  85.16 ( 88.78)\n","Epoch: [37][ 90/391]\tTime  0.093 ( 0.095)\tLoss 1.1549e+00 (1.3474e+00)\tAcc@1  71.09 ( 61.69)\tAcc@5  89.06 ( 88.28)\n","Epoch: [37][120/391]\tTime  0.094 ( 0.094)\tLoss 1.4253e+00 (1.3686e+00)\tAcc@1  58.59 ( 61.12)\tAcc@5  85.94 ( 87.98)\n","Epoch: [37][150/391]\tTime  0.091 ( 0.094)\tLoss 1.7192e+00 (1.3691e+00)\tAcc@1  58.59 ( 61.24)\tAcc@5  84.38 ( 88.06)\n","Epoch: [37][180/391]\tTime  0.092 ( 0.093)\tLoss 1.4494e+00 (1.3799e+00)\tAcc@1  57.81 ( 61.05)\tAcc@5  89.06 ( 87.96)\n","Epoch: [37][210/391]\tTime  0.096 ( 0.093)\tLoss 1.3953e+00 (1.3847e+00)\tAcc@1  60.16 ( 60.94)\tAcc@5  89.84 ( 87.91)\n","Epoch: [37][240/391]\tTime  0.101 ( 0.093)\tLoss 1.4448e+00 (1.3866e+00)\tAcc@1  58.59 ( 60.84)\tAcc@5  89.06 ( 87.87)\n","Epoch: [37][270/391]\tTime  0.091 ( 0.093)\tLoss 1.3585e+00 (1.3872e+00)\tAcc@1  65.62 ( 60.93)\tAcc@5  89.84 ( 87.84)\n","Epoch: [37][300/391]\tTime  0.091 ( 0.093)\tLoss 1.2139e+00 (1.3879e+00)\tAcc@1  70.31 ( 60.98)\tAcc@5  87.50 ( 87.81)\n","Epoch: [37][330/391]\tTime  0.090 ( 0.093)\tLoss 1.3433e+00 (1.3924e+00)\tAcc@1  60.94 ( 60.84)\tAcc@5  89.06 ( 87.77)\n","Epoch: [37][360/391]\tTime  0.095 ( 0.093)\tLoss 1.6150e+00 (1.3953e+00)\tAcc@1  55.47 ( 60.74)\tAcc@5  85.16 ( 87.74)\n","Epoch: [37][390/391]\tTime  0.082 ( 0.093)\tLoss 1.5803e+00 (1.3970e+00)\tAcc@1  53.75 ( 60.72)\tAcc@5  86.25 ( 87.69)\n","==> Train Accuracy: Acc@1 60.724 || Acc@5 87.694\n","==> Test Accuracy:  Acc@1 55.730 || Acc@5 85.090\n","==> 38.88 seconds to train this epoch\n","\n","\n","----- epoch: 38, lr: 0.1 -----\n","Epoch: [38][  0/391]\tTime  0.257 ( 0.257)\tLoss 1.3732e+00 (1.3732e+00)\tAcc@1  61.72 ( 61.72)\tAcc@5  85.16 ( 85.16)\n","Epoch: [38][ 30/391]\tTime  0.090 ( 0.099)\tLoss 1.3858e+00 (1.3315e+00)\tAcc@1  60.16 ( 62.42)\tAcc@5  92.19 ( 88.63)\n","Epoch: [38][ 60/391]\tTime  0.093 ( 0.096)\tLoss 1.2490e+00 (1.3452e+00)\tAcc@1  66.41 ( 62.21)\tAcc@5  89.84 ( 88.41)\n","Epoch: [38][ 90/391]\tTime  0.092 ( 0.094)\tLoss 1.6576e+00 (1.3512e+00)\tAcc@1  49.22 ( 61.79)\tAcc@5  87.50 ( 88.56)\n","Epoch: [38][120/391]\tTime  0.089 ( 0.094)\tLoss 1.4214e+00 (1.3701e+00)\tAcc@1  63.28 ( 61.23)\tAcc@5  88.28 ( 88.29)\n","Epoch: [38][150/391]\tTime  0.073 ( 0.093)\tLoss 1.3467e+00 (1.3763e+00)\tAcc@1  63.28 ( 61.17)\tAcc@5  90.62 ( 88.31)\n","Epoch: [38][180/391]\tTime  0.092 ( 0.093)\tLoss 1.5299e+00 (1.3810e+00)\tAcc@1  54.69 ( 60.90)\tAcc@5  89.84 ( 88.16)\n","Epoch: [38][210/391]\tTime  0.090 ( 0.093)\tLoss 1.4654e+00 (1.3898e+00)\tAcc@1  55.47 ( 60.70)\tAcc@5  90.62 ( 88.04)\n","Epoch: [38][240/391]\tTime  0.095 ( 0.093)\tLoss 1.3687e+00 (1.3918e+00)\tAcc@1  57.81 ( 60.71)\tAcc@5  88.28 ( 87.94)\n","Epoch: [38][270/391]\tTime  0.095 ( 0.093)\tLoss 1.5364e+00 (1.3897e+00)\tAcc@1  62.50 ( 60.76)\tAcc@5  85.16 ( 87.96)\n","Epoch: [38][300/391]\tTime  0.094 ( 0.093)\tLoss 1.4071e+00 (1.3923e+00)\tAcc@1  60.16 ( 60.74)\tAcc@5  88.28 ( 87.92)\n","Epoch: [38][330/391]\tTime  0.091 ( 0.093)\tLoss 1.4297e+00 (1.3932e+00)\tAcc@1  60.16 ( 60.84)\tAcc@5  89.84 ( 87.88)\n","Epoch: [38][360/391]\tTime  0.093 ( 0.093)\tLoss 1.3700e+00 (1.3932e+00)\tAcc@1  59.38 ( 60.81)\tAcc@5  87.50 ( 87.95)\n","Epoch: [38][390/391]\tTime  0.083 ( 0.093)\tLoss 1.3498e+00 (1.3959e+00)\tAcc@1  61.25 ( 60.73)\tAcc@5  91.25 ( 87.91)\n","==> Train Accuracy: Acc@1 60.734 || Acc@5 87.910\n","==> Test Accuracy:  Acc@1 53.990 || Acc@5 82.440\n","==> 38.92 seconds to train this epoch\n","\n","\n","----- epoch: 39, lr: 0.1 -----\n","Epoch: [39][  0/391]\tTime  0.267 ( 0.267)\tLoss 1.2431e+00 (1.2431e+00)\tAcc@1  61.72 ( 61.72)\tAcc@5  88.28 ( 88.28)\n","Epoch: [39][ 30/391]\tTime  0.093 ( 0.098)\tLoss 1.3884e+00 (1.2914e+00)\tAcc@1  60.94 ( 62.85)\tAcc@5  87.50 ( 89.74)\n","Epoch: [39][ 60/391]\tTime  0.094 ( 0.095)\tLoss 1.3093e+00 (1.3191e+00)\tAcc@1  63.28 ( 61.89)\tAcc@5  91.41 ( 89.32)\n","Epoch: [39][ 90/391]\tTime  0.095 ( 0.094)\tLoss 1.3584e+00 (1.3326e+00)\tAcc@1  62.50 ( 61.87)\tAcc@5  86.72 ( 89.00)\n","Epoch: [39][120/391]\tTime  0.089 ( 0.093)\tLoss 1.3841e+00 (1.3537e+00)\tAcc@1  55.47 ( 61.31)\tAcc@5  89.84 ( 88.69)\n","Epoch: [39][150/391]\tTime  0.094 ( 0.093)\tLoss 1.0979e+00 (1.3566e+00)\tAcc@1  64.84 ( 61.14)\tAcc@5  91.41 ( 88.66)\n","Epoch: [39][180/391]\tTime  0.094 ( 0.093)\tLoss 1.2418e+00 (1.3597e+00)\tAcc@1  62.50 ( 60.95)\tAcc@5  91.41 ( 88.68)\n","Epoch: [39][210/391]\tTime  0.089 ( 0.093)\tLoss 1.4089e+00 (1.3680e+00)\tAcc@1  64.06 ( 60.64)\tAcc@5  85.16 ( 88.61)\n","Epoch: [39][240/391]\tTime  0.086 ( 0.093)\tLoss 1.5334e+00 (1.3704e+00)\tAcc@1  56.25 ( 60.65)\tAcc@5  85.94 ( 88.58)\n","Epoch: [39][270/391]\tTime  0.091 ( 0.093)\tLoss 1.3542e+00 (1.3671e+00)\tAcc@1  61.72 ( 60.76)\tAcc@5  87.50 ( 88.64)\n","Epoch: [39][300/391]\tTime  0.089 ( 0.093)\tLoss 1.4817e+00 (1.3684e+00)\tAcc@1  56.25 ( 60.78)\tAcc@5  89.06 ( 88.61)\n","Epoch: [39][330/391]\tTime  0.095 ( 0.093)\tLoss 1.3773e+00 (1.3731e+00)\tAcc@1  62.50 ( 60.73)\tAcc@5  86.72 ( 88.51)\n","Epoch: [39][360/391]\tTime  0.092 ( 0.093)\tLoss 1.3849e+00 (1.3754e+00)\tAcc@1  61.72 ( 60.78)\tAcc@5  88.28 ( 88.45)\n","Epoch: [39][390/391]\tTime  0.082 ( 0.092)\tLoss 1.4520e+00 (1.3786e+00)\tAcc@1  57.50 ( 60.71)\tAcc@5  87.50 ( 88.35)\n","==> Train Accuracy: Acc@1 60.706 || Acc@5 88.354\n","==> Test Accuracy:  Acc@1 55.290 || Acc@5 83.960\n","==> 38.81 seconds to train this epoch\n","\n","\n","----- epoch: 40, lr: 0.1 -----\n","Epoch: [40][  0/391]\tTime  0.254 ( 0.254)\tLoss 1.4730e+00 (1.4730e+00)\tAcc@1  59.38 ( 59.38)\tAcc@5  83.59 ( 83.59)\n","Epoch: [40][ 30/391]\tTime  0.090 ( 0.098)\tLoss 1.3341e+00 (1.3355e+00)\tAcc@1  62.50 ( 61.57)\tAcc@5  84.38 ( 88.89)\n","Epoch: [40][ 60/391]\tTime  0.093 ( 0.095)\tLoss 1.1863e+00 (1.3373e+00)\tAcc@1  67.19 ( 61.86)\tAcc@5  90.62 ( 88.63)\n","Epoch: [40][ 90/391]\tTime  0.098 ( 0.094)\tLoss 1.4048e+00 (1.3488e+00)\tAcc@1  57.81 ( 61.79)\tAcc@5  89.06 ( 88.40)\n","Epoch: [40][120/391]\tTime  0.091 ( 0.094)\tLoss 1.3180e+00 (1.3519e+00)\tAcc@1  66.41 ( 61.68)\tAcc@5  89.84 ( 88.47)\n","Epoch: [40][150/391]\tTime  0.092 ( 0.093)\tLoss 1.2204e+00 (1.3522e+00)\tAcc@1  66.41 ( 61.54)\tAcc@5  87.50 ( 88.62)\n","Epoch: [40][180/391]\tTime  0.092 ( 0.093)\tLoss 1.4813e+00 (1.3518e+00)\tAcc@1  58.59 ( 61.47)\tAcc@5  85.94 ( 88.56)\n","Epoch: [40][210/391]\tTime  0.101 ( 0.093)\tLoss 1.6014e+00 (1.3532e+00)\tAcc@1  51.56 ( 61.49)\tAcc@5  86.72 ( 88.52)\n","Epoch: [40][240/391]\tTime  0.091 ( 0.093)\tLoss 1.7277e+00 (1.3568e+00)\tAcc@1  57.03 ( 61.36)\tAcc@5  85.16 ( 88.49)\n","Epoch: [40][270/391]\tTime  0.092 ( 0.093)\tLoss 1.2980e+00 (1.3682e+00)\tAcc@1  64.84 ( 61.10)\tAcc@5  92.19 ( 88.33)\n","Epoch: [40][300/391]\tTime  0.093 ( 0.093)\tLoss 1.5555e+00 (1.3745e+00)\tAcc@1  60.94 ( 60.93)\tAcc@5  85.94 ( 88.20)\n","Epoch: [40][330/391]\tTime  0.090 ( 0.093)\tLoss 1.3297e+00 (1.3761e+00)\tAcc@1  60.16 ( 60.90)\tAcc@5  93.75 ( 88.21)\n","Epoch: [40][360/391]\tTime  0.088 ( 0.092)\tLoss 1.3559e+00 (1.3764e+00)\tAcc@1  59.38 ( 60.82)\tAcc@5  91.41 ( 88.23)\n","Epoch: [40][390/391]\tTime  0.082 ( 0.092)\tLoss 1.1537e+00 (1.3801e+00)\tAcc@1  66.25 ( 60.73)\tAcc@5  90.00 ( 88.15)\n","==> Train Accuracy: Acc@1 60.730 || Acc@5 88.154\n","==> Test Accuracy:  Acc@1 56.710 || Acc@5 85.270\n","==> 38.79 seconds to train this epoch\n","\n","\n","----- epoch: 41, lr: 0.1 -----\n","Epoch: [41][  0/391]\tTime  0.260 ( 0.260)\tLoss 1.1766e+00 (1.1766e+00)\tAcc@1  64.84 ( 64.84)\tAcc@5  93.75 ( 93.75)\n","Epoch: [41][ 30/391]\tTime  0.092 ( 0.100)\tLoss 1.4340e+00 (1.2999e+00)\tAcc@1  57.03 ( 62.30)\tAcc@5  89.06 ( 89.14)\n","Epoch: [41][ 60/391]\tTime  0.089 ( 0.096)\tLoss 1.3753e+00 (1.3285e+00)\tAcc@1  62.50 ( 61.90)\tAcc@5  82.03 ( 88.95)\n","Epoch: [41][ 90/391]\tTime  0.091 ( 0.095)\tLoss 1.2839e+00 (1.3285e+00)\tAcc@1  63.28 ( 62.11)\tAcc@5  90.62 ( 88.83)\n","Epoch: [41][120/391]\tTime  0.090 ( 0.094)\tLoss 1.3555e+00 (1.3314e+00)\tAcc@1  61.72 ( 62.16)\tAcc@5  86.72 ( 88.85)\n","Epoch: [41][150/391]\tTime  0.093 ( 0.094)\tLoss 1.2918e+00 (1.3390e+00)\tAcc@1  60.94 ( 61.99)\tAcc@5  87.50 ( 88.93)\n","Epoch: [41][180/391]\tTime  0.093 ( 0.094)\tLoss 1.3535e+00 (1.3415e+00)\tAcc@1  62.50 ( 61.82)\tAcc@5  89.06 ( 88.91)\n","Epoch: [41][210/391]\tTime  0.084 ( 0.093)\tLoss 1.3817e+00 (1.3484e+00)\tAcc@1  60.94 ( 61.59)\tAcc@5  89.84 ( 88.70)\n","Epoch: [41][240/391]\tTime  0.092 ( 0.093)\tLoss 1.4637e+00 (1.3589e+00)\tAcc@1  61.72 ( 61.40)\tAcc@5  88.28 ( 88.52)\n","Epoch: [41][270/391]\tTime  0.092 ( 0.093)\tLoss 1.3546e+00 (1.3625e+00)\tAcc@1  62.50 ( 61.28)\tAcc@5  89.06 ( 88.52)\n","Epoch: [41][300/391]\tTime  0.098 ( 0.093)\tLoss 1.4129e+00 (1.3699e+00)\tAcc@1  57.81 ( 61.14)\tAcc@5  90.62 ( 88.36)\n","Epoch: [41][330/391]\tTime  0.090 ( 0.093)\tLoss 1.0913e+00 (1.3703e+00)\tAcc@1  69.53 ( 61.10)\tAcc@5  89.84 ( 88.32)\n","Epoch: [41][360/391]\tTime  0.090 ( 0.093)\tLoss 1.3688e+00 (1.3751e+00)\tAcc@1  63.28 ( 61.05)\tAcc@5  86.72 ( 88.26)\n","Epoch: [41][390/391]\tTime  0.083 ( 0.093)\tLoss 1.5600e+00 (1.3794e+00)\tAcc@1  55.00 ( 60.97)\tAcc@5  82.50 ( 88.22)\n","==> Train Accuracy: Acc@1 60.966 || Acc@5 88.224\n","==> Test Accuracy:  Acc@1 58.480 || Acc@5 86.350\n","==> 38.97 seconds to train this epoch\n","\n","\n","----- epoch: 42, lr: 0.1 -----\n","Epoch: [42][  0/391]\tTime  0.267 ( 0.267)\tLoss 1.4528e+00 (1.4528e+00)\tAcc@1  57.81 ( 57.81)\tAcc@5  86.72 ( 86.72)\n","Epoch: [42][ 30/391]\tTime  0.093 ( 0.098)\tLoss 1.4318e+00 (1.2968e+00)\tAcc@1  57.03 ( 63.26)\tAcc@5  87.50 ( 88.96)\n","Epoch: [42][ 60/391]\tTime  0.093 ( 0.095)\tLoss 1.1990e+00 (1.2991e+00)\tAcc@1  64.06 ( 63.03)\tAcc@5  92.97 ( 89.19)\n","Epoch: [42][ 90/391]\tTime  0.091 ( 0.094)\tLoss 1.3323e+00 (1.3282e+00)\tAcc@1  64.06 ( 62.36)\tAcc@5  86.72 ( 88.82)\n","Epoch: [42][120/391]\tTime  0.090 ( 0.094)\tLoss 1.1539e+00 (1.3349e+00)\tAcc@1  67.97 ( 62.12)\tAcc@5  89.06 ( 88.87)\n","Epoch: [42][150/391]\tTime  0.092 ( 0.093)\tLoss 1.3100e+00 (1.3499e+00)\tAcc@1  66.41 ( 61.80)\tAcc@5  87.50 ( 88.54)\n","Epoch: [42][180/391]\tTime  0.086 ( 0.093)\tLoss 1.5147e+00 (1.3583e+00)\tAcc@1  60.16 ( 61.70)\tAcc@5  85.16 ( 88.33)\n","Epoch: [42][210/391]\tTime  0.094 ( 0.093)\tLoss 1.3718e+00 (1.3596e+00)\tAcc@1  67.97 ( 61.78)\tAcc@5  85.16 ( 88.20)\n","Epoch: [42][240/391]\tTime  0.097 ( 0.093)\tLoss 1.2388e+00 (1.3614e+00)\tAcc@1  67.19 ( 61.75)\tAcc@5  90.62 ( 88.16)\n","Epoch: [42][270/391]\tTime  0.084 ( 0.093)\tLoss 1.3167e+00 (1.3623e+00)\tAcc@1  64.06 ( 61.60)\tAcc@5  89.84 ( 88.14)\n","Epoch: [42][300/391]\tTime  0.092 ( 0.093)\tLoss 1.2215e+00 (1.3680e+00)\tAcc@1  66.41 ( 61.44)\tAcc@5  86.72 ( 88.10)\n","Epoch: [42][330/391]\tTime  0.084 ( 0.093)\tLoss 1.4542e+00 (1.3692e+00)\tAcc@1  53.91 ( 61.41)\tAcc@5  85.94 ( 88.04)\n","Epoch: [42][360/391]\tTime  0.092 ( 0.093)\tLoss 1.3491e+00 (1.3731e+00)\tAcc@1  63.28 ( 61.26)\tAcc@5  86.72 ( 88.04)\n","Epoch: [42][390/391]\tTime  0.083 ( 0.093)\tLoss 1.5394e+00 (1.3730e+00)\tAcc@1  57.50 ( 61.28)\tAcc@5  87.50 ( 88.06)\n","==> Train Accuracy: Acc@1 61.282 || Acc@5 88.056\n","==> Test Accuracy:  Acc@1 58.090 || Acc@5 86.910\n","==> 38.95 seconds to train this epoch\n","\n","\n","----- epoch: 43, lr: 0.1 -----\n","Epoch: [43][  0/391]\tTime  0.276 ( 0.276)\tLoss 1.5415e+00 (1.5415e+00)\tAcc@1  60.94 ( 60.94)\tAcc@5  85.94 ( 85.94)\n","Epoch: [43][ 30/391]\tTime  0.087 ( 0.098)\tLoss 1.2279e+00 (1.3257e+00)\tAcc@1  67.97 ( 62.65)\tAcc@5  87.50 ( 88.63)\n","Epoch: [43][ 60/391]\tTime  0.097 ( 0.095)\tLoss 1.2052e+00 (1.3261e+00)\tAcc@1  60.94 ( 62.22)\tAcc@5  90.62 ( 88.64)\n","Epoch: [43][ 90/391]\tTime  0.091 ( 0.094)\tLoss 1.2003e+00 (1.3325e+00)\tAcc@1  64.06 ( 62.27)\tAcc@5  91.41 ( 88.52)\n","Epoch: [43][120/391]\tTime  0.092 ( 0.094)\tLoss 1.4064e+00 (1.3430e+00)\tAcc@1  60.94 ( 61.98)\tAcc@5  87.50 ( 88.26)\n","Epoch: [43][150/391]\tTime  0.092 ( 0.094)\tLoss 1.1741e+00 (1.3445e+00)\tAcc@1  71.09 ( 62.11)\tAcc@5  91.41 ( 88.25)\n","Epoch: [43][180/391]\tTime  0.094 ( 0.093)\tLoss 1.4217e+00 (1.3417e+00)\tAcc@1  58.59 ( 62.06)\tAcc@5  86.72 ( 88.34)\n","Epoch: [43][210/391]\tTime  0.092 ( 0.093)\tLoss 1.4120e+00 (1.3467e+00)\tAcc@1  59.38 ( 61.84)\tAcc@5  88.28 ( 88.33)\n","Epoch: [43][240/391]\tTime  0.097 ( 0.093)\tLoss 1.4627e+00 (1.3533e+00)\tAcc@1  55.47 ( 61.54)\tAcc@5  88.28 ( 88.35)\n","Epoch: [43][270/391]\tTime  0.086 ( 0.093)\tLoss 1.4353e+00 (1.3574e+00)\tAcc@1  57.81 ( 61.39)\tAcc@5  87.50 ( 88.34)\n","Epoch: [43][300/391]\tTime  0.091 ( 0.093)\tLoss 1.3157e+00 (1.3643e+00)\tAcc@1  56.25 ( 61.21)\tAcc@5  91.41 ( 88.24)\n","Epoch: [43][330/391]\tTime  0.090 ( 0.093)\tLoss 1.2323e+00 (1.3693e+00)\tAcc@1  67.97 ( 61.10)\tAcc@5  89.84 ( 88.16)\n","Epoch: [43][360/391]\tTime  0.093 ( 0.093)\tLoss 1.8962e+00 (1.3751e+00)\tAcc@1  45.31 ( 60.93)\tAcc@5  81.25 ( 88.16)\n","Epoch: [43][390/391]\tTime  0.082 ( 0.093)\tLoss 1.5002e+00 (1.3762e+00)\tAcc@1  61.25 ( 60.91)\tAcc@5  85.00 ( 88.14)\n","==> Train Accuracy: Acc@1 60.906 || Acc@5 88.136\n","==> Test Accuracy:  Acc@1 60.290 || Acc@5 86.710\n","==> 38.97 seconds to train this epoch\n","\n","\n","----- epoch: 44, lr: 0.1 -----\n","Epoch: [44][  0/391]\tTime  0.276 ( 0.276)\tLoss 1.3738e+00 (1.3738e+00)\tAcc@1  57.03 ( 57.03)\tAcc@5  88.28 ( 88.28)\n","Epoch: [44][ 30/391]\tTime  0.100 ( 0.100)\tLoss 1.2508e+00 (1.3388e+00)\tAcc@1  64.06 ( 62.47)\tAcc@5  92.97 ( 87.88)\n","Epoch: [44][ 60/391]\tTime  0.088 ( 0.097)\tLoss 1.3852e+00 (1.3307e+00)\tAcc@1  60.16 ( 62.46)\tAcc@5  90.62 ( 88.78)\n","Epoch: [44][ 90/391]\tTime  0.092 ( 0.095)\tLoss 1.1888e+00 (1.3445e+00)\tAcc@1  63.28 ( 61.82)\tAcc@5  91.41 ( 88.62)\n","Epoch: [44][120/391]\tTime  0.095 ( 0.094)\tLoss 1.4588e+00 (1.3383e+00)\tAcc@1  59.38 ( 61.89)\tAcc@5  85.94 ( 88.59)\n","Epoch: [44][150/391]\tTime  0.089 ( 0.094)\tLoss 1.4169e+00 (1.3430e+00)\tAcc@1  57.81 ( 61.75)\tAcc@5  86.72 ( 88.66)\n","Epoch: [44][180/391]\tTime  0.099 ( 0.094)\tLoss 1.3419e+00 (1.3430e+00)\tAcc@1  61.72 ( 61.80)\tAcc@5  89.06 ( 88.64)\n","Epoch: [44][210/391]\tTime  0.092 ( 0.093)\tLoss 1.4088e+00 (1.3394e+00)\tAcc@1  64.06 ( 61.94)\tAcc@5  82.03 ( 88.67)\n","Epoch: [44][240/391]\tTime  0.092 ( 0.093)\tLoss 1.4319e+00 (1.3457e+00)\tAcc@1  61.72 ( 61.71)\tAcc@5  88.28 ( 88.58)\n","Epoch: [44][270/391]\tTime  0.094 ( 0.093)\tLoss 1.7017e+00 (1.3532e+00)\tAcc@1  56.25 ( 61.57)\tAcc@5  77.34 ( 88.47)\n","Epoch: [44][300/391]\tTime  0.104 ( 0.093)\tLoss 1.2133e+00 (1.3595e+00)\tAcc@1  72.66 ( 61.43)\tAcc@5  92.19 ( 88.38)\n","Epoch: [44][330/391]\tTime  0.093 ( 0.093)\tLoss 1.2072e+00 (1.3629e+00)\tAcc@1  67.19 ( 61.29)\tAcc@5  90.62 ( 88.34)\n","Epoch: [44][360/391]\tTime  0.092 ( 0.093)\tLoss 1.3142e+00 (1.3645e+00)\tAcc@1  63.28 ( 61.20)\tAcc@5  89.06 ( 88.32)\n","Epoch: [44][390/391]\tTime  0.082 ( 0.093)\tLoss 1.1714e+00 (1.3694e+00)\tAcc@1  66.25 ( 61.14)\tAcc@5  91.25 ( 88.28)\n","==> Train Accuracy: Acc@1 61.142 || Acc@5 88.280\n","==> Test Accuracy:  Acc@1 59.610 || Acc@5 86.600\n","==> 38.99 seconds to train this epoch\n","\n","\n","----- epoch: 45, lr: 0.1 -----\n","Epoch: [45][  0/391]\tTime  0.272 ( 0.272)\tLoss 1.3116e+00 (1.3116e+00)\tAcc@1  58.59 ( 58.59)\tAcc@5  90.62 ( 90.62)\n","Epoch: [45][ 30/391]\tTime  0.092 ( 0.100)\tLoss 1.1899e+00 (1.3246e+00)\tAcc@1  65.62 ( 62.17)\tAcc@5  91.41 ( 89.16)\n","Epoch: [45][ 60/391]\tTime  0.091 ( 0.096)\tLoss 1.3092e+00 (1.3082e+00)\tAcc@1  67.19 ( 62.47)\tAcc@5  86.72 ( 89.10)\n","Epoch: [45][ 90/391]\tTime  0.090 ( 0.095)\tLoss 1.4389e+00 (1.3361e+00)\tAcc@1  61.72 ( 61.99)\tAcc@5  88.28 ( 88.65)\n","Epoch: [45][120/391]\tTime  0.090 ( 0.094)\tLoss 1.2700e+00 (1.3403e+00)\tAcc@1  64.84 ( 61.95)\tAcc@5  89.06 ( 88.64)\n","Epoch: [45][150/391]\tTime  0.092 ( 0.094)\tLoss 1.2519e+00 (1.3361e+00)\tAcc@1  64.84 ( 62.07)\tAcc@5  89.06 ( 88.59)\n","Epoch: [45][180/391]\tTime  0.095 ( 0.094)\tLoss 1.2592e+00 (1.3401e+00)\tAcc@1  65.62 ( 62.06)\tAcc@5  87.50 ( 88.48)\n","Epoch: [45][210/391]\tTime  0.094 ( 0.093)\tLoss 1.2762e+00 (1.3410e+00)\tAcc@1  66.41 ( 62.09)\tAcc@5  89.84 ( 88.47)\n","Epoch: [45][240/391]\tTime  0.092 ( 0.093)\tLoss 1.4329e+00 (1.3467e+00)\tAcc@1  61.72 ( 61.94)\tAcc@5  87.50 ( 88.45)\n","Epoch: [45][270/391]\tTime  0.092 ( 0.093)\tLoss 1.4485e+00 (1.3483e+00)\tAcc@1  56.25 ( 61.97)\tAcc@5  89.06 ( 88.45)\n","Epoch: [45][300/391]\tTime  0.096 ( 0.093)\tLoss 1.2754e+00 (1.3526e+00)\tAcc@1  65.62 ( 61.84)\tAcc@5  90.62 ( 88.41)\n","Epoch: [45][330/391]\tTime  0.092 ( 0.093)\tLoss 1.5081e+00 (1.3568e+00)\tAcc@1  54.69 ( 61.59)\tAcc@5  85.16 ( 88.39)\n","Epoch: [45][360/391]\tTime  0.100 ( 0.093)\tLoss 1.4489e+00 (1.3621e+00)\tAcc@1  58.59 ( 61.44)\tAcc@5  90.62 ( 88.39)\n","Epoch: [45][390/391]\tTime  0.082 ( 0.093)\tLoss 1.5661e+00 (1.3661e+00)\tAcc@1  63.75 ( 61.34)\tAcc@5  81.25 ( 88.32)\n","==> Train Accuracy: Acc@1 61.344 || Acc@5 88.318\n","==> Test Accuracy:  Acc@1 57.910 || Acc@5 86.980\n","==> 39.06 seconds to train this epoch\n","\n","\n","----- epoch: 46, lr: 0.1 -----\n","Epoch: [46][  0/391]\tTime  0.270 ( 0.270)\tLoss 1.3684e+00 (1.3684e+00)\tAcc@1  60.94 ( 60.94)\tAcc@5  88.28 ( 88.28)\n","Epoch: [46][ 30/391]\tTime  0.100 ( 0.100)\tLoss 1.3069e+00 (1.3042e+00)\tAcc@1  64.06 ( 63.31)\tAcc@5  91.41 ( 89.39)\n","Epoch: [46][ 60/391]\tTime  0.091 ( 0.096)\tLoss 1.1235e+00 (1.2943e+00)\tAcc@1  64.84 ( 62.94)\tAcc@5  92.19 ( 89.63)\n","Epoch: [46][ 90/391]\tTime  0.091 ( 0.095)\tLoss 1.2070e+00 (1.2948e+00)\tAcc@1  59.38 ( 62.77)\tAcc@5  91.41 ( 89.43)\n","Epoch: [46][120/391]\tTime  0.091 ( 0.094)\tLoss 1.3639e+00 (1.3137e+00)\tAcc@1  60.94 ( 62.20)\tAcc@5  89.84 ( 89.29)\n","Epoch: [46][150/391]\tTime  0.086 ( 0.094)\tLoss 1.3222e+00 (1.3187e+00)\tAcc@1  60.94 ( 61.95)\tAcc@5  90.62 ( 89.30)\n","Epoch: [46][180/391]\tTime  0.092 ( 0.094)\tLoss 1.3139e+00 (1.3308e+00)\tAcc@1  59.38 ( 61.63)\tAcc@5  90.62 ( 89.12)\n","Epoch: [46][210/391]\tTime  0.094 ( 0.094)\tLoss 1.3601e+00 (1.3406e+00)\tAcc@1  59.38 ( 61.54)\tAcc@5  89.06 ( 88.90)\n","Epoch: [46][240/391]\tTime  0.093 ( 0.093)\tLoss 1.2479e+00 (1.3433e+00)\tAcc@1  63.28 ( 61.60)\tAcc@5  89.06 ( 88.80)\n","Epoch: [46][270/391]\tTime  0.093 ( 0.093)\tLoss 1.3490e+00 (1.3438e+00)\tAcc@1  62.50 ( 61.64)\tAcc@5  85.94 ( 88.77)\n","Epoch: [46][300/391]\tTime  0.093 ( 0.093)\tLoss 1.1147e+00 (1.3475e+00)\tAcc@1  66.41 ( 61.53)\tAcc@5  93.75 ( 88.69)\n","Epoch: [46][330/391]\tTime  0.094 ( 0.093)\tLoss 1.3174e+00 (1.3454e+00)\tAcc@1  59.38 ( 61.59)\tAcc@5  89.06 ( 88.69)\n","Epoch: [46][360/391]\tTime  0.091 ( 0.093)\tLoss 1.3388e+00 (1.3494e+00)\tAcc@1  61.72 ( 61.52)\tAcc@5  89.84 ( 88.63)\n","Epoch: [46][390/391]\tTime  0.081 ( 0.093)\tLoss 1.5694e+00 (1.3555e+00)\tAcc@1  53.75 ( 61.43)\tAcc@5  83.75 ( 88.51)\n","==> Train Accuracy: Acc@1 61.428 || Acc@5 88.510\n","==> Test Accuracy:  Acc@1 57.040 || Acc@5 84.310\n","==> 39.03 seconds to train this epoch\n","\n","\n","----- epoch: 47, lr: 0.1 -----\n","Epoch: [47][  0/391]\tTime  0.282 ( 0.282)\tLoss 1.5104e+00 (1.5104e+00)\tAcc@1  59.38 ( 59.38)\tAcc@5  83.59 ( 83.59)\n","Epoch: [47][ 30/391]\tTime  0.094 ( 0.099)\tLoss 1.3930e+00 (1.2787e+00)\tAcc@1  60.94 ( 63.08)\tAcc@5  87.50 ( 90.20)\n","Epoch: [47][ 60/391]\tTime  0.092 ( 0.096)\tLoss 1.4466e+00 (1.2913e+00)\tAcc@1  60.94 ( 62.97)\tAcc@5  86.72 ( 89.63)\n","Epoch: [47][ 90/391]\tTime  0.092 ( 0.095)\tLoss 1.4530e+00 (1.2993e+00)\tAcc@1  57.03 ( 62.96)\tAcc@5  88.28 ( 89.46)\n","Epoch: [47][120/391]\tTime  0.095 ( 0.094)\tLoss 1.3185e+00 (1.3153e+00)\tAcc@1  57.81 ( 62.51)\tAcc@5  89.84 ( 89.06)\n","Epoch: [47][150/391]\tTime  0.094 ( 0.094)\tLoss 1.3890e+00 (1.3196e+00)\tAcc@1  60.94 ( 62.41)\tAcc@5  88.28 ( 89.11)\n","Epoch: [47][180/391]\tTime  0.094 ( 0.094)\tLoss 1.3800e+00 (1.3235e+00)\tAcc@1  61.72 ( 62.36)\tAcc@5  88.28 ( 89.05)\n","Epoch: [47][210/391]\tTime  0.092 ( 0.093)\tLoss 1.2945e+00 (1.3291e+00)\tAcc@1  64.06 ( 62.25)\tAcc@5  89.06 ( 88.93)\n","Epoch: [47][240/391]\tTime  0.092 ( 0.093)\tLoss 1.5144e+00 (1.3355e+00)\tAcc@1  53.91 ( 62.02)\tAcc@5  88.28 ( 88.86)\n","Epoch: [47][270/391]\tTime  0.092 ( 0.093)\tLoss 1.2437e+00 (1.3403e+00)\tAcc@1  64.06 ( 61.92)\tAcc@5  88.28 ( 88.74)\n","Epoch: [47][300/391]\tTime  0.098 ( 0.093)\tLoss 1.3662e+00 (1.3488e+00)\tAcc@1  57.03 ( 61.62)\tAcc@5  90.62 ( 88.64)\n","Epoch: [47][330/391]\tTime  0.089 ( 0.093)\tLoss 1.3250e+00 (1.3546e+00)\tAcc@1  62.50 ( 61.53)\tAcc@5  89.06 ( 88.51)\n","Epoch: [47][360/391]\tTime  0.094 ( 0.093)\tLoss 1.1672e+00 (1.3479e+00)\tAcc@1  64.06 ( 61.71)\tAcc@5  92.19 ( 88.61)\n","Epoch: [47][390/391]\tTime  0.081 ( 0.093)\tLoss 1.5995e+00 (1.3548e+00)\tAcc@1  57.50 ( 61.57)\tAcc@5  82.50 ( 88.55)\n","==> Train Accuracy: Acc@1 61.570 || Acc@5 88.548\n","==> Test Accuracy:  Acc@1 57.920 || Acc@5 86.200\n","==> 38.95 seconds to train this epoch\n","\n","\n","----- epoch: 48, lr: 0.1 -----\n","Epoch: [48][  0/391]\tTime  0.271 ( 0.271)\tLoss 1.3851e+00 (1.3851e+00)\tAcc@1  60.94 ( 60.94)\tAcc@5  90.62 ( 90.62)\n","Epoch: [48][ 30/391]\tTime  0.093 ( 0.099)\tLoss 1.0514e+00 (1.2488e+00)\tAcc@1  71.88 ( 64.39)\tAcc@5  95.31 ( 90.47)\n","Epoch: [48][ 60/391]\tTime  0.096 ( 0.096)\tLoss 1.5306e+00 (1.3012e+00)\tAcc@1  53.91 ( 62.87)\tAcc@5  83.59 ( 89.47)\n","Epoch: [48][ 90/391]\tTime  0.087 ( 0.095)\tLoss 1.4933e+00 (1.3045e+00)\tAcc@1  57.81 ( 62.90)\tAcc@5  89.84 ( 89.41)\n","Epoch: [48][120/391]\tTime  0.091 ( 0.094)\tLoss 1.4868e+00 (1.3072e+00)\tAcc@1  59.38 ( 62.89)\tAcc@5  84.38 ( 89.33)\n","Epoch: [48][150/391]\tTime  0.092 ( 0.094)\tLoss 1.3683e+00 (1.3236e+00)\tAcc@1  60.94 ( 62.53)\tAcc@5  90.62 ( 89.16)\n","Epoch: [48][180/391]\tTime  0.092 ( 0.094)\tLoss 1.3241e+00 (1.3274e+00)\tAcc@1  64.84 ( 62.50)\tAcc@5  87.50 ( 88.99)\n","Epoch: [48][210/391]\tTime  0.092 ( 0.093)\tLoss 1.3625e+00 (1.3405e+00)\tAcc@1  68.75 ( 62.21)\tAcc@5  89.06 ( 88.76)\n","Epoch: [48][240/391]\tTime  0.092 ( 0.093)\tLoss 1.5003e+00 (1.3433e+00)\tAcc@1  51.56 ( 62.14)\tAcc@5  89.06 ( 88.68)\n","Epoch: [48][270/391]\tTime  0.091 ( 0.093)\tLoss 1.1377e+00 (1.3458e+00)\tAcc@1  70.31 ( 62.02)\tAcc@5  90.62 ( 88.57)\n","Epoch: [48][300/391]\tTime  0.093 ( 0.093)\tLoss 1.3015e+00 (1.3495e+00)\tAcc@1  60.16 ( 61.85)\tAcc@5  94.53 ( 88.55)\n","Epoch: [48][330/391]\tTime  0.092 ( 0.093)\tLoss 1.3866e+00 (1.3503e+00)\tAcc@1  60.94 ( 61.76)\tAcc@5  91.41 ( 88.61)\n","Epoch: [48][360/391]\tTime  0.092 ( 0.093)\tLoss 1.3161e+00 (1.3500e+00)\tAcc@1  61.72 ( 61.79)\tAcc@5  86.72 ( 88.58)\n","Epoch: [48][390/391]\tTime  0.082 ( 0.093)\tLoss 1.5553e+00 (1.3558e+00)\tAcc@1  58.75 ( 61.59)\tAcc@5  83.75 ( 88.50)\n","==> Train Accuracy: Acc@1 61.588 || Acc@5 88.504\n","==> Test Accuracy:  Acc@1 58.550 || Acc@5 86.490\n","==> 38.96 seconds to train this epoch\n","\n","\n","----- epoch: 49, lr: 0.1 -----\n","Epoch: [49][  0/391]\tTime  0.244 ( 0.244)\tLoss 1.4340e+00 (1.4340e+00)\tAcc@1  60.16 ( 60.16)\tAcc@5  89.06 ( 89.06)\n","Epoch: [49][ 30/391]\tTime  0.089 ( 0.098)\tLoss 1.2692e+00 (1.2702e+00)\tAcc@1  61.72 ( 64.11)\tAcc@5  92.19 ( 89.59)\n","Epoch: [49][ 60/391]\tTime  0.089 ( 0.095)\tLoss 1.3644e+00 (1.2953e+00)\tAcc@1  61.72 ( 63.22)\tAcc@5  92.19 ( 89.43)\n","Epoch: [49][ 90/391]\tTime  0.093 ( 0.094)\tLoss 1.2974e+00 (1.3151e+00)\tAcc@1  59.38 ( 62.44)\tAcc@5  89.84 ( 89.25)\n","Epoch: [49][120/391]\tTime  0.093 ( 0.094)\tLoss 1.2399e+00 (1.3192e+00)\tAcc@1  66.41 ( 62.42)\tAcc@5  90.62 ( 89.31)\n","Epoch: [49][150/391]\tTime  0.096 ( 0.093)\tLoss 1.4919e+00 (1.3209e+00)\tAcc@1  60.94 ( 62.66)\tAcc@5  84.38 ( 89.13)\n","Epoch: [49][180/391]\tTime  0.089 ( 0.093)\tLoss 1.2235e+00 (1.3187e+00)\tAcc@1  63.28 ( 62.62)\tAcc@5  89.84 ( 89.09)\n","Epoch: [49][210/391]\tTime  0.091 ( 0.093)\tLoss 1.2516e+00 (1.3286e+00)\tAcc@1  64.84 ( 62.53)\tAcc@5  88.28 ( 88.85)\n","Epoch: [49][240/391]\tTime  0.095 ( 0.093)\tLoss 1.3716e+00 (1.3318e+00)\tAcc@1  58.59 ( 62.42)\tAcc@5  87.50 ( 88.71)\n","Epoch: [49][270/391]\tTime  0.090 ( 0.093)\tLoss 1.2393e+00 (1.3293e+00)\tAcc@1  64.06 ( 62.53)\tAcc@5  88.28 ( 88.66)\n","Epoch: [49][300/391]\tTime  0.091 ( 0.093)\tLoss 1.4771e+00 (1.3315e+00)\tAcc@1  57.81 ( 62.49)\tAcc@5  85.94 ( 88.67)\n","Epoch: [49][330/391]\tTime  0.092 ( 0.093)\tLoss 1.2626e+00 (1.3383e+00)\tAcc@1  61.72 ( 62.28)\tAcc@5  89.84 ( 88.58)\n","Epoch: [49][360/391]\tTime  0.092 ( 0.093)\tLoss 1.3426e+00 (1.3424e+00)\tAcc@1  63.28 ( 62.16)\tAcc@5  89.06 ( 88.61)\n","Epoch: [49][390/391]\tTime  0.082 ( 0.093)\tLoss 1.4387e+00 (1.3496e+00)\tAcc@1  62.50 ( 61.95)\tAcc@5  87.50 ( 88.53)\n","==> Train Accuracy: Acc@1 61.954 || Acc@5 88.530\n","==> Test Accuracy:  Acc@1 57.350 || Acc@5 84.090\n","==> 38.96 seconds to train this epoch\n","\n","\n","----- epoch: 50, lr: 0.1 -----\n","Epoch: [50][  0/391]\tTime  0.265 ( 0.265)\tLoss 1.0841e+00 (1.0841e+00)\tAcc@1  67.97 ( 67.97)\tAcc@5  93.75 ( 93.75)\n","Epoch: [50][ 30/391]\tTime  0.091 ( 0.099)\tLoss 1.3594e+00 (1.2808e+00)\tAcc@1  62.50 ( 63.73)\tAcc@5  92.19 ( 90.07)\n","Epoch: [50][ 60/391]\tTime  0.091 ( 0.096)\tLoss 1.2338e+00 (1.2819e+00)\tAcc@1  65.62 ( 63.70)\tAcc@5  89.84 ( 89.69)\n","Epoch: [50][ 90/391]\tTime  0.092 ( 0.094)\tLoss 1.2714e+00 (1.2915e+00)\tAcc@1  63.28 ( 63.44)\tAcc@5  89.06 ( 89.52)\n","Epoch: [50][120/391]\tTime  0.090 ( 0.094)\tLoss 1.3791e+00 (1.2921e+00)\tAcc@1  62.50 ( 63.29)\tAcc@5  85.94 ( 89.42)\n","Epoch: [50][150/391]\tTime  0.092 ( 0.094)\tLoss 1.0997e+00 (1.2938e+00)\tAcc@1  67.19 ( 63.26)\tAcc@5  92.19 ( 89.41)\n","Epoch: [50][180/391]\tTime  0.091 ( 0.093)\tLoss 1.4649e+00 (1.3019e+00)\tAcc@1  54.69 ( 63.01)\tAcc@5  88.28 ( 89.36)\n","Epoch: [50][210/391]\tTime  0.091 ( 0.093)\tLoss 1.4031e+00 (1.3137e+00)\tAcc@1  60.16 ( 62.66)\tAcc@5  89.06 ( 89.26)\n","Epoch: [50][240/391]\tTime  0.092 ( 0.093)\tLoss 1.4526e+00 (1.3218e+00)\tAcc@1  60.94 ( 62.41)\tAcc@5  87.50 ( 89.11)\n","Epoch: [50][270/391]\tTime  0.091 ( 0.093)\tLoss 1.3895e+00 (1.3263e+00)\tAcc@1  65.62 ( 62.34)\tAcc@5  86.72 ( 89.06)\n","Epoch: [50][300/391]\tTime  0.103 ( 0.093)\tLoss 1.2203e+00 (1.3289e+00)\tAcc@1  66.41 ( 62.26)\tAcc@5  92.19 ( 89.00)\n","Epoch: [50][330/391]\tTime  0.088 ( 0.093)\tLoss 1.3767e+00 (1.3337e+00)\tAcc@1  64.84 ( 62.20)\tAcc@5  87.50 ( 88.94)\n","Epoch: [50][360/391]\tTime  0.093 ( 0.093)\tLoss 1.4144e+00 (1.3425e+00)\tAcc@1  61.72 ( 61.98)\tAcc@5  84.38 ( 88.80)\n","Epoch: [50][390/391]\tTime  0.082 ( 0.093)\tLoss 1.4815e+00 (1.3469e+00)\tAcc@1  60.00 ( 61.91)\tAcc@5  82.50 ( 88.75)\n","==> Train Accuracy: Acc@1 61.910 || Acc@5 88.746\n","==> Test Accuracy:  Acc@1 54.950 || Acc@5 83.480\n","==> 38.93 seconds to train this epoch\n","\n","\n","----- epoch: 51, lr: 0.1 -----\n","Epoch: [51][  0/391]\tTime  0.280 ( 0.280)\tLoss 1.0763e+00 (1.0763e+00)\tAcc@1  70.31 ( 70.31)\tAcc@5  94.53 ( 94.53)\n","Epoch: [51][ 30/391]\tTime  0.095 ( 0.099)\tLoss 1.3520e+00 (1.2280e+00)\tAcc@1  64.06 ( 65.37)\tAcc@5  86.72 ( 90.15)\n","Epoch: [51][ 60/391]\tTime  0.092 ( 0.096)\tLoss 1.2565e+00 (1.2614e+00)\tAcc@1  62.50 ( 64.20)\tAcc@5  88.28 ( 89.57)\n","Epoch: [51][ 90/391]\tTime  0.094 ( 0.095)\tLoss 1.4788e+00 (1.2892e+00)\tAcc@1  57.81 ( 63.55)\tAcc@5  88.28 ( 89.18)\n","Epoch: [51][120/391]\tTime  0.089 ( 0.094)\tLoss 1.4018e+00 (1.2831e+00)\tAcc@1  59.38 ( 63.66)\tAcc@5  89.84 ( 89.22)\n","Epoch: [51][150/391]\tTime  0.093 ( 0.094)\tLoss 1.4153e+00 (1.2960e+00)\tAcc@1  60.94 ( 63.23)\tAcc@5  89.84 ( 89.11)\n","Epoch: [51][180/391]\tTime  0.090 ( 0.093)\tLoss 1.3588e+00 (1.3012e+00)\tAcc@1  63.28 ( 62.97)\tAcc@5  86.72 ( 89.13)\n","Epoch: [51][210/391]\tTime  0.104 ( 0.093)\tLoss 1.4665e+00 (1.3181e+00)\tAcc@1  57.81 ( 62.44)\tAcc@5  89.84 ( 89.01)\n","Epoch: [51][240/391]\tTime  0.095 ( 0.093)\tLoss 1.4985e+00 (1.3238e+00)\tAcc@1  56.25 ( 62.30)\tAcc@5  89.06 ( 88.95)\n","Epoch: [51][270/391]\tTime  0.091 ( 0.093)\tLoss 1.2215e+00 (1.3271e+00)\tAcc@1  64.84 ( 62.31)\tAcc@5  91.41 ( 88.92)\n","Epoch: [51][300/391]\tTime  0.092 ( 0.093)\tLoss 1.5015e+00 (1.3326e+00)\tAcc@1  59.38 ( 62.17)\tAcc@5  88.28 ( 88.82)\n","Epoch: [51][330/391]\tTime  0.095 ( 0.093)\tLoss 1.4934e+00 (1.3375e+00)\tAcc@1  57.03 ( 62.07)\tAcc@5  85.94 ( 88.77)\n","Epoch: [51][360/391]\tTime  0.092 ( 0.093)\tLoss 1.3793e+00 (1.3439e+00)\tAcc@1  60.94 ( 61.92)\tAcc@5  87.50 ( 88.69)\n","Epoch: [51][390/391]\tTime  0.080 ( 0.093)\tLoss 1.7459e+00 (1.3442e+00)\tAcc@1  53.75 ( 61.88)\tAcc@5  81.25 ( 88.68)\n","==> Train Accuracy: Acc@1 61.876 || Acc@5 88.680\n","==> Test Accuracy:  Acc@1 55.920 || Acc@5 84.180\n","==> 38.96 seconds to train this epoch\n","\n","\n","----- epoch: 52, lr: 0.1 -----\n","Epoch: [52][  0/391]\tTime  0.287 ( 0.287)\tLoss 1.1073e+00 (1.1073e+00)\tAcc@1  69.53 ( 69.53)\tAcc@5  89.84 ( 89.84)\n","Epoch: [52][ 30/391]\tTime  0.092 ( 0.100)\tLoss 1.2679e+00 (1.3156e+00)\tAcc@1  67.19 ( 62.68)\tAcc@5  88.28 ( 88.71)\n","Epoch: [52][ 60/391]\tTime  0.095 ( 0.096)\tLoss 1.1399e+00 (1.2921e+00)\tAcc@1  67.97 ( 62.99)\tAcc@5  94.53 ( 89.42)\n","Epoch: [52][ 90/391]\tTime  0.093 ( 0.095)\tLoss 1.5982e+00 (1.2921e+00)\tAcc@1  56.25 ( 62.83)\tAcc@5  88.28 ( 89.43)\n","Epoch: [52][120/391]\tTime  0.092 ( 0.094)\tLoss 1.2676e+00 (1.3104e+00)\tAcc@1  59.38 ( 62.11)\tAcc@5  90.62 ( 89.26)\n","Epoch: [52][150/391]\tTime  0.088 ( 0.094)\tLoss 1.5804e+00 (1.3137e+00)\tAcc@1  56.25 ( 62.15)\tAcc@5  82.03 ( 89.19)\n","Epoch: [52][180/391]\tTime  0.086 ( 0.094)\tLoss 1.3916e+00 (1.3221e+00)\tAcc@1  60.16 ( 61.95)\tAcc@5  87.50 ( 89.14)\n","Epoch: [52][210/391]\tTime  0.089 ( 0.094)\tLoss 1.0907e+00 (1.3200e+00)\tAcc@1  67.19 ( 62.07)\tAcc@5  92.19 ( 89.18)\n","Epoch: [52][240/391]\tTime  0.093 ( 0.093)\tLoss 1.5053e+00 (1.3255e+00)\tAcc@1  60.94 ( 62.10)\tAcc@5  85.16 ( 89.03)\n","Epoch: [52][270/391]\tTime  0.095 ( 0.093)\tLoss 1.2390e+00 (1.3257e+00)\tAcc@1  67.97 ( 62.07)\tAcc@5  88.28 ( 88.98)\n","Epoch: [52][300/391]\tTime  0.092 ( 0.093)\tLoss 1.4432e+00 (1.3299e+00)\tAcc@1  61.72 ( 62.10)\tAcc@5  85.94 ( 88.84)\n","Epoch: [52][330/391]\tTime  0.103 ( 0.093)\tLoss 1.4190e+00 (1.3367e+00)\tAcc@1  63.28 ( 61.91)\tAcc@5  89.84 ( 88.69)\n","Epoch: [52][360/391]\tTime  0.096 ( 0.093)\tLoss 1.4722e+00 (1.3404e+00)\tAcc@1  53.91 ( 61.81)\tAcc@5  85.94 ( 88.65)\n","Epoch: [52][390/391]\tTime  0.082 ( 0.093)\tLoss 1.2959e+00 (1.3405e+00)\tAcc@1  66.25 ( 61.78)\tAcc@5  91.25 ( 88.62)\n","==> Train Accuracy: Acc@1 61.780 || Acc@5 88.624\n","==> Test Accuracy:  Acc@1 55.040 || Acc@5 84.120\n","==> 39.01 seconds to train this epoch\n","\n","\n","----- epoch: 53, lr: 0.1 -----\n","Epoch: [53][  0/391]\tTime  0.282 ( 0.282)\tLoss 1.2968e+00 (1.2968e+00)\tAcc@1  64.06 ( 64.06)\tAcc@5  89.84 ( 89.84)\n","Epoch: [53][ 30/391]\tTime  0.092 ( 0.099)\tLoss 1.4109e+00 (1.2847e+00)\tAcc@1  60.94 ( 63.38)\tAcc@5  87.50 ( 89.62)\n","Epoch: [53][ 60/391]\tTime  0.091 ( 0.096)\tLoss 1.1005e+00 (1.2633e+00)\tAcc@1  67.97 ( 64.23)\tAcc@5  92.97 ( 89.88)\n","Epoch: [53][ 90/391]\tTime  0.091 ( 0.094)\tLoss 1.3053e+00 (1.2864e+00)\tAcc@1  61.72 ( 63.54)\tAcc@5  90.62 ( 89.54)\n","Epoch: [53][120/391]\tTime  0.095 ( 0.094)\tLoss 1.1415e+00 (1.2915e+00)\tAcc@1  69.53 ( 63.51)\tAcc@5  90.62 ( 89.51)\n","Epoch: [53][150/391]\tTime  0.093 ( 0.094)\tLoss 1.3927e+00 (1.3024e+00)\tAcc@1  61.72 ( 63.05)\tAcc@5  87.50 ( 89.40)\n","Epoch: [53][180/391]\tTime  0.092 ( 0.093)\tLoss 1.3010e+00 (1.3159e+00)\tAcc@1  58.59 ( 62.68)\tAcc@5  87.50 ( 89.19)\n","Epoch: [53][210/391]\tTime  0.102 ( 0.093)\tLoss 1.2765e+00 (1.3250e+00)\tAcc@1  64.84 ( 62.56)\tAcc@5  84.38 ( 89.07)\n","Epoch: [53][240/391]\tTime  0.097 ( 0.093)\tLoss 1.0847e+00 (1.3291e+00)\tAcc@1  67.97 ( 62.44)\tAcc@5  93.75 ( 88.98)\n","Epoch: [53][270/391]\tTime  0.091 ( 0.093)\tLoss 1.6260e+00 (1.3362e+00)\tAcc@1  57.03 ( 62.29)\tAcc@5  84.38 ( 88.83)\n","Epoch: [53][300/391]\tTime  0.091 ( 0.093)\tLoss 1.5223e+00 (1.3442e+00)\tAcc@1  57.03 ( 62.02)\tAcc@5  85.94 ( 88.69)\n","Epoch: [53][330/391]\tTime  0.092 ( 0.093)\tLoss 1.4001e+00 (1.3486e+00)\tAcc@1  60.94 ( 61.87)\tAcc@5  84.38 ( 88.61)\n","Epoch: [53][360/391]\tTime  0.092 ( 0.093)\tLoss 1.4551e+00 (1.3484e+00)\tAcc@1  62.50 ( 61.94)\tAcc@5  83.59 ( 88.62)\n","Epoch: [53][390/391]\tTime  0.082 ( 0.093)\tLoss 1.5869e+00 (1.3491e+00)\tAcc@1  61.25 ( 61.91)\tAcc@5  82.50 ( 88.58)\n","==> Train Accuracy: Acc@1 61.912 || Acc@5 88.578\n","==> Test Accuracy:  Acc@1 54.370 || Acc@5 82.380\n","==> 38.93 seconds to train this epoch\n","\n","\n","----- epoch: 54, lr: 0.1 -----\n","Epoch: [54][  0/391]\tTime  0.278 ( 0.278)\tLoss 1.2551e+00 (1.2551e+00)\tAcc@1  64.06 ( 64.06)\tAcc@5  89.06 ( 89.06)\n","Epoch: [54][ 30/391]\tTime  0.094 ( 0.099)\tLoss 1.0596e+00 (1.2293e+00)\tAcc@1  69.53 ( 64.16)\tAcc@5  92.97 ( 90.60)\n","Epoch: [54][ 60/391]\tTime  0.091 ( 0.096)\tLoss 1.3425e+00 (1.2582e+00)\tAcc@1  60.16 ( 63.86)\tAcc@5  91.41 ( 90.02)\n","Epoch: [54][ 90/391]\tTime  0.091 ( 0.094)\tLoss 1.4916e+00 (1.2622e+00)\tAcc@1  60.16 ( 63.91)\tAcc@5  87.50 ( 89.83)\n","Epoch: [54][120/391]\tTime  0.091 ( 0.094)\tLoss 9.3152e-01 (1.2635e+00)\tAcc@1  75.00 ( 63.95)\tAcc@5  92.19 ( 89.77)\n","Epoch: [54][150/391]\tTime  0.091 ( 0.093)\tLoss 1.3995e+00 (1.2771e+00)\tAcc@1  62.50 ( 63.52)\tAcc@5  85.16 ( 89.60)\n","Epoch: [54][180/391]\tTime  0.092 ( 0.093)\tLoss 1.3646e+00 (1.2933e+00)\tAcc@1  58.59 ( 63.16)\tAcc@5  85.94 ( 89.33)\n","Epoch: [54][210/391]\tTime  0.092 ( 0.093)\tLoss 1.3377e+00 (1.2981e+00)\tAcc@1  60.16 ( 63.03)\tAcc@5  90.62 ( 89.24)\n","Epoch: [54][240/391]\tTime  0.088 ( 0.093)\tLoss 1.3819e+00 (1.3092e+00)\tAcc@1  59.38 ( 62.67)\tAcc@5  85.94 ( 89.09)\n","Epoch: [54][270/391]\tTime  0.095 ( 0.093)\tLoss 1.1567e+00 (1.3175e+00)\tAcc@1  67.19 ( 62.53)\tAcc@5  89.84 ( 88.95)\n","Epoch: [54][300/391]\tTime  0.092 ( 0.093)\tLoss 1.4862e+00 (1.3286e+00)\tAcc@1  60.94 ( 62.25)\tAcc@5  85.16 ( 88.81)\n","Epoch: [54][330/391]\tTime  0.092 ( 0.093)\tLoss 1.4000e+00 (1.3289e+00)\tAcc@1  57.81 ( 62.19)\tAcc@5  89.84 ( 88.82)\n","Epoch: [54][360/391]\tTime  0.097 ( 0.093)\tLoss 1.3783e+00 (1.3333e+00)\tAcc@1  58.59 ( 62.09)\tAcc@5  89.06 ( 88.76)\n","Epoch: [54][390/391]\tTime  0.082 ( 0.093)\tLoss 1.3472e+00 (1.3372e+00)\tAcc@1  66.25 ( 62.00)\tAcc@5  85.00 ( 88.70)\n","==> Train Accuracy: Acc@1 62.004 || Acc@5 88.698\n","==> Test Accuracy:  Acc@1 55.080 || Acc@5 82.410\n","==> 38.92 seconds to train this epoch\n","\n","\n","----- epoch: 55, lr: 0.1 -----\n","Epoch: [55][  0/391]\tTime  0.270 ( 0.270)\tLoss 1.2931e+00 (1.2931e+00)\tAcc@1  60.94 ( 60.94)\tAcc@5  89.84 ( 89.84)\n","Epoch: [55][ 30/391]\tTime  0.092 ( 0.099)\tLoss 1.3901e+00 (1.2662e+00)\tAcc@1  58.59 ( 63.66)\tAcc@5  91.41 ( 90.07)\n","Epoch: [55][ 60/391]\tTime  0.092 ( 0.095)\tLoss 1.2543e+00 (1.2748e+00)\tAcc@1  64.06 ( 63.63)\tAcc@5  89.84 ( 89.82)\n","Epoch: [55][ 90/391]\tTime  0.094 ( 0.094)\tLoss 1.2350e+00 (1.2753e+00)\tAcc@1  67.19 ( 63.75)\tAcc@5  88.28 ( 89.75)\n","Epoch: [55][120/391]\tTime  0.094 ( 0.094)\tLoss 1.3226e+00 (1.2946e+00)\tAcc@1  66.41 ( 63.54)\tAcc@5  88.28 ( 89.48)\n","Epoch: [55][150/391]\tTime  0.098 ( 0.093)\tLoss 1.3401e+00 (1.3065e+00)\tAcc@1  61.72 ( 63.21)\tAcc@5  89.84 ( 89.28)\n","Epoch: [55][180/391]\tTime  0.096 ( 0.093)\tLoss 1.4338e+00 (1.3102e+00)\tAcc@1  59.38 ( 62.90)\tAcc@5  86.72 ( 89.24)\n","Epoch: [55][210/391]\tTime  0.092 ( 0.093)\tLoss 1.3542e+00 (1.3195e+00)\tAcc@1  59.38 ( 62.62)\tAcc@5  88.28 ( 89.07)\n","Epoch: [55][240/391]\tTime  0.091 ( 0.093)\tLoss 1.4463e+00 (1.3286e+00)\tAcc@1  64.06 ( 62.44)\tAcc@5  91.41 ( 88.91)\n","Epoch: [55][270/391]\tTime  0.089 ( 0.093)\tLoss 1.2669e+00 (1.3282e+00)\tAcc@1  64.06 ( 62.40)\tAcc@5  90.62 ( 88.91)\n","Epoch: [55][300/391]\tTime  0.091 ( 0.093)\tLoss 1.4972e+00 (1.3266e+00)\tAcc@1  58.59 ( 62.42)\tAcc@5  86.72 ( 88.93)\n","Epoch: [55][330/391]\tTime  0.089 ( 0.093)\tLoss 1.2046e+00 (1.3240e+00)\tAcc@1  64.84 ( 62.48)\tAcc@5  89.06 ( 88.96)\n","Epoch: [55][360/391]\tTime  0.095 ( 0.093)\tLoss 1.4613e+00 (1.3245e+00)\tAcc@1  60.16 ( 62.43)\tAcc@5  85.16 ( 88.98)\n","Epoch: [55][390/391]\tTime  0.080 ( 0.093)\tLoss 1.2685e+00 (1.3343e+00)\tAcc@1  63.75 ( 62.17)\tAcc@5  91.25 ( 88.86)\n","==> Train Accuracy: Acc@1 62.172 || Acc@5 88.856\n","==> Test Accuracy:  Acc@1 56.880 || Acc@5 85.760\n","==> 38.88 seconds to train this epoch\n","\n","\n","----- epoch: 56, lr: 0.1 -----\n","Epoch: [56][  0/391]\tTime  0.265 ( 0.265)\tLoss 1.2321e+00 (1.2321e+00)\tAcc@1  64.84 ( 64.84)\tAcc@5  89.84 ( 89.84)\n","Epoch: [56][ 30/391]\tTime  0.092 ( 0.099)\tLoss 1.1537e+00 (1.2358e+00)\tAcc@1  67.97 ( 64.62)\tAcc@5  92.19 ( 90.25)\n","Epoch: [56][ 60/391]\tTime  0.090 ( 0.096)\tLoss 1.4234e+00 (1.2463e+00)\tAcc@1  60.94 ( 64.66)\tAcc@5  86.72 ( 89.91)\n","Epoch: [56][ 90/391]\tTime  0.094 ( 0.095)\tLoss 1.1824e+00 (1.2781e+00)\tAcc@1  65.62 ( 63.96)\tAcc@5  89.84 ( 89.50)\n","Epoch: [56][120/391]\tTime  0.090 ( 0.094)\tLoss 1.5121e+00 (1.2880e+00)\tAcc@1  54.69 ( 63.61)\tAcc@5  88.28 ( 89.40)\n","Epoch: [56][150/391]\tTime  0.092 ( 0.094)\tLoss 1.2953e+00 (1.2973e+00)\tAcc@1  61.72 ( 63.36)\tAcc@5  90.62 ( 89.24)\n","Epoch: [56][180/391]\tTime  0.094 ( 0.093)\tLoss 1.3517e+00 (1.2988e+00)\tAcc@1  64.06 ( 63.17)\tAcc@5  89.84 ( 89.36)\n","Epoch: [56][210/391]\tTime  0.093 ( 0.093)\tLoss 1.2761e+00 (1.3023e+00)\tAcc@1  63.28 ( 63.07)\tAcc@5  92.97 ( 89.33)\n","Epoch: [56][240/391]\tTime  0.092 ( 0.093)\tLoss 1.2583e+00 (1.3098e+00)\tAcc@1  61.72 ( 62.89)\tAcc@5  88.28 ( 89.12)\n","Epoch: [56][270/391]\tTime  0.091 ( 0.093)\tLoss 1.1889e+00 (1.3143e+00)\tAcc@1  71.09 ( 62.84)\tAcc@5  89.06 ( 89.07)\n","Epoch: [56][300/391]\tTime  0.095 ( 0.093)\tLoss 1.2146e+00 (1.3180e+00)\tAcc@1  60.94 ( 62.76)\tAcc@5  93.75 ( 88.99)\n","Epoch: [56][330/391]\tTime  0.098 ( 0.093)\tLoss 1.2813e+00 (1.3219e+00)\tAcc@1  61.72 ( 62.66)\tAcc@5  88.28 ( 88.93)\n","Epoch: [56][360/391]\tTime  0.091 ( 0.093)\tLoss 1.2755e+00 (1.3242e+00)\tAcc@1  62.50 ( 62.67)\tAcc@5  91.41 ( 88.87)\n","Epoch: [56][390/391]\tTime  0.081 ( 0.093)\tLoss 1.2304e+00 (1.3292e+00)\tAcc@1  62.50 ( 62.62)\tAcc@5  91.25 ( 88.78)\n","==> Train Accuracy: Acc@1 62.624 || Acc@5 88.776\n","==> Test Accuracy:  Acc@1 57.670 || Acc@5 86.040\n","==> 38.99 seconds to train this epoch\n","\n","\n","----- epoch: 57, lr: 0.1 -----\n","Epoch: [57][  0/391]\tTime  0.268 ( 0.268)\tLoss 1.1999e+00 (1.1999e+00)\tAcc@1  67.19 ( 67.19)\tAcc@5  91.41 ( 91.41)\n","Epoch: [57][ 30/391]\tTime  0.094 ( 0.099)\tLoss 1.3566e+00 (1.3190e+00)\tAcc@1  62.50 ( 62.47)\tAcc@5  88.28 ( 89.21)\n","Epoch: [57][ 60/391]\tTime  0.089 ( 0.095)\tLoss 1.1861e+00 (1.2902e+00)\tAcc@1  66.41 ( 63.11)\tAcc@5  89.84 ( 89.56)\n","Epoch: [57][ 90/391]\tTime  0.091 ( 0.094)\tLoss 1.3717e+00 (1.3083e+00)\tAcc@1  57.81 ( 62.51)\tAcc@5  88.28 ( 89.48)\n","Epoch: [57][120/391]\tTime  0.091 ( 0.094)\tLoss 1.3071e+00 (1.3175e+00)\tAcc@1  63.28 ( 62.45)\tAcc@5  90.62 ( 89.17)\n","Epoch: [57][150/391]\tTime  0.097 ( 0.094)\tLoss 1.5247e+00 (1.3113e+00)\tAcc@1  60.16 ( 62.43)\tAcc@5  85.94 ( 89.23)\n","Epoch: [57][180/391]\tTime  0.092 ( 0.093)\tLoss 1.1697e+00 (1.3112e+00)\tAcc@1  67.19 ( 62.47)\tAcc@5  91.41 ( 89.25)\n","Epoch: [57][210/391]\tTime  0.091 ( 0.093)\tLoss 1.4185e+00 (1.3173e+00)\tAcc@1  62.50 ( 62.30)\tAcc@5  90.62 ( 89.18)\n","Epoch: [57][240/391]\tTime  0.092 ( 0.093)\tLoss 1.4011e+00 (1.3243e+00)\tAcc@1  60.16 ( 62.10)\tAcc@5  86.72 ( 89.06)\n","Epoch: [57][270/391]\tTime  0.091 ( 0.093)\tLoss 1.4008e+00 (1.3285e+00)\tAcc@1  59.38 ( 62.04)\tAcc@5  87.50 ( 89.00)\n","Epoch: [57][300/391]\tTime  0.091 ( 0.093)\tLoss 1.1734e+00 (1.3282e+00)\tAcc@1  66.41 ( 62.07)\tAcc@5  90.62 ( 89.00)\n","Epoch: [57][330/391]\tTime  0.093 ( 0.093)\tLoss 1.3387e+00 (1.3296e+00)\tAcc@1  61.72 ( 62.07)\tAcc@5  88.28 ( 88.98)\n","Epoch: [57][360/391]\tTime  0.092 ( 0.093)\tLoss 1.5028e+00 (1.3336e+00)\tAcc@1  56.25 ( 62.00)\tAcc@5  86.72 ( 88.85)\n","Epoch: [57][390/391]\tTime  0.082 ( 0.093)\tLoss 1.3334e+00 (1.3360e+00)\tAcc@1  60.00 ( 61.95)\tAcc@5  87.50 ( 88.86)\n","==> Train Accuracy: Acc@1 61.954 || Acc@5 88.856\n","==> Test Accuracy:  Acc@1 57.220 || Acc@5 85.540\n","==> 38.90 seconds to train this epoch\n","\n","\n","----- epoch: 58, lr: 0.1 -----\n","Epoch: [58][  0/391]\tTime  0.280 ( 0.280)\tLoss 1.3600e+00 (1.3600e+00)\tAcc@1  61.72 ( 61.72)\tAcc@5  90.62 ( 90.62)\n","Epoch: [58][ 30/391]\tTime  0.089 ( 0.099)\tLoss 1.4599e+00 (1.2939e+00)\tAcc@1  58.59 ( 62.30)\tAcc@5  85.16 ( 88.91)\n","Epoch: [58][ 60/391]\tTime  0.091 ( 0.096)\tLoss 1.2408e+00 (1.2602e+00)\tAcc@1  67.97 ( 63.36)\tAcc@5  90.62 ( 89.63)\n","Epoch: [58][ 90/391]\tTime  0.093 ( 0.095)\tLoss 1.2654e+00 (1.2689e+00)\tAcc@1  63.28 ( 63.27)\tAcc@5  92.19 ( 89.45)\n","Epoch: [58][120/391]\tTime  0.091 ( 0.094)\tLoss 1.2072e+00 (1.2941e+00)\tAcc@1  64.06 ( 62.76)\tAcc@5  89.06 ( 89.15)\n","Epoch: [58][150/391]\tTime  0.092 ( 0.094)\tLoss 1.6108e+00 (1.2977e+00)\tAcc@1  56.25 ( 62.97)\tAcc@5  83.59 ( 89.00)\n","Epoch: [58][180/391]\tTime  0.093 ( 0.094)\tLoss 1.2721e+00 (1.3005e+00)\tAcc@1  63.28 ( 62.89)\tAcc@5  86.72 ( 89.03)\n","Epoch: [58][210/391]\tTime  0.100 ( 0.094)\tLoss 1.5129e+00 (1.3081e+00)\tAcc@1  57.81 ( 62.64)\tAcc@5  85.16 ( 88.93)\n","Epoch: [58][240/391]\tTime  0.091 ( 0.093)\tLoss 1.2067e+00 (1.3179e+00)\tAcc@1  67.19 ( 62.48)\tAcc@5  87.50 ( 88.70)\n","Epoch: [58][270/391]\tTime  0.092 ( 0.093)\tLoss 1.1428e+00 (1.3208e+00)\tAcc@1  67.19 ( 62.50)\tAcc@5  92.97 ( 88.71)\n","Epoch: [58][300/391]\tTime  0.097 ( 0.093)\tLoss 1.1290e+00 (1.3217e+00)\tAcc@1  69.53 ( 62.54)\tAcc@5  86.72 ( 88.71)\n","Epoch: [58][330/391]\tTime  0.092 ( 0.093)\tLoss 1.3013e+00 (1.3199e+00)\tAcc@1  60.94 ( 62.64)\tAcc@5  91.41 ( 88.76)\n","Epoch: [58][360/391]\tTime  0.096 ( 0.093)\tLoss 1.3397e+00 (1.3248e+00)\tAcc@1  64.06 ( 62.52)\tAcc@5  89.06 ( 88.75)\n","Epoch: [58][390/391]\tTime  0.079 ( 0.093)\tLoss 1.3192e+00 (1.3283e+00)\tAcc@1  63.75 ( 62.38)\tAcc@5  91.25 ( 88.72)\n","==> Train Accuracy: Acc@1 62.382 || Acc@5 88.718\n","==> Test Accuracy:  Acc@1 59.100 || Acc@5 85.910\n","==> 39.02 seconds to train this epoch\n","\n","\n","----- epoch: 59, lr: 0.1 -----\n","Epoch: [59][  0/391]\tTime  0.270 ( 0.270)\tLoss 1.1720e+00 (1.1720e+00)\tAcc@1  64.06 ( 64.06)\tAcc@5  93.75 ( 93.75)\n","Epoch: [59][ 30/391]\tTime  0.093 ( 0.099)\tLoss 1.1621e+00 (1.2974e+00)\tAcc@1  65.62 ( 63.53)\tAcc@5  95.31 ( 89.62)\n","Epoch: [59][ 60/391]\tTime  0.096 ( 0.095)\tLoss 1.4470e+00 (1.2890e+00)\tAcc@1  60.94 ( 63.43)\tAcc@5  84.38 ( 89.60)\n","Epoch: [59][ 90/391]\tTime  0.092 ( 0.094)\tLoss 1.2344e+00 (1.3063e+00)\tAcc@1  69.53 ( 63.11)\tAcc@5  92.19 ( 89.44)\n","Epoch: [59][120/391]\tTime  0.091 ( 0.094)\tLoss 1.5213e+00 (1.2942e+00)\tAcc@1  57.03 ( 63.15)\tAcc@5  85.94 ( 89.42)\n","Epoch: [59][150/391]\tTime  0.092 ( 0.093)\tLoss 1.2708e+00 (1.3050e+00)\tAcc@1  64.06 ( 62.85)\tAcc@5  89.84 ( 89.20)\n","Epoch: [59][180/391]\tTime  0.089 ( 0.093)\tLoss 1.1175e+00 (1.3037e+00)\tAcc@1  69.53 ( 62.94)\tAcc@5  89.84 ( 89.15)\n","Epoch: [59][210/391]\tTime  0.095 ( 0.093)\tLoss 1.4342e+00 (1.3101e+00)\tAcc@1  57.03 ( 62.77)\tAcc@5  89.06 ( 89.07)\n","Epoch: [59][240/391]\tTime  0.093 ( 0.093)\tLoss 1.2616e+00 (1.3153e+00)\tAcc@1  60.16 ( 62.73)\tAcc@5  92.19 ( 88.98)\n","Epoch: [59][270/391]\tTime  0.087 ( 0.093)\tLoss 1.1758e+00 (1.3192e+00)\tAcc@1  68.75 ( 62.62)\tAcc@5  90.62 ( 89.00)\n","Epoch: [59][300/391]\tTime  0.090 ( 0.093)\tLoss 1.5228e+00 (1.3171e+00)\tAcc@1  56.25 ( 62.71)\tAcc@5  86.72 ( 88.98)\n","Epoch: [59][330/391]\tTime  0.095 ( 0.093)\tLoss 1.2623e+00 (1.3208e+00)\tAcc@1  64.06 ( 62.59)\tAcc@5  90.62 ( 88.94)\n","Epoch: [59][360/391]\tTime  0.098 ( 0.093)\tLoss 1.4429e+00 (1.3219e+00)\tAcc@1  61.72 ( 62.59)\tAcc@5  85.16 ( 88.92)\n","Epoch: [59][390/391]\tTime  0.082 ( 0.093)\tLoss 1.4989e+00 (1.3275e+00)\tAcc@1  57.50 ( 62.46)\tAcc@5  88.75 ( 88.82)\n","==> Train Accuracy: Acc@1 62.462 || Acc@5 88.816\n","==> Test Accuracy:  Acc@1 59.750 || Acc@5 87.270\n","==> 38.93 seconds to train this epoch\n","\n","\n","----- epoch: 60, lr: 0.020000000000000004 -----\n","Epoch: [60][  0/391]\tTime  0.278 ( 0.278)\tLoss 1.5410e+00 (1.5410e+00)\tAcc@1  57.03 ( 57.03)\tAcc@5  85.16 ( 85.16)\n","Epoch: [60][ 30/391]\tTime  0.093 ( 0.100)\tLoss 8.5727e-01 (1.0767e+00)\tAcc@1  74.22 ( 68.85)\tAcc@5  93.75 ( 91.89)\n","Epoch: [60][ 60/391]\tTime  0.092 ( 0.096)\tLoss 1.0128e+00 (1.0328e+00)\tAcc@1  68.75 ( 70.24)\tAcc@5  92.97 ( 92.64)\n","Epoch: [60][ 90/391]\tTime  0.084 ( 0.095)\tLoss 8.9420e-01 (1.0096e+00)\tAcc@1  71.09 ( 70.99)\tAcc@5  94.53 ( 92.84)\n","Epoch: [60][120/391]\tTime  0.093 ( 0.094)\tLoss 9.1865e-01 (9.8205e-01)\tAcc@1  75.00 ( 71.86)\tAcc@5  92.19 ( 93.10)\n","Epoch: [60][150/391]\tTime  0.092 ( 0.094)\tLoss 9.8508e-01 (9.6772e-01)\tAcc@1  71.88 ( 72.17)\tAcc@5  89.84 ( 93.23)\n","Epoch: [60][180/391]\tTime  0.090 ( 0.093)\tLoss 9.3604e-01 (9.5114e-01)\tAcc@1  70.31 ( 72.62)\tAcc@5  95.31 ( 93.40)\n","Epoch: [60][210/391]\tTime  0.092 ( 0.093)\tLoss 8.7895e-01 (9.3525e-01)\tAcc@1  78.12 ( 72.99)\tAcc@5  93.75 ( 93.55)\n","Epoch: [60][240/391]\tTime  0.093 ( 0.093)\tLoss 8.7171e-01 (9.2205e-01)\tAcc@1  71.09 ( 73.30)\tAcc@5  95.31 ( 93.70)\n","Epoch: [60][270/391]\tTime  0.094 ( 0.093)\tLoss 9.0979e-01 (9.1068e-01)\tAcc@1  72.66 ( 73.57)\tAcc@5  95.31 ( 93.85)\n","Epoch: [60][300/391]\tTime  0.097 ( 0.093)\tLoss 7.6440e-01 (9.0114e-01)\tAcc@1  76.56 ( 73.85)\tAcc@5  98.44 ( 93.94)\n","Epoch: [60][330/391]\tTime  0.092 ( 0.093)\tLoss 8.4947e-01 (8.9805e-01)\tAcc@1  73.44 ( 73.97)\tAcc@5  96.88 ( 93.99)\n","Epoch: [60][360/391]\tTime  0.090 ( 0.093)\tLoss 8.0355e-01 (8.9465e-01)\tAcc@1  75.78 ( 74.02)\tAcc@5  96.88 ( 94.02)\n","Epoch: [60][390/391]\tTime  0.082 ( 0.093)\tLoss 7.9687e-01 (8.8763e-01)\tAcc@1  80.00 ( 74.18)\tAcc@5  93.75 ( 94.10)\n","==> Train Accuracy: Acc@1 74.182 || Acc@5 94.102\n","==> Test Accuracy:  Acc@1 72.530 || Acc@5 93.140\n","==> 38.89 seconds to train this epoch\n","\n","\n","----- epoch: 61, lr: 0.020000000000000004 -----\n","Epoch: [61][  0/391]\tTime  0.273 ( 0.273)\tLoss 6.7519e-01 (6.7519e-01)\tAcc@1  81.25 ( 81.25)\tAcc@5  97.66 ( 97.66)\n","Epoch: [61][ 30/391]\tTime  0.094 ( 0.100)\tLoss 6.8441e-01 (7.5351e-01)\tAcc@1  78.12 ( 78.00)\tAcc@5  98.44 ( 95.16)\n","Epoch: [61][ 60/391]\tTime  0.090 ( 0.096)\tLoss 8.5382e-01 (7.4211e-01)\tAcc@1  78.12 ( 78.48)\tAcc@5  91.41 ( 95.43)\n","Epoch: [61][ 90/391]\tTime  0.091 ( 0.095)\tLoss 7.3496e-01 (7.4087e-01)\tAcc@1  80.47 ( 78.46)\tAcc@5  96.88 ( 95.57)\n","Epoch: [61][120/391]\tTime  0.090 ( 0.094)\tLoss 8.6078e-01 (7.3789e-01)\tAcc@1  75.00 ( 78.46)\tAcc@5  94.53 ( 95.66)\n","Epoch: [61][150/391]\tTime  0.088 ( 0.094)\tLoss 7.7654e-01 (7.4129e-01)\tAcc@1  78.12 ( 78.28)\tAcc@5  95.31 ( 95.61)\n","Epoch: [61][180/391]\tTime  0.100 ( 0.093)\tLoss 8.5221e-01 (7.4184e-01)\tAcc@1  77.34 ( 78.24)\tAcc@5  93.75 ( 95.63)\n","Epoch: [61][210/391]\tTime  0.094 ( 0.093)\tLoss 6.2088e-01 (7.4237e-01)\tAcc@1  80.47 ( 78.18)\tAcc@5  96.09 ( 95.65)\n","Epoch: [61][240/391]\tTime  0.096 ( 0.093)\tLoss 6.0523e-01 (7.4520e-01)\tAcc@1  81.25 ( 78.11)\tAcc@5  96.88 ( 95.63)\n","Epoch: [61][270/391]\tTime  0.089 ( 0.093)\tLoss 6.8781e-01 (7.4672e-01)\tAcc@1  78.91 ( 78.02)\tAcc@5  96.09 ( 95.61)\n","Epoch: [61][300/391]\tTime  0.096 ( 0.093)\tLoss 6.7255e-01 (7.4524e-01)\tAcc@1  78.91 ( 78.04)\tAcc@5  96.88 ( 95.66)\n","Epoch: [61][330/391]\tTime  0.092 ( 0.093)\tLoss 8.1532e-01 (7.4565e-01)\tAcc@1  73.44 ( 78.02)\tAcc@5  96.09 ( 95.66)\n","Epoch: [61][360/391]\tTime  0.091 ( 0.093)\tLoss 6.4323e-01 (7.4906e-01)\tAcc@1  78.91 ( 77.89)\tAcc@5  96.88 ( 95.61)\n","Epoch: [61][390/391]\tTime  0.082 ( 0.093)\tLoss 8.9909e-01 (7.5013e-01)\tAcc@1  73.75 ( 77.84)\tAcc@5  91.25 ( 95.60)\n","==> Train Accuracy: Acc@1 77.844 || Acc@5 95.596\n","==> Test Accuracy:  Acc@1 73.640 || Acc@5 93.780\n","==> 38.96 seconds to train this epoch\n","\n","\n","----- epoch: 62, lr: 0.020000000000000004 -----\n","Epoch: [62][  0/391]\tTime  0.260 ( 0.260)\tLoss 8.0931e-01 (8.0931e-01)\tAcc@1  75.78 ( 75.78)\tAcc@5  96.88 ( 96.88)\n","Epoch: [62][ 30/391]\tTime  0.091 ( 0.099)\tLoss 5.8524e-01 (6.9081e-01)\tAcc@1  85.16 ( 79.64)\tAcc@5  97.66 ( 96.40)\n","Epoch: [62][ 60/391]\tTime  0.094 ( 0.096)\tLoss 6.4588e-01 (6.7207e-01)\tAcc@1  77.34 ( 80.10)\tAcc@5  96.88 ( 96.59)\n","Epoch: [62][ 90/391]\tTime  0.091 ( 0.094)\tLoss 6.1073e-01 (6.7643e-01)\tAcc@1  80.47 ( 79.90)\tAcc@5  98.44 ( 96.45)\n","Epoch: [62][120/391]\tTime  0.092 ( 0.094)\tLoss 7.2998e-01 (6.7527e-01)\tAcc@1  84.38 ( 80.00)\tAcc@5  96.88 ( 96.36)\n","Epoch: [62][150/391]\tTime  0.092 ( 0.093)\tLoss 6.6049e-01 (6.7964e-01)\tAcc@1  78.91 ( 79.74)\tAcc@5  96.88 ( 96.24)\n","Epoch: [62][180/391]\tTime  0.094 ( 0.093)\tLoss 6.5363e-01 (6.8223e-01)\tAcc@1  82.81 ( 79.79)\tAcc@5  96.88 ( 96.22)\n","Epoch: [62][210/391]\tTime  0.091 ( 0.093)\tLoss 4.8246e-01 (6.8921e-01)\tAcc@1  87.50 ( 79.58)\tAcc@5  96.88 ( 96.18)\n","Epoch: [62][240/391]\tTime  0.089 ( 0.093)\tLoss 6.6971e-01 (6.9171e-01)\tAcc@1  77.34 ( 79.47)\tAcc@5  97.66 ( 96.17)\n","Epoch: [62][270/391]\tTime  0.091 ( 0.093)\tLoss 8.0760e-01 (6.9182e-01)\tAcc@1  70.31 ( 79.48)\tAcc@5  96.88 ( 96.16)\n","Epoch: [62][300/391]\tTime  0.090 ( 0.093)\tLoss 6.8718e-01 (6.9534e-01)\tAcc@1  82.03 ( 79.39)\tAcc@5  96.88 ( 96.17)\n","Epoch: [62][330/391]\tTime  0.091 ( 0.093)\tLoss 4.7056e-01 (6.9666e-01)\tAcc@1  88.28 ( 79.33)\tAcc@5  99.22 ( 96.15)\n","Epoch: [62][360/391]\tTime  0.092 ( 0.093)\tLoss 7.3472e-01 (6.9696e-01)\tAcc@1  82.81 ( 79.32)\tAcc@5  94.53 ( 96.15)\n","Epoch: [62][390/391]\tTime  0.081 ( 0.093)\tLoss 5.6858e-01 (6.9899e-01)\tAcc@1  82.50 ( 79.28)\tAcc@5  98.75 ( 96.13)\n","==> Train Accuracy: Acc@1 79.280 || Acc@5 96.130\n","==> Test Accuracy:  Acc@1 73.330 || Acc@5 93.640\n","==> 38.84 seconds to train this epoch\n","\n","\n","----- epoch: 63, lr: 0.020000000000000004 -----\n","Epoch: [63][  0/391]\tTime  0.271 ( 0.271)\tLoss 6.6008e-01 (6.6008e-01)\tAcc@1  79.69 ( 79.69)\tAcc@5  96.88 ( 96.88)\n","Epoch: [63][ 30/391]\tTime  0.091 ( 0.098)\tLoss 6.5239e-01 (6.4170e-01)\tAcc@1  78.91 ( 80.82)\tAcc@5  97.66 ( 96.57)\n","Epoch: [63][ 60/391]\tTime  0.091 ( 0.095)\tLoss 6.1913e-01 (6.4994e-01)\tAcc@1  81.25 ( 80.51)\tAcc@5  99.22 ( 96.41)\n","Epoch: [63][ 90/391]\tTime  0.088 ( 0.094)\tLoss 6.9625e-01 (6.5385e-01)\tAcc@1  79.69 ( 80.42)\tAcc@5  96.09 ( 96.56)\n","Epoch: [63][120/391]\tTime  0.085 ( 0.094)\tLoss 5.5748e-01 (6.5072e-01)\tAcc@1  82.03 ( 80.44)\tAcc@5  98.44 ( 96.57)\n","Epoch: [63][150/391]\tTime  0.091 ( 0.093)\tLoss 8.6296e-01 (6.5645e-01)\tAcc@1  77.34 ( 80.28)\tAcc@5  92.97 ( 96.52)\n","Epoch: [63][180/391]\tTime  0.092 ( 0.093)\tLoss 5.6228e-01 (6.4953e-01)\tAcc@1  82.03 ( 80.50)\tAcc@5  96.09 ( 96.62)\n","Epoch: [63][210/391]\tTime  0.095 ( 0.093)\tLoss 5.2793e-01 (6.5166e-01)\tAcc@1  82.81 ( 80.46)\tAcc@5  96.88 ( 96.58)\n","Epoch: [63][240/391]\tTime  0.093 ( 0.093)\tLoss 4.2138e-01 (6.5256e-01)\tAcc@1  85.16 ( 80.47)\tAcc@5  98.44 ( 96.55)\n","Epoch: [63][270/391]\tTime  0.090 ( 0.093)\tLoss 6.3927e-01 (6.5484e-01)\tAcc@1  82.03 ( 80.39)\tAcc@5  96.88 ( 96.51)\n","Epoch: [63][300/391]\tTime  0.093 ( 0.093)\tLoss 8.3010e-01 (6.6025e-01)\tAcc@1  71.88 ( 80.24)\tAcc@5  96.09 ( 96.47)\n","Epoch: [63][330/391]\tTime  0.090 ( 0.093)\tLoss 5.8432e-01 (6.6161e-01)\tAcc@1  82.03 ( 80.20)\tAcc@5  95.31 ( 96.49)\n","Epoch: [63][360/391]\tTime  0.094 ( 0.093)\tLoss 5.8741e-01 (6.6117e-01)\tAcc@1  82.03 ( 80.18)\tAcc@5  97.66 ( 96.54)\n","Epoch: [63][390/391]\tTime  0.082 ( 0.093)\tLoss 7.6103e-01 (6.6055e-01)\tAcc@1  80.00 ( 80.18)\tAcc@5  93.75 ( 96.54)\n","==> Train Accuracy: Acc@1 80.184 || Acc@5 96.538\n","==> Test Accuracy:  Acc@1 73.590 || Acc@5 93.890\n","==> 38.91 seconds to train this epoch\n","\n","\n","----- epoch: 64, lr: 0.020000000000000004 -----\n","Epoch: [64][  0/391]\tTime  0.264 ( 0.264)\tLoss 6.3315e-01 (6.3315e-01)\tAcc@1  82.03 ( 82.03)\tAcc@5  97.66 ( 97.66)\n","Epoch: [64][ 30/391]\tTime  0.089 ( 0.099)\tLoss 4.5168e-01 (6.2004e-01)\tAcc@1  85.16 ( 80.90)\tAcc@5  99.22 ( 97.33)\n","Epoch: [64][ 60/391]\tTime  0.092 ( 0.096)\tLoss 5.4057e-01 (6.1124e-01)\tAcc@1  79.69 ( 81.12)\tAcc@5  99.22 ( 97.28)\n","Epoch: [64][ 90/391]\tTime  0.093 ( 0.095)\tLoss 6.6146e-01 (6.1572e-01)\tAcc@1  79.69 ( 81.10)\tAcc@5  96.09 ( 97.04)\n","Epoch: [64][120/391]\tTime  0.093 ( 0.094)\tLoss 6.3540e-01 (6.1339e-01)\tAcc@1  82.81 ( 81.37)\tAcc@5  96.09 ( 96.95)\n","Epoch: [64][150/391]\tTime  0.090 ( 0.094)\tLoss 6.2503e-01 (6.1723e-01)\tAcc@1  76.56 ( 81.33)\tAcc@5  99.22 ( 96.90)\n","Epoch: [64][180/391]\tTime  0.090 ( 0.093)\tLoss 7.0306e-01 (6.1961e-01)\tAcc@1  78.91 ( 81.25)\tAcc@5  96.09 ( 96.90)\n","Epoch: [64][210/391]\tTime  0.094 ( 0.093)\tLoss 4.3218e-01 (6.2317e-01)\tAcc@1  89.06 ( 81.14)\tAcc@5  98.44 ( 96.86)\n","Epoch: [64][240/391]\tTime  0.094 ( 0.093)\tLoss 7.0522e-01 (6.2426e-01)\tAcc@1  76.56 ( 81.08)\tAcc@5  95.31 ( 96.87)\n","Epoch: [64][270/391]\tTime  0.099 ( 0.093)\tLoss 6.1728e-01 (6.2829e-01)\tAcc@1  78.12 ( 81.01)\tAcc@5  98.44 ( 96.86)\n","Epoch: [64][300/391]\tTime  0.091 ( 0.093)\tLoss 6.3518e-01 (6.3269e-01)\tAcc@1  81.25 ( 80.90)\tAcc@5  96.88 ( 96.82)\n","Epoch: [64][330/391]\tTime  0.094 ( 0.093)\tLoss 7.1591e-01 (6.3481e-01)\tAcc@1  77.34 ( 80.81)\tAcc@5  96.88 ( 96.81)\n","Epoch: [64][360/391]\tTime  0.089 ( 0.093)\tLoss 8.6799e-01 (6.3545e-01)\tAcc@1  75.78 ( 80.79)\tAcc@5  91.41 ( 96.78)\n","Epoch: [64][390/391]\tTime  0.081 ( 0.093)\tLoss 7.7443e-01 (6.3890e-01)\tAcc@1  78.75 ( 80.69)\tAcc@5  95.00 ( 96.73)\n","==> Train Accuracy: Acc@1 80.686 || Acc@5 96.732\n","==> Test Accuracy:  Acc@1 73.400 || Acc@5 93.830\n","==> 38.92 seconds to train this epoch\n","\n","\n","----- epoch: 65, lr: 0.020000000000000004 -----\n","Epoch: [65][  0/391]\tTime  0.266 ( 0.266)\tLoss 5.4983e-01 (5.4983e-01)\tAcc@1  85.16 ( 85.16)\tAcc@5  95.31 ( 95.31)\n","Epoch: [65][ 30/391]\tTime  0.093 ( 0.099)\tLoss 5.8442e-01 (5.6928e-01)\tAcc@1  81.25 ( 82.99)\tAcc@5  99.22 ( 97.23)\n","Epoch: [65][ 60/391]\tTime  0.097 ( 0.096)\tLoss 6.3621e-01 (5.7761e-01)\tAcc@1  81.25 ( 82.80)\tAcc@5  93.75 ( 97.02)\n","Epoch: [65][ 90/391]\tTime  0.092 ( 0.095)\tLoss 6.1580e-01 (5.7780e-01)\tAcc@1  82.03 ( 82.79)\tAcc@5  97.66 ( 97.30)\n","Epoch: [65][120/391]\tTime  0.092 ( 0.094)\tLoss 7.5658e-01 (5.8907e-01)\tAcc@1  76.56 ( 82.44)\tAcc@5  96.09 ( 97.20)\n","Epoch: [65][150/391]\tTime  0.090 ( 0.094)\tLoss 4.8282e-01 (5.9447e-01)\tAcc@1  85.16 ( 82.33)\tAcc@5  96.09 ( 97.13)\n","Epoch: [65][180/391]\tTime  0.091 ( 0.093)\tLoss 6.4127e-01 (5.9961e-01)\tAcc@1  83.59 ( 82.15)\tAcc@5  96.88 ( 97.10)\n","Epoch: [65][210/391]\tTime  0.092 ( 0.093)\tLoss 5.0682e-01 (6.0244e-01)\tAcc@1  86.72 ( 82.05)\tAcc@5  98.44 ( 97.08)\n","Epoch: [65][240/391]\tTime  0.095 ( 0.093)\tLoss 7.8574e-01 (6.0795e-01)\tAcc@1  77.34 ( 81.88)\tAcc@5  96.09 ( 97.01)\n","Epoch: [65][270/391]\tTime  0.095 ( 0.093)\tLoss 5.8471e-01 (6.0976e-01)\tAcc@1  81.25 ( 81.77)\tAcc@5  97.66 ( 97.04)\n","Epoch: [65][300/391]\tTime  0.090 ( 0.093)\tLoss 5.0671e-01 (6.1217e-01)\tAcc@1  83.59 ( 81.69)\tAcc@5  96.88 ( 97.01)\n","Epoch: [65][330/391]\tTime  0.092 ( 0.093)\tLoss 5.9292e-01 (6.1444e-01)\tAcc@1  83.59 ( 81.57)\tAcc@5  97.66 ( 97.00)\n","Epoch: [65][360/391]\tTime  0.090 ( 0.093)\tLoss 6.7674e-01 (6.1682e-01)\tAcc@1  78.12 ( 81.48)\tAcc@5  98.44 ( 96.98)\n","Epoch: [65][390/391]\tTime  0.082 ( 0.093)\tLoss 6.1681e-01 (6.1940e-01)\tAcc@1  81.25 ( 81.40)\tAcc@5  93.75 ( 96.97)\n","==> Train Accuracy: Acc@1 81.396 || Acc@5 96.966\n","==> Test Accuracy:  Acc@1 72.280 || Acc@5 93.280\n","==> 38.95 seconds to train this epoch\n","\n","\n","----- epoch: 66, lr: 0.020000000000000004 -----\n","Epoch: [66][  0/391]\tTime  0.269 ( 0.269)\tLoss 6.6407e-01 (6.6407e-01)\tAcc@1  80.47 ( 80.47)\tAcc@5  97.66 ( 97.66)\n","Epoch: [66][ 30/391]\tTime  0.093 ( 0.098)\tLoss 5.9220e-01 (5.9240e-01)\tAcc@1  81.25 ( 82.43)\tAcc@5  96.88 ( 97.10)\n","Epoch: [66][ 60/391]\tTime  0.095 ( 0.095)\tLoss 6.3104e-01 (5.7447e-01)\tAcc@1  82.03 ( 82.83)\tAcc@5  96.88 ( 97.28)\n","Epoch: [66][ 90/391]\tTime  0.098 ( 0.094)\tLoss 4.8384e-01 (5.7330e-01)\tAcc@1  85.94 ( 82.90)\tAcc@5  97.66 ( 97.30)\n","Epoch: [66][120/391]\tTime  0.092 ( 0.094)\tLoss 5.4724e-01 (5.7715e-01)\tAcc@1  85.16 ( 82.83)\tAcc@5  95.31 ( 97.26)\n","Epoch: [66][150/391]\tTime  0.089 ( 0.093)\tLoss 7.0383e-01 (5.7984e-01)\tAcc@1  78.91 ( 82.68)\tAcc@5  95.31 ( 97.17)\n","Epoch: [66][180/391]\tTime  0.095 ( 0.093)\tLoss 6.5378e-01 (5.7809e-01)\tAcc@1  80.47 ( 82.66)\tAcc@5  96.09 ( 97.26)\n","Epoch: [66][210/391]\tTime  0.086 ( 0.093)\tLoss 5.9020e-01 (5.8262e-01)\tAcc@1  81.25 ( 82.47)\tAcc@5  97.66 ( 97.26)\n","Epoch: [66][240/391]\tTime  0.092 ( 0.093)\tLoss 7.1565e-01 (5.8351e-01)\tAcc@1  76.56 ( 82.33)\tAcc@5  96.09 ( 97.27)\n","Epoch: [66][270/391]\tTime  0.092 ( 0.093)\tLoss 7.0940e-01 (5.8771e-01)\tAcc@1  73.44 ( 82.18)\tAcc@5  95.31 ( 97.22)\n","Epoch: [66][300/391]\tTime  0.092 ( 0.093)\tLoss 6.6635e-01 (5.9113e-01)\tAcc@1  79.69 ( 82.05)\tAcc@5  96.88 ( 97.16)\n","Epoch: [66][330/391]\tTime  0.091 ( 0.093)\tLoss 5.3686e-01 (5.9209e-01)\tAcc@1  82.81 ( 82.04)\tAcc@5  97.66 ( 97.14)\n","Epoch: [66][360/391]\tTime  0.090 ( 0.093)\tLoss 6.6948e-01 (5.9571e-01)\tAcc@1  76.56 ( 81.92)\tAcc@5  96.88 ( 97.14)\n","Epoch: [66][390/391]\tTime  0.081 ( 0.093)\tLoss 8.0155e-01 (5.9838e-01)\tAcc@1  81.25 ( 81.85)\tAcc@5  92.50 ( 97.13)\n","==> Train Accuracy: Acc@1 81.854 || Acc@5 97.132\n","==> Test Accuracy:  Acc@1 72.830 || Acc@5 93.620\n","==> 38.92 seconds to train this epoch\n","\n","\n","----- epoch: 67, lr: 0.020000000000000004 -----\n","Epoch: [67][  0/391]\tTime  0.276 ( 0.276)\tLoss 6.0558e-01 (6.0558e-01)\tAcc@1  83.59 ( 83.59)\tAcc@5  97.66 ( 97.66)\n","Epoch: [67][ 30/391]\tTime  0.093 ( 0.100)\tLoss 6.5007e-01 (5.3802e-01)\tAcc@1  77.34 ( 83.39)\tAcc@5  98.44 ( 97.66)\n","Epoch: [67][ 60/391]\tTime  0.090 ( 0.096)\tLoss 5.0268e-01 (5.3409e-01)\tAcc@1  85.16 ( 83.56)\tAcc@5  97.66 ( 97.78)\n","Epoch: [67][ 90/391]\tTime  0.091 ( 0.095)\tLoss 5.9047e-01 (5.5004e-01)\tAcc@1  85.16 ( 83.32)\tAcc@5  96.09 ( 97.51)\n","Epoch: [67][120/391]\tTime  0.093 ( 0.094)\tLoss 5.6466e-01 (5.5358e-01)\tAcc@1  83.59 ( 83.23)\tAcc@5  96.88 ( 97.43)\n","Epoch: [67][150/391]\tTime  0.090 ( 0.094)\tLoss 6.7517e-01 (5.6028e-01)\tAcc@1  77.34 ( 82.96)\tAcc@5  96.88 ( 97.43)\n","Epoch: [67][180/391]\tTime  0.095 ( 0.094)\tLoss 6.3728e-01 (5.6546e-01)\tAcc@1  77.34 ( 82.82)\tAcc@5  96.09 ( 97.46)\n","Epoch: [67][210/391]\tTime  0.091 ( 0.093)\tLoss 5.7746e-01 (5.7331e-01)\tAcc@1  83.59 ( 82.49)\tAcc@5  97.66 ( 97.41)\n","Epoch: [67][240/391]\tTime  0.091 ( 0.093)\tLoss 4.8028e-01 (5.7858e-01)\tAcc@1  86.72 ( 82.36)\tAcc@5  99.22 ( 97.36)\n","Epoch: [67][270/391]\tTime  0.091 ( 0.093)\tLoss 5.7514e-01 (5.8415e-01)\tAcc@1  83.59 ( 82.16)\tAcc@5  98.44 ( 97.31)\n","Epoch: [67][300/391]\tTime  0.092 ( 0.093)\tLoss 6.1720e-01 (5.8844e-01)\tAcc@1  79.69 ( 81.97)\tAcc@5  97.66 ( 97.29)\n","Epoch: [67][330/391]\tTime  0.090 ( 0.093)\tLoss 4.6579e-01 (5.8937e-01)\tAcc@1  87.50 ( 81.93)\tAcc@5  98.44 ( 97.30)\n","Epoch: [67][360/391]\tTime  0.090 ( 0.093)\tLoss 6.3283e-01 (5.9280e-01)\tAcc@1  79.69 ( 81.82)\tAcc@5  98.44 ( 97.27)\n","Epoch: [67][390/391]\tTime  0.082 ( 0.093)\tLoss 6.0873e-01 (5.9452e-01)\tAcc@1  81.25 ( 81.78)\tAcc@5  96.25 ( 97.27)\n","==> Train Accuracy: Acc@1 81.778 || Acc@5 97.270\n","==> Test Accuracy:  Acc@1 72.310 || Acc@5 93.400\n","==> 39.09 seconds to train this epoch\n","\n","\n","----- epoch: 68, lr: 0.020000000000000004 -----\n","Epoch: [68][  0/391]\tTime  0.271 ( 0.271)\tLoss 4.7799e-01 (4.7799e-01)\tAcc@1  86.72 ( 86.72)\tAcc@5  97.66 ( 97.66)\n","Epoch: [68][ 30/391]\tTime  0.093 ( 0.099)\tLoss 5.2434e-01 (5.6552e-01)\tAcc@1  79.69 ( 82.69)\tAcc@5 100.00 ( 97.81)\n","Epoch: [68][ 60/391]\tTime  0.092 ( 0.095)\tLoss 5.1567e-01 (5.7487e-01)\tAcc@1  86.72 ( 82.49)\tAcc@5  96.88 ( 97.59)\n","Epoch: [68][ 90/391]\tTime  0.091 ( 0.094)\tLoss 6.4384e-01 (5.7103e-01)\tAcc@1  77.34 ( 82.47)\tAcc@5  96.88 ( 97.65)\n","Epoch: [68][120/391]\tTime  0.089 ( 0.094)\tLoss 6.2048e-01 (5.6673e-01)\tAcc@1  77.34 ( 82.65)\tAcc@5  98.44 ( 97.63)\n","Epoch: [68][150/391]\tTime  0.094 ( 0.093)\tLoss 6.5924e-01 (5.7247e-01)\tAcc@1  80.47 ( 82.57)\tAcc@5  94.53 ( 97.49)\n","Epoch: [68][180/391]\tTime  0.096 ( 0.093)\tLoss 6.1817e-01 (5.7293e-01)\tAcc@1  80.47 ( 82.53)\tAcc@5  99.22 ( 97.52)\n","Epoch: [68][210/391]\tTime  0.093 ( 0.093)\tLoss 6.2297e-01 (5.7910e-01)\tAcc@1  79.69 ( 82.31)\tAcc@5  96.88 ( 97.47)\n","Epoch: [68][240/391]\tTime  0.091 ( 0.093)\tLoss 5.8945e-01 (5.7956e-01)\tAcc@1  83.59 ( 82.37)\tAcc@5  98.44 ( 97.46)\n","Epoch: [68][270/391]\tTime  0.091 ( 0.093)\tLoss 6.1320e-01 (5.8153e-01)\tAcc@1  82.81 ( 82.25)\tAcc@5  98.44 ( 97.45)\n","Epoch: [68][300/391]\tTime  0.095 ( 0.093)\tLoss 5.8682e-01 (5.8709e-01)\tAcc@1  83.59 ( 82.15)\tAcc@5  97.66 ( 97.42)\n","Epoch: [68][330/391]\tTime  0.092 ( 0.093)\tLoss 4.0508e-01 (5.8928e-01)\tAcc@1  84.38 ( 82.10)\tAcc@5  99.22 ( 97.40)\n","Epoch: [68][360/391]\tTime  0.088 ( 0.093)\tLoss 7.6937e-01 (5.9314e-01)\tAcc@1  74.22 ( 81.96)\tAcc@5  95.31 ( 97.33)\n","Epoch: [68][390/391]\tTime  0.082 ( 0.093)\tLoss 8.7019e-01 (5.9798e-01)\tAcc@1  71.25 ( 81.81)\tAcc@5  95.00 ( 97.31)\n","==> Train Accuracy: Acc@1 81.814 || Acc@5 97.308\n","==> Test Accuracy:  Acc@1 72.190 || Acc@5 92.870\n","==> 39.01 seconds to train this epoch\n","\n","\n","----- epoch: 69, lr: 0.020000000000000004 -----\n","Epoch: [69][  0/391]\tTime  0.265 ( 0.265)\tLoss 7.3841e-01 (7.3841e-01)\tAcc@1  82.03 ( 82.03)\tAcc@5  95.31 ( 95.31)\n","Epoch: [69][ 30/391]\tTime  0.107 ( 0.100)\tLoss 4.5102e-01 (5.6465e-01)\tAcc@1  87.50 ( 82.74)\tAcc@5  96.88 ( 97.35)\n","Epoch: [69][ 60/391]\tTime  0.093 ( 0.096)\tLoss 7.0149e-01 (5.5920e-01)\tAcc@1  81.25 ( 83.04)\tAcc@5  97.66 ( 97.44)\n","Epoch: [69][ 90/391]\tTime  0.095 ( 0.095)\tLoss 6.9057e-01 (5.6259e-01)\tAcc@1  80.47 ( 82.98)\tAcc@5  96.09 ( 97.34)\n","Epoch: [69][120/391]\tTime  0.097 ( 0.094)\tLoss 5.9676e-01 (5.5866e-01)\tAcc@1  82.03 ( 83.03)\tAcc@5  97.66 ( 97.46)\n","Epoch: [69][150/391]\tTime  0.093 ( 0.094)\tLoss 4.5010e-01 (5.5619e-01)\tAcc@1  89.06 ( 83.07)\tAcc@5  98.44 ( 97.49)\n","Epoch: [69][180/391]\tTime  0.091 ( 0.094)\tLoss 5.6809e-01 (5.6377e-01)\tAcc@1  81.25 ( 82.98)\tAcc@5  98.44 ( 97.43)\n","Epoch: [69][210/391]\tTime  0.091 ( 0.093)\tLoss 3.9981e-01 (5.6630e-01)\tAcc@1  88.28 ( 82.93)\tAcc@5  99.22 ( 97.42)\n","Epoch: [69][240/391]\tTime  0.093 ( 0.093)\tLoss 4.6666e-01 (5.7351e-01)\tAcc@1  85.94 ( 82.73)\tAcc@5 100.00 ( 97.36)\n","Epoch: [69][270/391]\tTime  0.089 ( 0.093)\tLoss 5.2434e-01 (5.7629e-01)\tAcc@1  83.59 ( 82.62)\tAcc@5  96.88 ( 97.33)\n","Epoch: [69][300/391]\tTime  0.114 ( 0.093)\tLoss 3.7702e-01 (5.7919e-01)\tAcc@1  87.50 ( 82.49)\tAcc@5 100.00 ( 97.32)\n","Epoch: [69][330/391]\tTime  0.091 ( 0.093)\tLoss 4.0225e-01 (5.8357e-01)\tAcc@1  89.84 ( 82.36)\tAcc@5  96.88 ( 97.29)\n","Epoch: [69][360/391]\tTime  0.093 ( 0.093)\tLoss 6.0326e-01 (5.8756e-01)\tAcc@1  82.81 ( 82.20)\tAcc@5  97.66 ( 97.25)\n","Epoch: [69][390/391]\tTime  0.082 ( 0.093)\tLoss 5.9491e-01 (5.8867e-01)\tAcc@1  82.50 ( 82.17)\tAcc@5  97.50 ( 97.26)\n","==> Train Accuracy: Acc@1 82.166 || Acc@5 97.264\n","==> Test Accuracy:  Acc@1 71.960 || Acc@5 92.810\n","==> 39.00 seconds to train this epoch\n","\n","\n","----- epoch: 70, lr: 0.020000000000000004 -----\n","Epoch: [70][  0/391]\tTime  0.273 ( 0.273)\tLoss 4.4409e-01 (4.4409e-01)\tAcc@1  86.72 ( 86.72)\tAcc@5  99.22 ( 99.22)\n","Epoch: [70][ 30/391]\tTime  0.091 ( 0.099)\tLoss 5.1346e-01 (5.4621e-01)\tAcc@1  82.81 ( 83.52)\tAcc@5  99.22 ( 97.88)\n","Epoch: [70][ 60/391]\tTime  0.097 ( 0.096)\tLoss 5.5803e-01 (5.3724e-01)\tAcc@1  85.16 ( 83.86)\tAcc@5  99.22 ( 97.78)\n","Epoch: [70][ 90/391]\tTime  0.092 ( 0.095)\tLoss 4.0904e-01 (5.3807e-01)\tAcc@1  87.50 ( 83.71)\tAcc@5  97.66 ( 97.78)\n","Epoch: [70][120/391]\tTime  0.091 ( 0.094)\tLoss 5.2743e-01 (5.4771e-01)\tAcc@1  84.38 ( 83.54)\tAcc@5  98.44 ( 97.67)\n","Epoch: [70][150/391]\tTime  0.096 ( 0.094)\tLoss 4.8513e-01 (5.4911e-01)\tAcc@1  84.38 ( 83.41)\tAcc@5  97.66 ( 97.66)\n","Epoch: [70][180/391]\tTime  0.087 ( 0.094)\tLoss 6.1907e-01 (5.5204e-01)\tAcc@1  83.59 ( 83.34)\tAcc@5  95.31 ( 97.60)\n","Epoch: [70][210/391]\tTime  0.084 ( 0.093)\tLoss 6.1132e-01 (5.5540e-01)\tAcc@1  84.38 ( 83.20)\tAcc@5  98.44 ( 97.59)\n","Epoch: [70][240/391]\tTime  0.093 ( 0.093)\tLoss 9.0443e-01 (5.6542e-01)\tAcc@1  72.66 ( 82.90)\tAcc@5  93.75 ( 97.53)\n","Epoch: [70][270/391]\tTime  0.096 ( 0.093)\tLoss 5.1276e-01 (5.7056e-01)\tAcc@1  83.59 ( 82.72)\tAcc@5  98.44 ( 97.48)\n","Epoch: [70][300/391]\tTime  0.094 ( 0.093)\tLoss 5.6445e-01 (5.7680e-01)\tAcc@1  83.59 ( 82.54)\tAcc@5  98.44 ( 97.40)\n","Epoch: [70][330/391]\tTime  0.095 ( 0.093)\tLoss 5.6691e-01 (5.8072e-01)\tAcc@1  82.03 ( 82.39)\tAcc@5  96.09 ( 97.35)\n","Epoch: [70][360/391]\tTime  0.090 ( 0.093)\tLoss 6.4611e-01 (5.8387e-01)\tAcc@1  78.12 ( 82.26)\tAcc@5  97.66 ( 97.35)\n","Epoch: [70][390/391]\tTime  0.080 ( 0.093)\tLoss 5.5304e-01 (5.8718e-01)\tAcc@1  87.50 ( 82.21)\tAcc@5  98.75 ( 97.30)\n","==> Train Accuracy: Acc@1 82.206 || Acc@5 97.296\n","==> Test Accuracy:  Acc@1 71.440 || Acc@5 92.580\n","==> 39.03 seconds to train this epoch\n","\n","\n","----- epoch: 71, lr: 0.020000000000000004 -----\n","Epoch: [71][  0/391]\tTime  0.262 ( 0.262)\tLoss 5.4228e-01 (5.4228e-01)\tAcc@1  82.03 ( 82.03)\tAcc@5  98.44 ( 98.44)\n","Epoch: [71][ 30/391]\tTime  0.091 ( 0.100)\tLoss 4.5067e-01 (5.5126e-01)\tAcc@1  88.28 ( 82.99)\tAcc@5  96.09 ( 97.56)\n","Epoch: [71][ 60/391]\tTime  0.091 ( 0.096)\tLoss 6.1721e-01 (5.3854e-01)\tAcc@1  83.59 ( 83.81)\tAcc@5  96.09 ( 97.71)\n","Epoch: [71][ 90/391]\tTime  0.086 ( 0.095)\tLoss 4.7671e-01 (5.4146e-01)\tAcc@1  86.72 ( 83.65)\tAcc@5  99.22 ( 97.77)\n","Epoch: [71][120/391]\tTime  0.093 ( 0.094)\tLoss 6.6657e-01 (5.4121e-01)\tAcc@1  77.34 ( 83.53)\tAcc@5  98.44 ( 97.91)\n","Epoch: [71][150/391]\tTime  0.092 ( 0.094)\tLoss 5.7916e-01 (5.4473e-01)\tAcc@1  84.38 ( 83.44)\tAcc@5  98.44 ( 97.84)\n","Epoch: [71][180/391]\tTime  0.090 ( 0.093)\tLoss 5.7962e-01 (5.5140e-01)\tAcc@1  82.03 ( 83.24)\tAcc@5  99.22 ( 97.74)\n","Epoch: [71][210/391]\tTime  0.092 ( 0.093)\tLoss 5.1469e-01 (5.5386e-01)\tAcc@1  83.59 ( 83.12)\tAcc@5  97.66 ( 97.69)\n","Epoch: [71][240/391]\tTime  0.086 ( 0.093)\tLoss 6.3549e-01 (5.5975e-01)\tAcc@1  83.59 ( 82.93)\tAcc@5  96.09 ( 97.63)\n","Epoch: [71][270/391]\tTime  0.093 ( 0.093)\tLoss 5.7089e-01 (5.6412e-01)\tAcc@1  82.81 ( 82.79)\tAcc@5  96.88 ( 97.55)\n","Epoch: [71][300/391]\tTime  0.095 ( 0.093)\tLoss 6.4584e-01 (5.6569e-01)\tAcc@1  78.91 ( 82.73)\tAcc@5  98.44 ( 97.55)\n","Epoch: [71][330/391]\tTime  0.093 ( 0.093)\tLoss 6.1726e-01 (5.6766e-01)\tAcc@1  83.59 ( 82.73)\tAcc@5  96.88 ( 97.52)\n","Epoch: [71][360/391]\tTime  0.104 ( 0.093)\tLoss 6.4976e-01 (5.7418e-01)\tAcc@1  82.03 ( 82.54)\tAcc@5  94.53 ( 97.48)\n","Epoch: [71][390/391]\tTime  0.082 ( 0.093)\tLoss 7.3039e-01 (5.7765e-01)\tAcc@1  80.00 ( 82.38)\tAcc@5  96.25 ( 97.48)\n","==> Train Accuracy: Acc@1 82.384 || Acc@5 97.478\n","==> Test Accuracy:  Acc@1 71.680 || Acc@5 92.580\n","==> 38.93 seconds to train this epoch\n","\n","\n","----- epoch: 72, lr: 0.020000000000000004 -----\n","Epoch: [72][  0/391]\tTime  0.253 ( 0.253)\tLoss 6.3222e-01 (6.3222e-01)\tAcc@1  83.59 ( 83.59)\tAcc@5  95.31 ( 95.31)\n","Epoch: [72][ 30/391]\tTime  0.091 ( 0.099)\tLoss 5.7331e-01 (5.5221e-01)\tAcc@1  84.38 ( 83.24)\tAcc@5  96.09 ( 97.76)\n","Epoch: [72][ 60/391]\tTime  0.092 ( 0.096)\tLoss 4.5319e-01 (5.5173e-01)\tAcc@1  85.94 ( 83.15)\tAcc@5  97.66 ( 97.85)\n","Epoch: [72][ 90/391]\tTime  0.092 ( 0.094)\tLoss 6.2889e-01 (5.4936e-01)\tAcc@1  81.25 ( 83.45)\tAcc@5  97.66 ( 97.78)\n","Epoch: [72][120/391]\tTime  0.091 ( 0.094)\tLoss 5.5146e-01 (5.4398e-01)\tAcc@1  82.81 ( 83.61)\tAcc@5  97.66 ( 97.75)\n","Epoch: [72][150/391]\tTime  0.091 ( 0.093)\tLoss 7.2728e-01 (5.4353e-01)\tAcc@1  76.56 ( 83.50)\tAcc@5  96.88 ( 97.78)\n","Epoch: [72][180/391]\tTime  0.094 ( 0.093)\tLoss 6.1335e-01 (5.4439e-01)\tAcc@1  81.25 ( 83.48)\tAcc@5  98.44 ( 97.88)\n","Epoch: [72][210/391]\tTime  0.095 ( 0.093)\tLoss 6.0156e-01 (5.4980e-01)\tAcc@1  82.03 ( 83.36)\tAcc@5  98.44 ( 97.82)\n","Epoch: [72][240/391]\tTime  0.094 ( 0.093)\tLoss 6.6018e-01 (5.5983e-01)\tAcc@1  78.91 ( 83.01)\tAcc@5  96.88 ( 97.66)\n","Epoch: [72][270/391]\tTime  0.091 ( 0.093)\tLoss 5.7641e-01 (5.6334e-01)\tAcc@1  82.81 ( 82.88)\tAcc@5  97.66 ( 97.64)\n","Epoch: [72][300/391]\tTime  0.090 ( 0.093)\tLoss 5.9136e-01 (5.6888e-01)\tAcc@1  81.25 ( 82.71)\tAcc@5  98.44 ( 97.61)\n","Epoch: [72][330/391]\tTime  0.091 ( 0.093)\tLoss 4.9378e-01 (5.7236e-01)\tAcc@1  84.38 ( 82.58)\tAcc@5  96.88 ( 97.57)\n","Epoch: [72][360/391]\tTime  0.092 ( 0.093)\tLoss 6.1105e-01 (5.7730e-01)\tAcc@1  84.38 ( 82.41)\tAcc@5  95.31 ( 97.55)\n","Epoch: [72][390/391]\tTime  0.082 ( 0.093)\tLoss 5.6717e-01 (5.8240e-01)\tAcc@1  85.00 ( 82.26)\tAcc@5  95.00 ( 97.48)\n","==> Train Accuracy: Acc@1 82.264 || Acc@5 97.480\n","==> Test Accuracy:  Acc@1 71.990 || Acc@5 92.800\n","==> 38.90 seconds to train this epoch\n","\n","\n","----- epoch: 73, lr: 0.020000000000000004 -----\n","Epoch: [73][  0/391]\tTime  0.274 ( 0.274)\tLoss 5.1500e-01 (5.1500e-01)\tAcc@1  85.16 ( 85.16)\tAcc@5  96.88 ( 96.88)\n","Epoch: [73][ 30/391]\tTime  0.090 ( 0.099)\tLoss 5.8131e-01 (5.2356e-01)\tAcc@1  82.81 ( 84.05)\tAcc@5  97.66 ( 97.58)\n","Epoch: [73][ 60/391]\tTime  0.091 ( 0.096)\tLoss 6.3610e-01 (5.1701e-01)\tAcc@1  78.91 ( 84.02)\tAcc@5  98.44 ( 97.87)\n","Epoch: [73][ 90/391]\tTime  0.091 ( 0.095)\tLoss 4.6207e-01 (5.2319e-01)\tAcc@1  85.16 ( 83.78)\tAcc@5  98.44 ( 97.97)\n","Epoch: [73][120/391]\tTime  0.093 ( 0.094)\tLoss 5.6170e-01 (5.2863e-01)\tAcc@1  82.81 ( 83.58)\tAcc@5  98.44 ( 97.95)\n","Epoch: [73][150/391]\tTime  0.094 ( 0.094)\tLoss 4.9599e-01 (5.3572e-01)\tAcc@1  82.81 ( 83.41)\tAcc@5  96.88 ( 97.92)\n","Epoch: [73][180/391]\tTime  0.087 ( 0.093)\tLoss 4.9471e-01 (5.4003e-01)\tAcc@1  82.81 ( 83.25)\tAcc@5  97.66 ( 97.90)\n","Epoch: [73][210/391]\tTime  0.093 ( 0.093)\tLoss 6.4814e-01 (5.4441e-01)\tAcc@1  78.91 ( 83.10)\tAcc@5  98.44 ( 97.83)\n","Epoch: [73][240/391]\tTime  0.090 ( 0.093)\tLoss 6.2186e-01 (5.5355e-01)\tAcc@1  79.69 ( 82.87)\tAcc@5  98.44 ( 97.73)\n","Epoch: [73][270/391]\tTime  0.096 ( 0.093)\tLoss 3.9535e-01 (5.5761e-01)\tAcc@1  91.41 ( 82.86)\tAcc@5  97.66 ( 97.72)\n","Epoch: [73][300/391]\tTime  0.096 ( 0.093)\tLoss 7.6372e-01 (5.6110e-01)\tAcc@1  73.44 ( 82.80)\tAcc@5  97.66 ( 97.63)\n","Epoch: [73][330/391]\tTime  0.091 ( 0.093)\tLoss 7.3272e-01 (5.6617e-01)\tAcc@1  77.34 ( 82.63)\tAcc@5  95.31 ( 97.61)\n","Epoch: [73][360/391]\tTime  0.093 ( 0.093)\tLoss 7.2213e-01 (5.7007e-01)\tAcc@1  79.69 ( 82.51)\tAcc@5  96.88 ( 97.58)\n","Epoch: [73][390/391]\tTime  0.083 ( 0.093)\tLoss 4.4743e-01 (5.7245e-01)\tAcc@1  83.75 ( 82.39)\tAcc@5  98.75 ( 97.57)\n","==> Train Accuracy: Acc@1 82.394 || Acc@5 97.570\n","==> Test Accuracy:  Acc@1 71.620 || Acc@5 92.470\n","==> 39.05 seconds to train this epoch\n","\n","\n","----- epoch: 74, lr: 0.020000000000000004 -----\n","Epoch: [74][  0/391]\tTime  0.264 ( 0.264)\tLoss 4.8314e-01 (4.8314e-01)\tAcc@1  87.50 ( 87.50)\tAcc@5  99.22 ( 99.22)\n","Epoch: [74][ 30/391]\tTime  0.093 ( 0.098)\tLoss 5.5317e-01 (5.5395e-01)\tAcc@1  82.03 ( 83.52)\tAcc@5  99.22 ( 97.76)\n","Epoch: [74][ 60/391]\tTime  0.090 ( 0.095)\tLoss 5.0355e-01 (5.3420e-01)\tAcc@1  83.59 ( 83.95)\tAcc@5  96.88 ( 97.81)\n","Epoch: [74][ 90/391]\tTime  0.093 ( 0.094)\tLoss 4.7345e-01 (5.3628e-01)\tAcc@1  81.25 ( 83.67)\tAcc@5 100.00 ( 97.79)\n","Epoch: [74][120/391]\tTime  0.090 ( 0.094)\tLoss 5.5489e-01 (5.3816e-01)\tAcc@1  81.25 ( 83.67)\tAcc@5  99.22 ( 97.84)\n","Epoch: [74][150/391]\tTime  0.088 ( 0.093)\tLoss 5.6800e-01 (5.4070e-01)\tAcc@1  83.59 ( 83.61)\tAcc@5  98.44 ( 97.89)\n","Epoch: [74][180/391]\tTime  0.092 ( 0.093)\tLoss 5.5798e-01 (5.4378e-01)\tAcc@1  82.81 ( 83.55)\tAcc@5  99.22 ( 97.85)\n","Epoch: [74][210/391]\tTime  0.094 ( 0.093)\tLoss 5.0597e-01 (5.4556e-01)\tAcc@1  85.94 ( 83.46)\tAcc@5  98.44 ( 97.82)\n","Epoch: [74][240/391]\tTime  0.116 ( 0.093)\tLoss 5.3681e-01 (5.5238e-01)\tAcc@1  81.25 ( 83.21)\tAcc@5  97.66 ( 97.80)\n","Epoch: [74][270/391]\tTime  0.092 ( 0.093)\tLoss 5.3087e-01 (5.5493e-01)\tAcc@1  85.94 ( 83.10)\tAcc@5  96.88 ( 97.74)\n","Epoch: [74][300/391]\tTime  0.092 ( 0.093)\tLoss 5.9933e-01 (5.5845e-01)\tAcc@1  82.03 ( 83.05)\tAcc@5  96.09 ( 97.67)\n","Epoch: [74][330/391]\tTime  0.092 ( 0.093)\tLoss 5.8789e-01 (5.6068e-01)\tAcc@1  85.16 ( 82.95)\tAcc@5  97.66 ( 97.66)\n","Epoch: [74][360/391]\tTime  0.089 ( 0.093)\tLoss 7.6918e-01 (5.6540e-01)\tAcc@1  75.00 ( 82.82)\tAcc@5  96.09 ( 97.59)\n","Epoch: [74][390/391]\tTime  0.082 ( 0.093)\tLoss 6.7156e-01 (5.7098e-01)\tAcc@1  82.50 ( 82.65)\tAcc@5  95.00 ( 97.57)\n","==> Train Accuracy: Acc@1 82.654 || Acc@5 97.572\n","==> Test Accuracy:  Acc@1 70.900 || Acc@5 92.270\n","==> 38.93 seconds to train this epoch\n","\n","\n","----- epoch: 75, lr: 0.020000000000000004 -----\n","Epoch: [75][  0/391]\tTime  0.250 ( 0.250)\tLoss 4.8436e-01 (4.8436e-01)\tAcc@1  88.28 ( 88.28)\tAcc@5  98.44 ( 98.44)\n","Epoch: [75][ 30/391]\tTime  0.093 ( 0.099)\tLoss 3.6329e-01 (5.1802e-01)\tAcc@1  91.41 ( 84.98)\tAcc@5  97.66 ( 97.48)\n","Epoch: [75][ 60/391]\tTime  0.096 ( 0.096)\tLoss 5.8670e-01 (5.2376e-01)\tAcc@1  83.59 ( 84.57)\tAcc@5  96.09 ( 97.64)\n","Epoch: [75][ 90/391]\tTime  0.091 ( 0.095)\tLoss 5.7825e-01 (5.2647e-01)\tAcc@1  83.59 ( 84.35)\tAcc@5  97.66 ( 97.66)\n","Epoch: [75][120/391]\tTime  0.094 ( 0.094)\tLoss 4.4592e-01 (5.2852e-01)\tAcc@1  86.72 ( 84.32)\tAcc@5  99.22 ( 97.67)\n","Epoch: [75][150/391]\tTime  0.092 ( 0.094)\tLoss 5.3153e-01 (5.3297e-01)\tAcc@1  81.25 ( 84.07)\tAcc@5  97.66 ( 97.67)\n","Epoch: [75][180/391]\tTime  0.092 ( 0.094)\tLoss 6.1622e-01 (5.3737e-01)\tAcc@1  81.25 ( 83.92)\tAcc@5  96.88 ( 97.68)\n","Epoch: [75][210/391]\tTime  0.090 ( 0.093)\tLoss 4.5147e-01 (5.3980e-01)\tAcc@1  85.94 ( 83.91)\tAcc@5  99.22 ( 97.67)\n","Epoch: [75][240/391]\tTime  0.091 ( 0.093)\tLoss 5.9888e-01 (5.4803e-01)\tAcc@1  85.16 ( 83.65)\tAcc@5  97.66 ( 97.61)\n","Epoch: [75][270/391]\tTime  0.091 ( 0.093)\tLoss 4.8899e-01 (5.5199e-01)\tAcc@1  82.03 ( 83.42)\tAcc@5  98.44 ( 97.63)\n","Epoch: [75][300/391]\tTime  0.093 ( 0.093)\tLoss 5.1732e-01 (5.5483e-01)\tAcc@1  84.38 ( 83.28)\tAcc@5  97.66 ( 97.64)\n","Epoch: [75][330/391]\tTime  0.092 ( 0.093)\tLoss 5.4597e-01 (5.6120e-01)\tAcc@1  84.38 ( 83.08)\tAcc@5  97.66 ( 97.58)\n","Epoch: [75][360/391]\tTime  0.097 ( 0.093)\tLoss 6.8039e-01 (5.6383e-01)\tAcc@1  77.34 ( 83.00)\tAcc@5  97.66 ( 97.58)\n","Epoch: [75][390/391]\tTime  0.082 ( 0.093)\tLoss 7.0113e-01 (5.6785e-01)\tAcc@1  80.00 ( 82.89)\tAcc@5  98.75 ( 97.54)\n","==> Train Accuracy: Acc@1 82.886 || Acc@5 97.540\n","==> Test Accuracy:  Acc@1 69.700 || Acc@5 91.800\n","==> 39.02 seconds to train this epoch\n","\n","\n","----- epoch: 76, lr: 0.020000000000000004 -----\n","Epoch: [76][  0/391]\tTime  0.252 ( 0.252)\tLoss 7.1129e-01 (7.1129e-01)\tAcc@1  79.69 ( 79.69)\tAcc@5  95.31 ( 95.31)\n","Epoch: [76][ 30/391]\tTime  0.083 ( 0.100)\tLoss 4.9027e-01 (5.1219e-01)\tAcc@1  87.50 ( 84.68)\tAcc@5  97.66 ( 97.88)\n","Epoch: [76][ 60/391]\tTime  0.092 ( 0.096)\tLoss 4.4022e-01 (5.2660e-01)\tAcc@1  87.50 ( 84.18)\tAcc@5  99.22 ( 97.75)\n","Epoch: [76][ 90/391]\tTime  0.095 ( 0.095)\tLoss 4.0018e-01 (5.2836e-01)\tAcc@1  88.28 ( 84.15)\tAcc@5 100.00 ( 97.75)\n","Epoch: [76][120/391]\tTime  0.090 ( 0.094)\tLoss 5.4682e-01 (5.3298e-01)\tAcc@1  82.81 ( 83.99)\tAcc@5  96.88 ( 97.75)\n","Epoch: [76][150/391]\tTime  0.090 ( 0.094)\tLoss 5.1577e-01 (5.3838e-01)\tAcc@1  83.59 ( 83.83)\tAcc@5  98.44 ( 97.70)\n","Epoch: [76][180/391]\tTime  0.087 ( 0.093)\tLoss 5.1358e-01 (5.4057e-01)\tAcc@1  84.38 ( 83.73)\tAcc@5  96.09 ( 97.71)\n","Epoch: [76][210/391]\tTime  0.091 ( 0.093)\tLoss 5.0526e-01 (5.5157e-01)\tAcc@1  85.16 ( 83.44)\tAcc@5  96.88 ( 97.59)\n","Epoch: [76][240/391]\tTime  0.089 ( 0.093)\tLoss 4.7039e-01 (5.5430e-01)\tAcc@1  89.06 ( 83.31)\tAcc@5 100.00 ( 97.61)\n","Epoch: [76][270/391]\tTime  0.090 ( 0.093)\tLoss 5.2748e-01 (5.5676e-01)\tAcc@1  85.16 ( 83.13)\tAcc@5  96.09 ( 97.65)\n","Epoch: [76][300/391]\tTime  0.091 ( 0.093)\tLoss 4.8492e-01 (5.5768e-01)\tAcc@1  81.25 ( 83.05)\tAcc@5  98.44 ( 97.65)\n","Epoch: [76][330/391]\tTime  0.091 ( 0.093)\tLoss 5.2567e-01 (5.6231e-01)\tAcc@1  82.03 ( 82.93)\tAcc@5  98.44 ( 97.63)\n","Epoch: [76][360/391]\tTime  0.087 ( 0.093)\tLoss 6.9317e-01 (5.6420e-01)\tAcc@1  80.47 ( 82.84)\tAcc@5  94.53 ( 97.61)\n","Epoch: [76][390/391]\tTime  0.082 ( 0.093)\tLoss 6.1972e-01 (5.6517e-01)\tAcc@1  85.00 ( 82.85)\tAcc@5  92.50 ( 97.60)\n","==> Train Accuracy: Acc@1 82.854 || Acc@5 97.596\n","==> Test Accuracy:  Acc@1 70.180 || Acc@5 92.040\n","==> 39.00 seconds to train this epoch\n","\n","\n","----- epoch: 77, lr: 0.020000000000000004 -----\n","Epoch: [77][  0/391]\tTime  0.287 ( 0.287)\tLoss 5.1514e-01 (5.1514e-01)\tAcc@1  84.38 ( 84.38)\tAcc@5  98.44 ( 98.44)\n","Epoch: [77][ 30/391]\tTime  0.095 ( 0.100)\tLoss 5.4987e-01 (4.8319e-01)\tAcc@1  82.81 ( 85.61)\tAcc@5  99.22 ( 98.54)\n","Epoch: [77][ 60/391]\tTime  0.090 ( 0.096)\tLoss 4.0593e-01 (4.8678e-01)\tAcc@1  86.72 ( 85.54)\tAcc@5  99.22 ( 98.41)\n","Epoch: [77][ 90/391]\tTime  0.089 ( 0.095)\tLoss 6.2494e-01 (4.9354e-01)\tAcc@1  85.16 ( 85.35)\tAcc@5  96.09 ( 98.31)\n","Epoch: [77][120/391]\tTime  0.090 ( 0.094)\tLoss 4.8894e-01 (5.0346e-01)\tAcc@1  85.16 ( 84.93)\tAcc@5  99.22 ( 98.26)\n","Epoch: [77][150/391]\tTime  0.093 ( 0.094)\tLoss 5.4687e-01 (5.1057e-01)\tAcc@1  82.03 ( 84.66)\tAcc@5  98.44 ( 98.13)\n","Epoch: [77][180/391]\tTime  0.092 ( 0.094)\tLoss 6.4756e-01 (5.1865e-01)\tAcc@1  79.69 ( 84.31)\tAcc@5  98.44 ( 98.11)\n","Epoch: [77][210/391]\tTime  0.092 ( 0.094)\tLoss 4.6963e-01 (5.2952e-01)\tAcc@1  83.59 ( 83.96)\tAcc@5  99.22 ( 97.99)\n","Epoch: [77][240/391]\tTime  0.084 ( 0.094)\tLoss 5.4766e-01 (5.3809e-01)\tAcc@1  82.81 ( 83.66)\tAcc@5  96.88 ( 97.91)\n","Epoch: [77][270/391]\tTime  0.093 ( 0.093)\tLoss 5.5917e-01 (5.4388e-01)\tAcc@1  82.81 ( 83.49)\tAcc@5  98.44 ( 97.86)\n","Epoch: [77][300/391]\tTime  0.098 ( 0.093)\tLoss 5.0634e-01 (5.4749e-01)\tAcc@1  82.81 ( 83.37)\tAcc@5 100.00 ( 97.85)\n","Epoch: [77][330/391]\tTime  0.097 ( 0.093)\tLoss 5.9008e-01 (5.5319e-01)\tAcc@1  81.25 ( 83.14)\tAcc@5  98.44 ( 97.79)\n","Epoch: [77][360/391]\tTime  0.097 ( 0.093)\tLoss 6.2501e-01 (5.5674e-01)\tAcc@1  76.56 ( 83.02)\tAcc@5  99.22 ( 97.79)\n","Epoch: [77][390/391]\tTime  0.082 ( 0.093)\tLoss 5.3481e-01 (5.6090e-01)\tAcc@1  87.50 ( 82.87)\tAcc@5  96.25 ( 97.75)\n","==> Train Accuracy: Acc@1 82.874 || Acc@5 97.748\n","==> Test Accuracy:  Acc@1 69.920 || Acc@5 92.170\n","==> 39.12 seconds to train this epoch\n","\n","\n","----- epoch: 78, lr: 0.020000000000000004 -----\n","Epoch: [78][  0/391]\tTime  0.261 ( 0.261)\tLoss 4.0865e-01 (4.0865e-01)\tAcc@1  86.72 ( 86.72)\tAcc@5  98.44 ( 98.44)\n","Epoch: [78][ 30/391]\tTime  0.091 ( 0.098)\tLoss 4.5976e-01 (5.2486e-01)\tAcc@1  87.50 ( 84.48)\tAcc@5  96.88 ( 97.86)\n","Epoch: [78][ 60/391]\tTime  0.096 ( 0.095)\tLoss 5.1901e-01 (5.1992e-01)\tAcc@1  82.81 ( 84.14)\tAcc@5  97.66 ( 98.01)\n","Epoch: [78][ 90/391]\tTime  0.091 ( 0.094)\tLoss 4.5274e-01 (5.1334e-01)\tAcc@1  85.16 ( 84.28)\tAcc@5  99.22 ( 98.15)\n","Epoch: [78][120/391]\tTime  0.098 ( 0.094)\tLoss 5.2493e-01 (5.1317e-01)\tAcc@1  81.25 ( 84.16)\tAcc@5  98.44 ( 98.13)\n","Epoch: [78][150/391]\tTime  0.094 ( 0.094)\tLoss 3.9368e-01 (5.1800e-01)\tAcc@1  88.28 ( 83.94)\tAcc@5 100.00 ( 98.11)\n","Epoch: [78][180/391]\tTime  0.092 ( 0.094)\tLoss 4.0389e-01 (5.3593e-01)\tAcc@1  91.41 ( 83.43)\tAcc@5  99.22 ( 97.94)\n","Epoch: [78][210/391]\tTime  0.091 ( 0.093)\tLoss 5.5366e-01 (5.4499e-01)\tAcc@1  84.38 ( 83.17)\tAcc@5  97.66 ( 97.85)\n","Epoch: [78][240/391]\tTime  0.094 ( 0.093)\tLoss 5.3404e-01 (5.4802e-01)\tAcc@1  81.25 ( 83.06)\tAcc@5  98.44 ( 97.83)\n","Epoch: [78][270/391]\tTime  0.094 ( 0.093)\tLoss 6.4726e-01 (5.5188e-01)\tAcc@1  78.91 ( 82.94)\tAcc@5  97.66 ( 97.81)\n","Epoch: [78][300/391]\tTime  0.091 ( 0.093)\tLoss 6.2042e-01 (5.5607e-01)\tAcc@1  83.59 ( 82.90)\tAcc@5  94.53 ( 97.76)\n","Epoch: [78][330/391]\tTime  0.094 ( 0.093)\tLoss 6.0833e-01 (5.6116e-01)\tAcc@1  81.25 ( 82.76)\tAcc@5  97.66 ( 97.73)\n","Epoch: [78][360/391]\tTime  0.102 ( 0.093)\tLoss 6.2541e-01 (5.6559e-01)\tAcc@1  81.25 ( 82.64)\tAcc@5  98.44 ( 97.68)\n","Epoch: [78][390/391]\tTime  0.081 ( 0.093)\tLoss 5.2270e-01 (5.6623e-01)\tAcc@1  82.50 ( 82.64)\tAcc@5 100.00 ( 97.67)\n","==> Train Accuracy: Acc@1 82.636 || Acc@5 97.668\n","==> Test Accuracy:  Acc@1 70.900 || Acc@5 92.540\n","==> 39.06 seconds to train this epoch\n","\n","\n","----- epoch: 79, lr: 0.020000000000000004 -----\n","Epoch: [79][  0/391]\tTime  0.286 ( 0.286)\tLoss 4.8365e-01 (4.8365e-01)\tAcc@1  85.16 ( 85.16)\tAcc@5  96.88 ( 96.88)\n","Epoch: [79][ 30/391]\tTime  0.092 ( 0.100)\tLoss 4.4345e-01 (5.0808e-01)\tAcc@1  87.50 ( 84.85)\tAcc@5  97.66 ( 97.86)\n","Epoch: [79][ 60/391]\tTime  0.092 ( 0.096)\tLoss 5.0155e-01 (4.9986e-01)\tAcc@1  83.59 ( 85.12)\tAcc@5  98.44 ( 97.99)\n","Epoch: [79][ 90/391]\tTime  0.102 ( 0.095)\tLoss 5.6436e-01 (5.0291e-01)\tAcc@1  83.59 ( 85.00)\tAcc@5  98.44 ( 97.96)\n","Epoch: [79][120/391]\tTime  0.092 ( 0.094)\tLoss 4.3503e-01 (5.0549e-01)\tAcc@1  83.59 ( 84.95)\tAcc@5  99.22 ( 98.02)\n","Epoch: [79][150/391]\tTime  0.091 ( 0.094)\tLoss 4.8089e-01 (5.0730e-01)\tAcc@1  82.81 ( 84.91)\tAcc@5  97.66 ( 98.08)\n","Epoch: [79][180/391]\tTime  0.093 ( 0.093)\tLoss 3.1742e-01 (5.1140e-01)\tAcc@1  89.84 ( 84.79)\tAcc@5  99.22 ( 98.04)\n","Epoch: [79][210/391]\tTime  0.094 ( 0.093)\tLoss 5.4234e-01 (5.2063e-01)\tAcc@1  85.16 ( 84.46)\tAcc@5  97.66 ( 97.97)\n","Epoch: [79][240/391]\tTime  0.084 ( 0.093)\tLoss 4.9242e-01 (5.2528e-01)\tAcc@1  84.38 ( 84.31)\tAcc@5 100.00 ( 97.96)\n","Epoch: [79][270/391]\tTime  0.091 ( 0.093)\tLoss 5.6078e-01 (5.3138e-01)\tAcc@1  86.72 ( 84.14)\tAcc@5  98.44 ( 97.89)\n","Epoch: [79][300/391]\tTime  0.087 ( 0.093)\tLoss 7.2273e-01 (5.3928e-01)\tAcc@1  79.69 ( 83.92)\tAcc@5  96.88 ( 97.79)\n","Epoch: [79][330/391]\tTime  0.091 ( 0.093)\tLoss 5.1498e-01 (5.4523e-01)\tAcc@1  82.81 ( 83.68)\tAcc@5  98.44 ( 97.75)\n","Epoch: [79][360/391]\tTime  0.091 ( 0.093)\tLoss 7.2111e-01 (5.5135e-01)\tAcc@1  76.56 ( 83.44)\tAcc@5  93.75 ( 97.71)\n","Epoch: [79][390/391]\tTime  0.082 ( 0.093)\tLoss 5.6797e-01 (5.5528e-01)\tAcc@1  83.75 ( 83.31)\tAcc@5  96.25 ( 97.67)\n","==> Train Accuracy: Acc@1 83.306 || Acc@5 97.668\n","==> Test Accuracy:  Acc@1 72.070 || Acc@5 92.350\n","==> 39.04 seconds to train this epoch\n","\n","\n","----- epoch: 80, lr: 0.020000000000000004 -----\n","Epoch: [80][  0/391]\tTime  0.274 ( 0.274)\tLoss 3.7957e-01 (3.7957e-01)\tAcc@1  85.16 ( 85.16)\tAcc@5  98.44 ( 98.44)\n","Epoch: [80][ 30/391]\tTime  0.093 ( 0.100)\tLoss 4.5487e-01 (5.2062e-01)\tAcc@1  88.28 ( 83.95)\tAcc@5  97.66 ( 98.03)\n","Epoch: [80][ 60/391]\tTime  0.094 ( 0.097)\tLoss 6.6541e-01 (5.1713e-01)\tAcc@1  80.47 ( 84.35)\tAcc@5  97.66 ( 98.05)\n","Epoch: [80][ 90/391]\tTime  0.098 ( 0.096)\tLoss 6.3089e-01 (5.1757e-01)\tAcc@1  79.69 ( 84.13)\tAcc@5  99.22 ( 98.18)\n","Epoch: [80][120/391]\tTime  0.092 ( 0.095)\tLoss 6.5359e-01 (5.1375e-01)\tAcc@1  78.91 ( 84.26)\tAcc@5  97.66 ( 98.19)\n","Epoch: [80][150/391]\tTime  0.089 ( 0.094)\tLoss 5.7391e-01 (5.2059e-01)\tAcc@1  82.03 ( 84.10)\tAcc@5  98.44 ( 98.10)\n","Epoch: [80][180/391]\tTime  0.091 ( 0.094)\tLoss 6.4101e-01 (5.2614e-01)\tAcc@1  79.69 ( 83.93)\tAcc@5  98.44 ( 98.01)\n","Epoch: [80][210/391]\tTime  0.088 ( 0.094)\tLoss 5.4696e-01 (5.2942e-01)\tAcc@1  83.59 ( 83.81)\tAcc@5  99.22 ( 97.99)\n","Epoch: [80][240/391]\tTime  0.094 ( 0.094)\tLoss 4.6827e-01 (5.3042e-01)\tAcc@1  86.72 ( 83.81)\tAcc@5  97.66 ( 97.98)\n","Epoch: [80][270/391]\tTime  0.094 ( 0.094)\tLoss 4.5578e-01 (5.3613e-01)\tAcc@1  84.38 ( 83.60)\tAcc@5  99.22 ( 97.91)\n","Epoch: [80][300/391]\tTime  0.091 ( 0.093)\tLoss 5.1962e-01 (5.3565e-01)\tAcc@1  84.38 ( 83.52)\tAcc@5  97.66 ( 97.91)\n","Epoch: [80][330/391]\tTime  0.103 ( 0.093)\tLoss 4.7897e-01 (5.3912e-01)\tAcc@1  84.38 ( 83.43)\tAcc@5  98.44 ( 97.90)\n","Epoch: [80][360/391]\tTime  0.094 ( 0.093)\tLoss 6.7974e-01 (5.4416e-01)\tAcc@1  79.69 ( 83.29)\tAcc@5  96.88 ( 97.86)\n","Epoch: [80][390/391]\tTime  0.083 ( 0.093)\tLoss 6.6830e-01 (5.4899e-01)\tAcc@1  80.00 ( 83.13)\tAcc@5  96.25 ( 97.83)\n","==> Train Accuracy: Acc@1 83.128 || Acc@5 97.830\n","==> Test Accuracy:  Acc@1 70.400 || Acc@5 91.320\n","==> 39.14 seconds to train this epoch\n","\n","\n","----- epoch: 81, lr: 0.020000000000000004 -----\n","Epoch: [81][  0/391]\tTime  0.294 ( 0.294)\tLoss 6.8857e-01 (6.8857e-01)\tAcc@1  75.78 ( 75.78)\tAcc@5  96.09 ( 96.09)\n","Epoch: [81][ 30/391]\tTime  0.091 ( 0.100)\tLoss 5.4451e-01 (5.0765e-01)\tAcc@1  82.81 ( 84.07)\tAcc@5  96.88 ( 98.21)\n","Epoch: [81][ 60/391]\tTime  0.094 ( 0.096)\tLoss 4.0759e-01 (5.1152e-01)\tAcc@1  85.94 ( 84.17)\tAcc@5 100.00 ( 98.07)\n","Epoch: [81][ 90/391]\tTime  0.091 ( 0.095)\tLoss 4.3429e-01 (5.1539e-01)\tAcc@1  83.59 ( 84.14)\tAcc@5  97.66 ( 98.04)\n","Epoch: [81][120/391]\tTime  0.090 ( 0.094)\tLoss 3.9786e-01 (5.2196e-01)\tAcc@1  88.28 ( 84.00)\tAcc@5  99.22 ( 97.99)\n","Epoch: [81][150/391]\tTime  0.095 ( 0.094)\tLoss 5.2100e-01 (5.2885e-01)\tAcc@1  84.38 ( 83.79)\tAcc@5  99.22 ( 97.95)\n","Epoch: [81][180/391]\tTime  0.092 ( 0.094)\tLoss 5.3485e-01 (5.3102e-01)\tAcc@1  85.16 ( 83.74)\tAcc@5  97.66 ( 97.96)\n","Epoch: [81][210/391]\tTime  0.092 ( 0.094)\tLoss 5.5408e-01 (5.3157e-01)\tAcc@1  81.25 ( 83.71)\tAcc@5  96.88 ( 97.95)\n","Epoch: [81][240/391]\tTime  0.092 ( 0.093)\tLoss 5.6023e-01 (5.3360e-01)\tAcc@1  83.59 ( 83.66)\tAcc@5  98.44 ( 97.92)\n","Epoch: [81][270/391]\tTime  0.092 ( 0.093)\tLoss 4.6342e-01 (5.3704e-01)\tAcc@1  85.94 ( 83.65)\tAcc@5 100.00 ( 97.88)\n","Epoch: [81][300/391]\tTime  0.088 ( 0.093)\tLoss 6.3927e-01 (5.4186e-01)\tAcc@1  82.03 ( 83.52)\tAcc@5  96.09 ( 97.86)\n","Epoch: [81][330/391]\tTime  0.089 ( 0.093)\tLoss 4.8902e-01 (5.4551e-01)\tAcc@1  85.94 ( 83.47)\tAcc@5  97.66 ( 97.82)\n","Epoch: [81][360/391]\tTime  0.073 ( 0.093)\tLoss 6.3527e-01 (5.5028e-01)\tAcc@1  80.47 ( 83.34)\tAcc@5  96.88 ( 97.76)\n","Epoch: [81][390/391]\tTime  0.082 ( 0.093)\tLoss 7.4787e-01 (5.5319e-01)\tAcc@1  76.25 ( 83.26)\tAcc@5  96.25 ( 97.75)\n","==> Train Accuracy: Acc@1 83.262 || Acc@5 97.748\n","==> Test Accuracy:  Acc@1 69.790 || Acc@5 91.260\n","==> 39.13 seconds to train this epoch\n","\n","\n","----- epoch: 82, lr: 0.020000000000000004 -----\n","Epoch: [82][  0/391]\tTime  0.262 ( 0.262)\tLoss 4.6508e-01 (4.6508e-01)\tAcc@1  84.38 ( 84.38)\tAcc@5  98.44 ( 98.44)\n","Epoch: [82][ 30/391]\tTime  0.094 ( 0.099)\tLoss 5.1135e-01 (5.2420e-01)\tAcc@1  84.38 ( 84.25)\tAcc@5  96.88 ( 97.81)\n","Epoch: [82][ 60/391]\tTime  0.093 ( 0.096)\tLoss 4.3958e-01 (5.1850e-01)\tAcc@1  86.72 ( 84.68)\tAcc@5  98.44 ( 97.86)\n","Epoch: [82][ 90/391]\tTime  0.092 ( 0.094)\tLoss 6.4125e-01 (5.2540e-01)\tAcc@1  78.12 ( 84.31)\tAcc@5  96.88 ( 97.79)\n","Epoch: [82][120/391]\tTime  0.091 ( 0.094)\tLoss 5.2083e-01 (5.2644e-01)\tAcc@1  82.81 ( 84.14)\tAcc@5  99.22 ( 97.85)\n","Epoch: [82][150/391]\tTime  0.092 ( 0.094)\tLoss 4.7439e-01 (5.2298e-01)\tAcc@1  82.81 ( 84.20)\tAcc@5  98.44 ( 97.89)\n","Epoch: [82][180/391]\tTime  0.090 ( 0.093)\tLoss 5.1465e-01 (5.2167e-01)\tAcc@1  84.38 ( 84.25)\tAcc@5  99.22 ( 97.94)\n","Epoch: [82][210/391]\tTime  0.093 ( 0.093)\tLoss 5.6521e-01 (5.2391e-01)\tAcc@1  82.81 ( 84.17)\tAcc@5  96.88 ( 97.94)\n","Epoch: [82][240/391]\tTime  0.092 ( 0.093)\tLoss 7.5082e-01 (5.2488e-01)\tAcc@1  78.91 ( 84.11)\tAcc@5  96.09 ( 97.91)\n","Epoch: [82][270/391]\tTime  0.090 ( 0.093)\tLoss 5.1381e-01 (5.3164e-01)\tAcc@1  85.16 ( 83.91)\tAcc@5  97.66 ( 97.83)\n","Epoch: [82][300/391]\tTime  0.097 ( 0.093)\tLoss 5.8055e-01 (5.3563e-01)\tAcc@1  85.94 ( 83.78)\tAcc@5  96.88 ( 97.80)\n","Epoch: [82][330/391]\tTime  0.098 ( 0.093)\tLoss 6.0131e-01 (5.3537e-01)\tAcc@1  83.59 ( 83.79)\tAcc@5  96.88 ( 97.82)\n","Epoch: [82][360/391]\tTime  0.089 ( 0.093)\tLoss 5.5799e-01 (5.3783e-01)\tAcc@1  84.38 ( 83.69)\tAcc@5  95.31 ( 97.81)\n","Epoch: [82][390/391]\tTime  0.082 ( 0.093)\tLoss 6.8814e-01 (5.4149e-01)\tAcc@1  80.00 ( 83.53)\tAcc@5  95.00 ( 97.83)\n","==> Train Accuracy: Acc@1 83.528 || Acc@5 97.826\n","==> Test Accuracy:  Acc@1 68.690 || Acc@5 91.310\n","==> 38.98 seconds to train this epoch\n","\n","\n","----- epoch: 83, lr: 0.020000000000000004 -----\n","Epoch: [83][  0/391]\tTime  0.284 ( 0.284)\tLoss 4.3068e-01 (4.3068e-01)\tAcc@1  89.06 ( 89.06)\tAcc@5  98.44 ( 98.44)\n","Epoch: [83][ 30/391]\tTime  0.092 ( 0.100)\tLoss 3.5572e-01 (4.9667e-01)\tAcc@1  89.06 ( 84.63)\tAcc@5  98.44 ( 98.24)\n","Epoch: [83][ 60/391]\tTime  0.087 ( 0.096)\tLoss 3.6865e-01 (4.9968e-01)\tAcc@1  86.72 ( 84.70)\tAcc@5 100.00 ( 98.17)\n","Epoch: [83][ 90/391]\tTime  0.092 ( 0.095)\tLoss 5.0846e-01 (4.9451e-01)\tAcc@1  78.91 ( 84.81)\tAcc@5  96.88 ( 98.17)\n","Epoch: [83][120/391]\tTime  0.091 ( 0.094)\tLoss 4.9051e-01 (4.9922e-01)\tAcc@1  84.38 ( 84.73)\tAcc@5  99.22 ( 98.23)\n","Epoch: [83][150/391]\tTime  0.095 ( 0.094)\tLoss 5.6594e-01 (5.0398e-01)\tAcc@1  84.38 ( 84.60)\tAcc@5  96.88 ( 98.19)\n","Epoch: [83][180/391]\tTime  0.094 ( 0.093)\tLoss 3.4374e-01 (5.0351e-01)\tAcc@1  89.84 ( 84.56)\tAcc@5 100.00 ( 98.26)\n","Epoch: [83][210/391]\tTime  0.089 ( 0.093)\tLoss 5.0141e-01 (5.0907e-01)\tAcc@1  83.59 ( 84.42)\tAcc@5  96.09 ( 98.19)\n","Epoch: [83][240/391]\tTime  0.091 ( 0.093)\tLoss 4.5683e-01 (5.1496e-01)\tAcc@1  83.59 ( 84.20)\tAcc@5  98.44 ( 98.12)\n","Epoch: [83][270/391]\tTime  0.091 ( 0.093)\tLoss 5.0198e-01 (5.1930e-01)\tAcc@1  84.38 ( 84.13)\tAcc@5  98.44 ( 98.04)\n","Epoch: [83][300/391]\tTime  0.090 ( 0.093)\tLoss 5.6194e-01 (5.2319e-01)\tAcc@1  82.03 ( 84.02)\tAcc@5  98.44 ( 97.98)\n","Epoch: [83][330/391]\tTime  0.090 ( 0.093)\tLoss 6.8186e-01 (5.2910e-01)\tAcc@1  80.47 ( 83.80)\tAcc@5  96.88 ( 97.93)\n","Epoch: [83][360/391]\tTime  0.092 ( 0.093)\tLoss 5.9802e-01 (5.3482e-01)\tAcc@1  82.03 ( 83.68)\tAcc@5  97.66 ( 97.88)\n","Epoch: [83][390/391]\tTime  0.082 ( 0.093)\tLoss 4.1925e-01 (5.3786e-01)\tAcc@1  87.50 ( 83.58)\tAcc@5  97.50 ( 97.85)\n","==> Train Accuracy: Acc@1 83.580 || Acc@5 97.852\n","==> Test Accuracy:  Acc@1 70.880 || Acc@5 91.740\n","==> 38.96 seconds to train this epoch\n","\n","\n","----- epoch: 84, lr: 0.020000000000000004 -----\n","Epoch: [84][  0/391]\tTime  0.253 ( 0.253)\tLoss 4.0034e-01 (4.0034e-01)\tAcc@1  88.28 ( 88.28)\tAcc@5  98.44 ( 98.44)\n","Epoch: [84][ 30/391]\tTime  0.094 ( 0.099)\tLoss 5.1750e-01 (4.7065e-01)\tAcc@1  81.25 ( 85.43)\tAcc@5  97.66 ( 98.41)\n","Epoch: [84][ 60/391]\tTime  0.093 ( 0.097)\tLoss 4.5778e-01 (4.8734e-01)\tAcc@1  85.94 ( 85.17)\tAcc@5  98.44 ( 98.21)\n","Epoch: [84][ 90/391]\tTime  0.094 ( 0.095)\tLoss 3.3063e-01 (4.8685e-01)\tAcc@1  89.06 ( 85.04)\tAcc@5 100.00 ( 98.29)\n","Epoch: [84][120/391]\tTime  0.094 ( 0.094)\tLoss 3.7598e-01 (4.9143e-01)\tAcc@1  89.06 ( 84.78)\tAcc@5  99.22 ( 98.24)\n","Epoch: [84][150/391]\tTime  0.091 ( 0.094)\tLoss 5.5465e-01 (4.9548e-01)\tAcc@1  85.94 ( 84.76)\tAcc@5  96.88 ( 98.18)\n","Epoch: [84][180/391]\tTime  0.090 ( 0.094)\tLoss 4.6996e-01 (5.0139e-01)\tAcc@1  90.62 ( 84.61)\tAcc@5  96.88 ( 98.15)\n","Epoch: [84][210/391]\tTime  0.092 ( 0.093)\tLoss 6.7566e-01 (5.0943e-01)\tAcc@1  82.81 ( 84.38)\tAcc@5  96.88 ( 98.08)\n","Epoch: [84][240/391]\tTime  0.092 ( 0.093)\tLoss 5.0496e-01 (5.1046e-01)\tAcc@1  80.47 ( 84.28)\tAcc@5  99.22 ( 98.10)\n","Epoch: [84][270/391]\tTime  0.093 ( 0.093)\tLoss 4.9811e-01 (5.1389e-01)\tAcc@1  88.28 ( 84.19)\tAcc@5  95.31 ( 98.05)\n","Epoch: [84][300/391]\tTime  0.094 ( 0.093)\tLoss 6.3906e-01 (5.1804e-01)\tAcc@1  81.25 ( 84.02)\tAcc@5  96.09 ( 98.06)\n","Epoch: [84][330/391]\tTime  0.096 ( 0.093)\tLoss 5.8118e-01 (5.2355e-01)\tAcc@1  85.16 ( 83.90)\tAcc@5  96.09 ( 97.99)\n","Epoch: [84][360/391]\tTime  0.091 ( 0.093)\tLoss 6.1260e-01 (5.2590e-01)\tAcc@1  78.91 ( 83.87)\tAcc@5  97.66 ( 97.98)\n","Epoch: [84][390/391]\tTime  0.082 ( 0.093)\tLoss 6.3285e-01 (5.2998e-01)\tAcc@1  81.25 ( 83.74)\tAcc@5  95.00 ( 97.95)\n","==> Train Accuracy: Acc@1 83.738 || Acc@5 97.948\n","==> Test Accuracy:  Acc@1 70.780 || Acc@5 92.020\n","==> 39.03 seconds to train this epoch\n","\n","\n","----- epoch: 85, lr: 0.020000000000000004 -----\n","Epoch: [85][  0/391]\tTime  0.285 ( 0.285)\tLoss 4.4258e-01 (4.4258e-01)\tAcc@1  86.72 ( 86.72)\tAcc@5  99.22 ( 99.22)\n","Epoch: [85][ 30/391]\tTime  0.091 ( 0.099)\tLoss 4.4175e-01 (4.9540e-01)\tAcc@1  86.72 ( 84.93)\tAcc@5  97.66 ( 98.29)\n","Epoch: [85][ 60/391]\tTime  0.091 ( 0.096)\tLoss 5.5966e-01 (4.9253e-01)\tAcc@1  85.16 ( 84.96)\tAcc@5  96.88 ( 98.25)\n","Epoch: [85][ 90/391]\tTime  0.091 ( 0.094)\tLoss 3.9589e-01 (4.9567e-01)\tAcc@1  86.72 ( 84.89)\tAcc@5  98.44 ( 98.19)\n","Epoch: [85][120/391]\tTime  0.099 ( 0.094)\tLoss 5.2800e-01 (4.9311e-01)\tAcc@1  84.38 ( 85.14)\tAcc@5  98.44 ( 98.11)\n","Epoch: [85][150/391]\tTime  0.095 ( 0.094)\tLoss 5.7783e-01 (4.9903e-01)\tAcc@1  82.81 ( 84.96)\tAcc@5  98.44 ( 98.07)\n","Epoch: [85][180/391]\tTime  0.091 ( 0.093)\tLoss 4.7738e-01 (5.0341e-01)\tAcc@1  85.94 ( 84.75)\tAcc@5  99.22 ( 98.04)\n","Epoch: [85][210/391]\tTime  0.089 ( 0.093)\tLoss 5.7624e-01 (5.0972e-01)\tAcc@1  78.91 ( 84.52)\tAcc@5  97.66 ( 98.00)\n","Epoch: [85][240/391]\tTime  0.088 ( 0.093)\tLoss 5.7106e-01 (5.1527e-01)\tAcc@1  83.59 ( 84.38)\tAcc@5  96.88 ( 97.96)\n","Epoch: [85][270/391]\tTime  0.093 ( 0.093)\tLoss 5.2505e-01 (5.1681e-01)\tAcc@1  85.94 ( 84.35)\tAcc@5  97.66 ( 97.93)\n","Epoch: [85][300/391]\tTime  0.093 ( 0.093)\tLoss 4.6012e-01 (5.1944e-01)\tAcc@1  85.94 ( 84.27)\tAcc@5  98.44 ( 97.92)\n","Epoch: [85][330/391]\tTime  0.093 ( 0.093)\tLoss 6.3434e-01 (5.2341e-01)\tAcc@1  80.47 ( 84.15)\tAcc@5  99.22 ( 97.89)\n","Epoch: [85][360/391]\tTime  0.090 ( 0.093)\tLoss 4.8260e-01 (5.2437e-01)\tAcc@1  86.72 ( 84.12)\tAcc@5  97.66 ( 97.91)\n","Epoch: [85][390/391]\tTime  0.082 ( 0.093)\tLoss 5.2349e-01 (5.2855e-01)\tAcc@1  87.50 ( 84.00)\tAcc@5 100.00 ( 97.87)\n","==> Train Accuracy: Acc@1 84.004 || Acc@5 97.874\n","==> Test Accuracy:  Acc@1 69.250 || Acc@5 91.260\n","==> 38.95 seconds to train this epoch\n","\n","\n","----- epoch: 86, lr: 0.020000000000000004 -----\n","Epoch: [86][  0/391]\tTime  0.273 ( 0.273)\tLoss 4.4163e-01 (4.4163e-01)\tAcc@1  87.50 ( 87.50)\tAcc@5  99.22 ( 99.22)\n","Epoch: [86][ 30/391]\tTime  0.090 ( 0.099)\tLoss 6.4473e-01 (4.8354e-01)\tAcc@1  82.81 ( 85.43)\tAcc@5  97.66 ( 98.24)\n","Epoch: [86][ 60/391]\tTime  0.088 ( 0.096)\tLoss 6.0680e-01 (4.7612e-01)\tAcc@1  82.81 ( 85.76)\tAcc@5  96.88 ( 98.23)\n","Epoch: [86][ 90/391]\tTime  0.092 ( 0.094)\tLoss 4.5985e-01 (4.7997e-01)\tAcc@1  91.41 ( 85.60)\tAcc@5  96.88 ( 98.17)\n","Epoch: [86][120/391]\tTime  0.093 ( 0.094)\tLoss 5.1148e-01 (4.9162e-01)\tAcc@1  85.94 ( 85.31)\tAcc@5  98.44 ( 98.07)\n","Epoch: [86][150/391]\tTime  0.091 ( 0.094)\tLoss 6.1048e-01 (4.9407e-01)\tAcc@1  81.25 ( 85.20)\tAcc@5  97.66 ( 98.08)\n","Epoch: [86][180/391]\tTime  0.092 ( 0.093)\tLoss 4.4945e-01 (4.9739e-01)\tAcc@1  89.06 ( 85.10)\tAcc@5  98.44 ( 98.01)\n","Epoch: [86][210/391]\tTime  0.092 ( 0.093)\tLoss 3.7915e-01 (5.0114e-01)\tAcc@1  89.06 ( 84.98)\tAcc@5  98.44 ( 97.98)\n","Epoch: [86][240/391]\tTime  0.090 ( 0.093)\tLoss 5.7082e-01 (5.0365e-01)\tAcc@1  78.12 ( 84.95)\tAcc@5  99.22 ( 97.94)\n","Epoch: [86][270/391]\tTime  0.094 ( 0.093)\tLoss 6.1921e-01 (5.0594e-01)\tAcc@1  80.47 ( 84.79)\tAcc@5  96.88 ( 97.96)\n","Epoch: [86][300/391]\tTime  0.084 ( 0.093)\tLoss 4.5074e-01 (5.1005e-01)\tAcc@1  85.94 ( 84.67)\tAcc@5  97.66 ( 97.91)\n","Epoch: [86][330/391]\tTime  0.093 ( 0.093)\tLoss 5.4719e-01 (5.1528e-01)\tAcc@1  81.25 ( 84.49)\tAcc@5  99.22 ( 97.89)\n","Epoch: [86][360/391]\tTime  0.099 ( 0.093)\tLoss 6.8421e-01 (5.1977e-01)\tAcc@1  82.81 ( 84.32)\tAcc@5  96.88 ( 97.88)\n","Epoch: [86][390/391]\tTime  0.082 ( 0.093)\tLoss 5.4407e-01 (5.2367e-01)\tAcc@1  82.50 ( 84.18)\tAcc@5  97.50 ( 97.84)\n","==> Train Accuracy: Acc@1 84.178 || Acc@5 97.844\n","==> Test Accuracy:  Acc@1 70.950 || Acc@5 91.930\n","==> 38.92 seconds to train this epoch\n","\n","\n","----- epoch: 87, lr: 0.020000000000000004 -----\n","Epoch: [87][  0/391]\tTime  0.291 ( 0.291)\tLoss 4.2233e-01 (4.2233e-01)\tAcc@1  89.84 ( 89.84)\tAcc@5  99.22 ( 99.22)\n","Epoch: [87][ 30/391]\tTime  0.096 ( 0.099)\tLoss 4.7559e-01 (4.9292e-01)\tAcc@1  84.38 ( 85.46)\tAcc@5  99.22 ( 98.44)\n","Epoch: [87][ 60/391]\tTime  0.089 ( 0.096)\tLoss 4.4883e-01 (4.8170e-01)\tAcc@1  85.94 ( 85.60)\tAcc@5  97.66 ( 98.37)\n","Epoch: [87][ 90/391]\tTime  0.090 ( 0.095)\tLoss 4.1936e-01 (4.8904e-01)\tAcc@1  84.38 ( 85.24)\tAcc@5 100.00 ( 98.33)\n","Epoch: [87][120/391]\tTime  0.093 ( 0.094)\tLoss 5.2136e-01 (4.8412e-01)\tAcc@1  85.16 ( 85.39)\tAcc@5  99.22 ( 98.36)\n","Epoch: [87][150/391]\tTime  0.091 ( 0.094)\tLoss 5.1489e-01 (4.7970e-01)\tAcc@1  83.59 ( 85.44)\tAcc@5  99.22 ( 98.37)\n","Epoch: [87][180/391]\tTime  0.091 ( 0.093)\tLoss 4.3201e-01 (4.7861e-01)\tAcc@1  88.28 ( 85.38)\tAcc@5  98.44 ( 98.39)\n","Epoch: [87][210/391]\tTime  0.091 ( 0.093)\tLoss 4.4564e-01 (4.7900e-01)\tAcc@1  85.94 ( 85.34)\tAcc@5  98.44 ( 98.36)\n","Epoch: [87][240/391]\tTime  0.094 ( 0.093)\tLoss 5.3452e-01 (4.8515e-01)\tAcc@1  83.59 ( 85.11)\tAcc@5  98.44 ( 98.35)\n","Epoch: [87][270/391]\tTime  0.091 ( 0.093)\tLoss 5.8494e-01 (4.9394e-01)\tAcc@1  83.59 ( 84.83)\tAcc@5  97.66 ( 98.28)\n","Epoch: [87][300/391]\tTime  0.089 ( 0.093)\tLoss 6.0419e-01 (5.0135e-01)\tAcc@1  82.03 ( 84.66)\tAcc@5  97.66 ( 98.22)\n","Epoch: [87][330/391]\tTime  0.091 ( 0.093)\tLoss 5.2228e-01 (5.1095e-01)\tAcc@1  83.59 ( 84.37)\tAcc@5  96.88 ( 98.13)\n","Epoch: [87][360/391]\tTime  0.095 ( 0.093)\tLoss 4.3543e-01 (5.1449e-01)\tAcc@1  87.50 ( 84.27)\tAcc@5  97.66 ( 98.10)\n","Epoch: [87][390/391]\tTime  0.082 ( 0.093)\tLoss 7.1839e-01 (5.2008e-01)\tAcc@1  75.00 ( 84.11)\tAcc@5  96.25 ( 98.04)\n","==> Train Accuracy: Acc@1 84.106 || Acc@5 98.044\n","==> Test Accuracy:  Acc@1 69.900 || Acc@5 91.680\n","==> 39.05 seconds to train this epoch\n","\n","\n","----- epoch: 88, lr: 0.020000000000000004 -----\n","Epoch: [88][  0/391]\tTime  0.268 ( 0.268)\tLoss 3.8226e-01 (3.8226e-01)\tAcc@1  89.06 ( 89.06)\tAcc@5  98.44 ( 98.44)\n","Epoch: [88][ 30/391]\tTime  0.097 ( 0.099)\tLoss 5.4638e-01 (4.6688e-01)\tAcc@1  85.16 ( 85.94)\tAcc@5  97.66 ( 98.29)\n","Epoch: [88][ 60/391]\tTime  0.093 ( 0.096)\tLoss 4.6359e-01 (4.6388e-01)\tAcc@1  86.72 ( 85.84)\tAcc@5  99.22 ( 98.31)\n","Epoch: [88][ 90/391]\tTime  0.091 ( 0.095)\tLoss 3.7911e-01 (4.6971e-01)\tAcc@1  90.62 ( 85.83)\tAcc@5  99.22 ( 98.26)\n","Epoch: [88][120/391]\tTime  0.093 ( 0.094)\tLoss 5.2407e-01 (4.7795e-01)\tAcc@1  89.06 ( 85.48)\tAcc@5  97.66 ( 98.23)\n","Epoch: [88][150/391]\tTime  0.092 ( 0.094)\tLoss 3.9859e-01 (4.8254e-01)\tAcc@1  88.28 ( 85.30)\tAcc@5  96.88 ( 98.15)\n","Epoch: [88][180/391]\tTime  0.092 ( 0.094)\tLoss 6.1218e-01 (4.8954e-01)\tAcc@1  81.25 ( 85.21)\tAcc@5  96.88 ( 98.04)\n","Epoch: [88][210/391]\tTime  0.090 ( 0.094)\tLoss 5.9795e-01 (4.9739e-01)\tAcc@1  81.25 ( 85.03)\tAcc@5  98.44 ( 98.04)\n","Epoch: [88][240/391]\tTime  0.090 ( 0.093)\tLoss 4.9906e-01 (5.0102e-01)\tAcc@1  89.06 ( 84.90)\tAcc@5  97.66 ( 98.01)\n","Epoch: [88][270/391]\tTime  0.110 ( 0.093)\tLoss 5.1737e-01 (5.0339e-01)\tAcc@1  82.81 ( 84.86)\tAcc@5 100.00 ( 98.00)\n","Epoch: [88][300/391]\tTime  0.091 ( 0.093)\tLoss 4.1103e-01 (5.0702e-01)\tAcc@1  89.84 ( 84.67)\tAcc@5  97.66 ( 98.02)\n","Epoch: [88][330/391]\tTime  0.091 ( 0.093)\tLoss 4.8104e-01 (5.1184e-01)\tAcc@1  85.16 ( 84.51)\tAcc@5  98.44 ( 97.98)\n","Epoch: [88][360/391]\tTime  0.096 ( 0.093)\tLoss 4.7119e-01 (5.1406e-01)\tAcc@1  84.38 ( 84.45)\tAcc@5  97.66 ( 97.99)\n","Epoch: [88][390/391]\tTime  0.084 ( 0.093)\tLoss 3.7897e-01 (5.1704e-01)\tAcc@1  90.00 ( 84.35)\tAcc@5  98.75 ( 97.97)\n","==> Train Accuracy: Acc@1 84.350 || Acc@5 97.972\n","==> Test Accuracy:  Acc@1 70.650 || Acc@5 92.090\n","==> 39.00 seconds to train this epoch\n","\n","\n","----- epoch: 89, lr: 0.020000000000000004 -----\n","Epoch: [89][  0/391]\tTime  0.276 ( 0.276)\tLoss 3.0476e-01 (3.0476e-01)\tAcc@1  91.41 ( 91.41)\tAcc@5 100.00 (100.00)\n","Epoch: [89][ 30/391]\tTime  0.112 ( 0.100)\tLoss 5.1832e-01 (4.7907e-01)\tAcc@1  86.72 ( 85.48)\tAcc@5  96.09 ( 98.16)\n","Epoch: [89][ 60/391]\tTime  0.097 ( 0.097)\tLoss 3.7513e-01 (4.6135e-01)\tAcc@1  89.06 ( 86.12)\tAcc@5  98.44 ( 98.27)\n","Epoch: [89][ 90/391]\tTime  0.091 ( 0.095)\tLoss 4.4931e-01 (4.7355e-01)\tAcc@1  85.94 ( 85.71)\tAcc@5  97.66 ( 98.26)\n","Epoch: [89][120/391]\tTime  0.092 ( 0.095)\tLoss 3.8207e-01 (4.7592e-01)\tAcc@1  89.06 ( 85.52)\tAcc@5 100.00 ( 98.26)\n","Epoch: [89][150/391]\tTime  0.092 ( 0.094)\tLoss 6.5163e-01 (4.8300e-01)\tAcc@1  78.12 ( 85.31)\tAcc@5  96.09 ( 98.18)\n","Epoch: [89][180/391]\tTime  0.092 ( 0.094)\tLoss 4.2182e-01 (4.8474e-01)\tAcc@1  85.94 ( 85.29)\tAcc@5  98.44 ( 98.22)\n","Epoch: [89][210/391]\tTime  0.091 ( 0.094)\tLoss 4.2472e-01 (4.8586e-01)\tAcc@1  86.72 ( 85.27)\tAcc@5  98.44 ( 98.17)\n","Epoch: [89][240/391]\tTime  0.096 ( 0.094)\tLoss 3.5617e-01 (4.8707e-01)\tAcc@1  89.06 ( 85.24)\tAcc@5  97.66 ( 98.15)\n","Epoch: [89][270/391]\tTime  0.089 ( 0.093)\tLoss 6.9792e-01 (4.9131e-01)\tAcc@1  80.47 ( 85.06)\tAcc@5  96.09 ( 98.15)\n","Epoch: [89][300/391]\tTime  0.087 ( 0.093)\tLoss 4.2613e-01 (4.9173e-01)\tAcc@1  84.38 ( 85.08)\tAcc@5  96.88 ( 98.16)\n","Epoch: [89][330/391]\tTime  0.092 ( 0.093)\tLoss 5.0899e-01 (4.9470e-01)\tAcc@1  84.38 ( 84.96)\tAcc@5  98.44 ( 98.14)\n","Epoch: [89][360/391]\tTime  0.096 ( 0.093)\tLoss 5.9131e-01 (4.9988e-01)\tAcc@1  80.47 ( 84.79)\tAcc@5  96.88 ( 98.11)\n","Epoch: [89][390/391]\tTime  0.082 ( 0.093)\tLoss 4.7028e-01 (5.0403e-01)\tAcc@1  86.25 ( 84.73)\tAcc@5  98.75 ( 98.06)\n","==> Train Accuracy: Acc@1 84.726 || Acc@5 98.058\n","==> Test Accuracy:  Acc@1 70.530 || Acc@5 92.040\n","==> 39.06 seconds to train this epoch\n","\n","\n","----- epoch: 90, lr: 0.004000000000000001 -----\n","Epoch: [90][  0/391]\tTime  0.286 ( 0.286)\tLoss 5.4208e-01 (5.4208e-01)\tAcc@1  85.16 ( 85.16)\tAcc@5  96.88 ( 96.88)\n","Epoch: [90][ 30/391]\tTime  0.089 ( 0.100)\tLoss 4.3068e-01 (4.2338e-01)\tAcc@1  86.72 ( 87.32)\tAcc@5  99.22 ( 98.54)\n","Epoch: [90][ 60/391]\tTime  0.092 ( 0.096)\tLoss 3.7712e-01 (3.9207e-01)\tAcc@1  89.06 ( 88.47)\tAcc@5  98.44 ( 98.54)\n","Epoch: [90][ 90/391]\tTime  0.092 ( 0.095)\tLoss 2.5575e-01 (3.6804e-01)\tAcc@1  94.53 ( 89.28)\tAcc@5  99.22 ( 98.79)\n","Epoch: [90][120/391]\tTime  0.093 ( 0.094)\tLoss 2.1566e-01 (3.5495e-01)\tAcc@1  93.75 ( 89.70)\tAcc@5 100.00 ( 98.87)\n","Epoch: [90][150/391]\tTime  0.087 ( 0.094)\tLoss 2.9592e-01 (3.4273e-01)\tAcc@1  90.62 ( 89.97)\tAcc@5  97.66 ( 98.95)\n","Epoch: [90][180/391]\tTime  0.092 ( 0.093)\tLoss 2.3150e-01 (3.3495e-01)\tAcc@1  92.97 ( 90.15)\tAcc@5 100.00 ( 99.01)\n","Epoch: [90][210/391]\tTime  0.092 ( 0.093)\tLoss 3.3301e-01 (3.2903e-01)\tAcc@1  91.41 ( 90.38)\tAcc@5  99.22 ( 99.01)\n","Epoch: [90][240/391]\tTime  0.092 ( 0.093)\tLoss 2.8675e-01 (3.2072e-01)\tAcc@1  93.75 ( 90.68)\tAcc@5  98.44 ( 99.06)\n","Epoch: [90][270/391]\tTime  0.091 ( 0.093)\tLoss 1.8851e-01 (3.1608e-01)\tAcc@1  96.09 ( 90.80)\tAcc@5  99.22 ( 99.09)\n","Epoch: [90][300/391]\tTime  0.090 ( 0.093)\tLoss 1.9759e-01 (3.0952e-01)\tAcc@1  95.31 ( 91.03)\tAcc@5 100.00 ( 99.13)\n","Epoch: [90][330/391]\tTime  0.092 ( 0.093)\tLoss 1.3470e-01 (3.0806e-01)\tAcc@1  97.66 ( 91.09)\tAcc@5 100.00 ( 99.13)\n","Epoch: [90][360/391]\tTime  0.087 ( 0.093)\tLoss 2.6880e-01 (3.0346e-01)\tAcc@1  92.19 ( 91.23)\tAcc@5  98.44 ( 99.15)\n","Epoch: [90][390/391]\tTime  0.082 ( 0.093)\tLoss 3.6262e-01 (2.9921e-01)\tAcc@1  92.50 ( 91.31)\tAcc@5  98.75 ( 99.18)\n","==> Train Accuracy: Acc@1 91.314 || Acc@5 99.176\n","==> Test Accuracy:  Acc@1 76.580 || Acc@5 94.560\n","==> 39.02 seconds to train this epoch\n","\n","\n","----- epoch: 91, lr: 0.004000000000000001 -----\n","Epoch: [91][  0/391]\tTime  0.261 ( 0.261)\tLoss 2.3732e-01 (2.3732e-01)\tAcc@1  94.53 ( 94.53)\tAcc@5 100.00 (100.00)\n","Epoch: [91][ 30/391]\tTime  0.093 ( 0.099)\tLoss 2.9110e-01 (2.4941e-01)\tAcc@1  92.19 ( 92.82)\tAcc@5  96.88 ( 99.24)\n","Epoch: [91][ 60/391]\tTime  0.102 ( 0.096)\tLoss 1.7618e-01 (2.3893e-01)\tAcc@1  96.88 ( 93.19)\tAcc@5  98.44 ( 99.23)\n","Epoch: [91][ 90/391]\tTime  0.093 ( 0.095)\tLoss 2.8967e-01 (2.3441e-01)\tAcc@1  90.62 ( 93.37)\tAcc@5 100.00 ( 99.30)\n","Epoch: [91][120/391]\tTime  0.092 ( 0.095)\tLoss 2.6908e-01 (2.3484e-01)\tAcc@1  92.97 ( 93.34)\tAcc@5  99.22 ( 99.34)\n","Epoch: [91][150/391]\tTime  0.090 ( 0.094)\tLoss 1.4376e-01 (2.3421e-01)\tAcc@1  96.09 ( 93.37)\tAcc@5 100.00 ( 99.37)\n","Epoch: [91][180/391]\tTime  0.098 ( 0.094)\tLoss 2.3481e-01 (2.3258e-01)\tAcc@1  92.97 ( 93.40)\tAcc@5  99.22 ( 99.37)\n","Epoch: [91][210/391]\tTime  0.098 ( 0.094)\tLoss 1.3540e-01 (2.3116e-01)\tAcc@1  97.66 ( 93.42)\tAcc@5  99.22 ( 99.37)\n","Epoch: [91][240/391]\tTime  0.094 ( 0.093)\tLoss 1.8328e-01 (2.3221e-01)\tAcc@1  93.75 ( 93.38)\tAcc@5 100.00 ( 99.36)\n","Epoch: [91][270/391]\tTime  0.090 ( 0.093)\tLoss 1.4384e-01 (2.3034e-01)\tAcc@1  96.09 ( 93.45)\tAcc@5 100.00 ( 99.37)\n","Epoch: [91][300/391]\tTime  0.092 ( 0.093)\tLoss 1.5384e-01 (2.3012e-01)\tAcc@1  96.09 ( 93.43)\tAcc@5 100.00 ( 99.38)\n","Epoch: [91][330/391]\tTime  0.099 ( 0.093)\tLoss 1.9149e-01 (2.2895e-01)\tAcc@1  96.88 ( 93.47)\tAcc@5  99.22 ( 99.40)\n","Epoch: [91][360/391]\tTime  0.092 ( 0.093)\tLoss 2.5751e-01 (2.2681e-01)\tAcc@1  95.31 ( 93.55)\tAcc@5  99.22 ( 99.41)\n","Epoch: [91][390/391]\tTime  0.083 ( 0.093)\tLoss 8.1233e-02 (2.2598e-01)\tAcc@1  98.75 ( 93.56)\tAcc@5 100.00 ( 99.42)\n","==> Train Accuracy: Acc@1 93.564 || Acc@5 99.424\n","==> Test Accuracy:  Acc@1 76.500 || Acc@5 94.410\n","==> 39.08 seconds to train this epoch\n","\n","\n","----- epoch: 92, lr: 0.004000000000000001 -----\n","Epoch: [92][  0/391]\tTime  0.284 ( 0.284)\tLoss 1.0144e-01 (1.0144e-01)\tAcc@1  97.66 ( 97.66)\tAcc@5 100.00 (100.00)\n","Epoch: [92][ 30/391]\tTime  0.089 ( 0.099)\tLoss 2.0022e-01 (1.9168e-01)\tAcc@1  94.53 ( 95.01)\tAcc@5 100.00 ( 99.57)\n","Epoch: [92][ 60/391]\tTime  0.094 ( 0.096)\tLoss 2.2044e-01 (1.9638e-01)\tAcc@1  93.75 ( 94.47)\tAcc@5  99.22 ( 99.56)\n","Epoch: [92][ 90/391]\tTime  0.090 ( 0.095)\tLoss 1.3456e-01 (1.9736e-01)\tAcc@1  96.09 ( 94.38)\tAcc@5 100.00 ( 99.60)\n","Epoch: [92][120/391]\tTime  0.091 ( 0.094)\tLoss 1.9579e-01 (1.9528e-01)\tAcc@1  94.53 ( 94.49)\tAcc@5  98.44 ( 99.61)\n","Epoch: [92][150/391]\tTime  0.091 ( 0.094)\tLoss 1.7135e-01 (1.9305e-01)\tAcc@1  95.31 ( 94.47)\tAcc@5 100.00 ( 99.63)\n","Epoch: [92][180/391]\tTime  0.089 ( 0.094)\tLoss 1.6073e-01 (1.9331e-01)\tAcc@1  96.09 ( 94.44)\tAcc@5  99.22 ( 99.65)\n","Epoch: [92][210/391]\tTime  0.101 ( 0.093)\tLoss 1.5930e-01 (1.9486e-01)\tAcc@1  95.31 ( 94.39)\tAcc@5 100.00 ( 99.63)\n","Epoch: [92][240/391]\tTime  0.094 ( 0.093)\tLoss 2.2197e-01 (1.9453e-01)\tAcc@1  93.75 ( 94.41)\tAcc@5  99.22 ( 99.63)\n","Epoch: [92][270/391]\tTime  0.095 ( 0.093)\tLoss 1.1824e-01 (1.9667e-01)\tAcc@1  98.44 ( 94.36)\tAcc@5 100.00 ( 99.62)\n","Epoch: [92][300/391]\tTime  0.092 ( 0.093)\tLoss 1.8524e-01 (1.9494e-01)\tAcc@1  92.97 ( 94.43)\tAcc@5 100.00 ( 99.61)\n","Epoch: [92][330/391]\tTime  0.094 ( 0.093)\tLoss 1.9159e-01 (1.9562e-01)\tAcc@1  96.88 ( 94.44)\tAcc@5  99.22 ( 99.59)\n","Epoch: [92][360/391]\tTime  0.091 ( 0.093)\tLoss 4.0346e-01 (1.9635e-01)\tAcc@1  88.28 ( 94.41)\tAcc@5  98.44 ( 99.59)\n","Epoch: [92][390/391]\tTime  0.082 ( 0.093)\tLoss 1.3618e-01 (1.9670e-01)\tAcc@1  97.50 ( 94.40)\tAcc@5 100.00 ( 99.58)\n","==> Train Accuracy: Acc@1 94.398 || Acc@5 99.580\n","==> Test Accuracy:  Acc@1 76.870 || Acc@5 94.610\n","==> 39.05 seconds to train this epoch\n","\n","\n","----- epoch: 93, lr: 0.004000000000000001 -----\n","Epoch: [93][  0/391]\tTime  0.269 ( 0.269)\tLoss 1.5197e-01 (1.5197e-01)\tAcc@1  95.31 ( 95.31)\tAcc@5 100.00 (100.00)\n","Epoch: [93][ 30/391]\tTime  0.094 ( 0.099)\tLoss 1.1974e-01 (1.7668e-01)\tAcc@1  97.66 ( 94.93)\tAcc@5  99.22 ( 99.62)\n","Epoch: [93][ 60/391]\tTime  0.095 ( 0.096)\tLoss 1.2858e-01 (1.7605e-01)\tAcc@1  94.53 ( 94.81)\tAcc@5 100.00 ( 99.59)\n","Epoch: [93][ 90/391]\tTime  0.093 ( 0.095)\tLoss 1.4047e-01 (1.7458e-01)\tAcc@1  95.31 ( 95.00)\tAcc@5 100.00 ( 99.63)\n","Epoch: [93][120/391]\tTime  0.094 ( 0.094)\tLoss 1.4554e-01 (1.7844e-01)\tAcc@1  96.09 ( 94.91)\tAcc@5  99.22 ( 99.61)\n","Epoch: [93][150/391]\tTime  0.092 ( 0.094)\tLoss 1.4064e-01 (1.7551e-01)\tAcc@1  96.88 ( 95.03)\tAcc@5 100.00 ( 99.63)\n","Epoch: [93][180/391]\tTime  0.088 ( 0.094)\tLoss 2.3338e-01 (1.7526e-01)\tAcc@1  93.75 ( 95.08)\tAcc@5  98.44 ( 99.62)\n","Epoch: [93][210/391]\tTime  0.092 ( 0.093)\tLoss 1.9418e-01 (1.7627e-01)\tAcc@1  92.97 ( 95.02)\tAcc@5 100.00 ( 99.62)\n","Epoch: [93][240/391]\tTime  0.093 ( 0.093)\tLoss 1.7638e-01 (1.7723e-01)\tAcc@1  92.97 ( 94.99)\tAcc@5 100.00 ( 99.62)\n","Epoch: [93][270/391]\tTime  0.091 ( 0.093)\tLoss 1.7195e-01 (1.7754e-01)\tAcc@1  94.53 ( 94.96)\tAcc@5  99.22 ( 99.64)\n","Epoch: [93][300/391]\tTime  0.094 ( 0.093)\tLoss 1.6403e-01 (1.7861e-01)\tAcc@1  96.09 ( 94.96)\tAcc@5  99.22 ( 99.64)\n","Epoch: [93][330/391]\tTime  0.096 ( 0.093)\tLoss 2.0189e-01 (1.7872e-01)\tAcc@1  93.75 ( 94.96)\tAcc@5 100.00 ( 99.65)\n","Epoch: [93][360/391]\tTime  0.114 ( 0.093)\tLoss 2.8538e-01 (1.7889e-01)\tAcc@1  91.41 ( 94.97)\tAcc@5  99.22 ( 99.64)\n","Epoch: [93][390/391]\tTime  0.082 ( 0.093)\tLoss 1.0209e-01 (1.7892e-01)\tAcc@1 100.00 ( 94.97)\tAcc@5 100.00 ( 99.65)\n","==> Train Accuracy: Acc@1 94.972 || Acc@5 99.650\n","==> Test Accuracy:  Acc@1 76.660 || Acc@5 94.640\n","==> 39.09 seconds to train this epoch\n","\n","\n","----- epoch: 94, lr: 0.004000000000000001 -----\n","Epoch: [94][  0/391]\tTime  0.255 ( 0.255)\tLoss 1.8243e-01 (1.8243e-01)\tAcc@1  92.19 ( 92.19)\tAcc@5 100.00 (100.00)\n","Epoch: [94][ 30/391]\tTime  0.093 ( 0.098)\tLoss 1.2851e-01 (1.6182e-01)\tAcc@1  96.88 ( 95.51)\tAcc@5 100.00 ( 99.62)\n","Epoch: [94][ 60/391]\tTime  0.094 ( 0.095)\tLoss 2.3715e-01 (1.6729e-01)\tAcc@1  92.19 ( 95.38)\tAcc@5  99.22 ( 99.67)\n","Epoch: [94][ 90/391]\tTime  0.091 ( 0.094)\tLoss 1.9497e-01 (1.6857e-01)\tAcc@1  95.31 ( 95.33)\tAcc@5  99.22 ( 99.65)\n","Epoch: [94][120/391]\tTime  0.094 ( 0.094)\tLoss 1.4666e-01 (1.6725e-01)\tAcc@1  96.09 ( 95.30)\tAcc@5 100.00 ( 99.68)\n","Epoch: [94][150/391]\tTime  0.091 ( 0.094)\tLoss 1.5226e-01 (1.6634e-01)\tAcc@1  94.53 ( 95.35)\tAcc@5 100.00 ( 99.70)\n","Epoch: [94][180/391]\tTime  0.103 ( 0.094)\tLoss 9.6419e-02 (1.6501e-01)\tAcc@1  97.66 ( 95.43)\tAcc@5 100.00 ( 99.69)\n","Epoch: [94][210/391]\tTime  0.095 ( 0.093)\tLoss 1.8713e-01 (1.6747e-01)\tAcc@1  96.88 ( 95.39)\tAcc@5  99.22 ( 99.69)\n","Epoch: [94][240/391]\tTime  0.091 ( 0.093)\tLoss 2.1704e-01 (1.6753e-01)\tAcc@1  92.97 ( 95.38)\tAcc@5 100.00 ( 99.69)\n","Epoch: [94][270/391]\tTime  0.093 ( 0.093)\tLoss 1.4006e-01 (1.6857e-01)\tAcc@1  95.31 ( 95.32)\tAcc@5 100.00 ( 99.68)\n","Epoch: [94][300/391]\tTime  0.094 ( 0.093)\tLoss 1.5726e-01 (1.6744e-01)\tAcc@1  96.09 ( 95.36)\tAcc@5 100.00 ( 99.68)\n","Epoch: [94][330/391]\tTime  0.093 ( 0.093)\tLoss 1.5339e-01 (1.6741e-01)\tAcc@1  96.09 ( 95.36)\tAcc@5  99.22 ( 99.69)\n","Epoch: [94][360/391]\tTime  0.091 ( 0.093)\tLoss 2.4224e-01 (1.6706e-01)\tAcc@1  92.97 ( 95.39)\tAcc@5  98.44 ( 99.68)\n","Epoch: [94][390/391]\tTime  0.080 ( 0.093)\tLoss 1.5097e-01 (1.6729e-01)\tAcc@1  97.50 ( 95.36)\tAcc@5  98.75 ( 99.68)\n","==> Train Accuracy: Acc@1 95.362 || Acc@5 99.682\n","==> Test Accuracy:  Acc@1 76.820 || Acc@5 94.560\n","==> 39.01 seconds to train this epoch\n","\n","\n","----- epoch: 95, lr: 0.004000000000000001 -----\n","Epoch: [95][  0/391]\tTime  0.288 ( 0.288)\tLoss 1.7221e-01 (1.7221e-01)\tAcc@1  94.53 ( 94.53)\tAcc@5 100.00 (100.00)\n","Epoch: [95][ 30/391]\tTime  0.093 ( 0.099)\tLoss 1.2049e-01 (1.5523e-01)\tAcc@1  94.53 ( 95.67)\tAcc@5 100.00 ( 99.82)\n","Epoch: [95][ 60/391]\tTime  0.090 ( 0.096)\tLoss 1.1276e-01 (1.5429e-01)\tAcc@1  97.66 ( 95.70)\tAcc@5 100.00 ( 99.77)\n","Epoch: [95][ 90/391]\tTime  0.091 ( 0.095)\tLoss 1.3227e-01 (1.5342e-01)\tAcc@1  95.31 ( 95.79)\tAcc@5 100.00 ( 99.79)\n","Epoch: [95][120/391]\tTime  0.091 ( 0.094)\tLoss 8.1987e-02 (1.5012e-01)\tAcc@1  99.22 ( 95.96)\tAcc@5 100.00 ( 99.75)\n","Epoch: [95][150/391]\tTime  0.087 ( 0.094)\tLoss 1.4754e-01 (1.5163e-01)\tAcc@1  94.53 ( 95.87)\tAcc@5  99.22 ( 99.75)\n","Epoch: [95][180/391]\tTime  0.090 ( 0.094)\tLoss 1.3130e-01 (1.4935e-01)\tAcc@1  96.88 ( 95.95)\tAcc@5 100.00 ( 99.75)\n","Epoch: [95][210/391]\tTime  0.095 ( 0.093)\tLoss 2.0279e-01 (1.5198e-01)\tAcc@1  94.53 ( 95.84)\tAcc@5 100.00 ( 99.76)\n","Epoch: [95][240/391]\tTime  0.087 ( 0.093)\tLoss 1.4430e-01 (1.5217e-01)\tAcc@1  95.31 ( 95.80)\tAcc@5  99.22 ( 99.75)\n","Epoch: [95][270/391]\tTime  0.096 ( 0.093)\tLoss 7.1175e-02 (1.5308e-01)\tAcc@1  97.66 ( 95.73)\tAcc@5 100.00 ( 99.76)\n","Epoch: [95][300/391]\tTime  0.094 ( 0.093)\tLoss 1.5259e-01 (1.5234e-01)\tAcc@1  95.31 ( 95.78)\tAcc@5 100.00 ( 99.76)\n","Epoch: [95][330/391]\tTime  0.095 ( 0.093)\tLoss 1.7890e-01 (1.5331e-01)\tAcc@1  94.53 ( 95.76)\tAcc@5 100.00 ( 99.76)\n","Epoch: [95][360/391]\tTime  0.091 ( 0.093)\tLoss 1.9613e-01 (1.5312e-01)\tAcc@1  94.53 ( 95.76)\tAcc@5 100.00 ( 99.76)\n","Epoch: [95][390/391]\tTime  0.082 ( 0.093)\tLoss 1.5691e-01 (1.5330e-01)\tAcc@1  96.25 ( 95.74)\tAcc@5 100.00 ( 99.76)\n","==> Train Accuracy: Acc@1 95.742 || Acc@5 99.758\n","==> Test Accuracy:  Acc@1 76.710 || Acc@5 94.580\n","==> 39.05 seconds to train this epoch\n","\n","\n","----- epoch: 96, lr: 0.004000000000000001 -----\n","Epoch: [96][  0/391]\tTime  0.271 ( 0.271)\tLoss 1.7572e-01 (1.7572e-01)\tAcc@1  96.09 ( 96.09)\tAcc@5  99.22 ( 99.22)\n","Epoch: [96][ 30/391]\tTime  0.089 ( 0.099)\tLoss 1.2295e-01 (1.5134e-01)\tAcc@1  97.66 ( 95.84)\tAcc@5 100.00 ( 99.72)\n","Epoch: [96][ 60/391]\tTime  0.093 ( 0.096)\tLoss 1.3541e-01 (1.4665e-01)\tAcc@1  96.88 ( 96.06)\tAcc@5 100.00 ( 99.76)\n","Epoch: [96][ 90/391]\tTime  0.094 ( 0.095)\tLoss 1.2763e-01 (1.4977e-01)\tAcc@1  96.09 ( 95.94)\tAcc@5  99.22 ( 99.69)\n","Epoch: [96][120/391]\tTime  0.087 ( 0.094)\tLoss 1.5975e-01 (1.4582e-01)\tAcc@1  96.09 ( 96.03)\tAcc@5  99.22 ( 99.74)\n","Epoch: [96][150/391]\tTime  0.093 ( 0.094)\tLoss 8.5567e-02 (1.4555e-01)\tAcc@1  97.66 ( 96.05)\tAcc@5 100.00 ( 99.74)\n","Epoch: [96][180/391]\tTime  0.093 ( 0.093)\tLoss 1.6575e-01 (1.4624e-01)\tAcc@1  94.53 ( 96.02)\tAcc@5  99.22 ( 99.73)\n","Epoch: [96][210/391]\tTime  0.092 ( 0.093)\tLoss 1.5083e-01 (1.4868e-01)\tAcc@1  96.88 ( 95.96)\tAcc@5  98.44 ( 99.72)\n","Epoch: [96][240/391]\tTime  0.094 ( 0.093)\tLoss 1.5084e-01 (1.4931e-01)\tAcc@1  96.09 ( 95.98)\tAcc@5 100.00 ( 99.71)\n","Epoch: [96][270/391]\tTime  0.091 ( 0.093)\tLoss 1.2627e-01 (1.4954e-01)\tAcc@1  97.66 ( 95.93)\tAcc@5 100.00 ( 99.72)\n","Epoch: [96][300/391]\tTime  0.092 ( 0.093)\tLoss 2.3518e-01 (1.4999e-01)\tAcc@1  92.97 ( 95.91)\tAcc@5 100.00 ( 99.72)\n","Epoch: [96][330/391]\tTime  0.093 ( 0.093)\tLoss 2.2655e-01 (1.4987e-01)\tAcc@1  95.31 ( 95.92)\tAcc@5  97.66 ( 99.72)\n","Epoch: [96][360/391]\tTime  0.087 ( 0.093)\tLoss 1.5765e-01 (1.4937e-01)\tAcc@1  95.31 ( 95.93)\tAcc@5 100.00 ( 99.72)\n","Epoch: [96][390/391]\tTime  0.080 ( 0.093)\tLoss 1.4724e-01 (1.5025e-01)\tAcc@1  96.25 ( 95.90)\tAcc@5 100.00 ( 99.72)\n","==> Train Accuracy: Acc@1 95.902 || Acc@5 99.722\n","==> Test Accuracy:  Acc@1 76.700 || Acc@5 94.430\n","==> 38.95 seconds to train this epoch\n","\n","\n","----- epoch: 97, lr: 0.004000000000000001 -----\n","Epoch: [97][  0/391]\tTime  0.267 ( 0.267)\tLoss 1.2340e-01 (1.2340e-01)\tAcc@1  94.53 ( 94.53)\tAcc@5 100.00 (100.00)\n","Epoch: [97][ 30/391]\tTime  0.090 ( 0.099)\tLoss 1.0423e-01 (1.3977e-01)\tAcc@1  96.09 ( 95.99)\tAcc@5 100.00 ( 99.85)\n","Epoch: [97][ 60/391]\tTime  0.105 ( 0.096)\tLoss 1.1858e-01 (1.4445e-01)\tAcc@1  94.53 ( 95.98)\tAcc@5 100.00 ( 99.77)\n","Epoch: [97][ 90/391]\tTime  0.090 ( 0.095)\tLoss 1.4579e-01 (1.4296e-01)\tAcc@1  95.31 ( 96.14)\tAcc@5 100.00 ( 99.77)\n","Epoch: [97][120/391]\tTime  0.094 ( 0.094)\tLoss 1.4340e-01 (1.4461e-01)\tAcc@1  95.31 ( 96.13)\tAcc@5 100.00 ( 99.76)\n","Epoch: [97][150/391]\tTime  0.092 ( 0.094)\tLoss 1.5551e-01 (1.4170e-01)\tAcc@1  96.09 ( 96.24)\tAcc@5 100.00 ( 99.76)\n","Epoch: [97][180/391]\tTime  0.092 ( 0.093)\tLoss 8.8044e-02 (1.4385e-01)\tAcc@1  98.44 ( 96.17)\tAcc@5 100.00 ( 99.74)\n","Epoch: [97][210/391]\tTime  0.091 ( 0.093)\tLoss 1.6935e-01 (1.4472e-01)\tAcc@1  93.75 ( 96.12)\tAcc@5 100.00 ( 99.73)\n","Epoch: [97][240/391]\tTime  0.092 ( 0.093)\tLoss 1.2510e-01 (1.4487e-01)\tAcc@1  97.66 ( 96.12)\tAcc@5  99.22 ( 99.71)\n","Epoch: [97][270/391]\tTime  0.100 ( 0.093)\tLoss 1.7485e-01 (1.4546e-01)\tAcc@1  95.31 ( 96.05)\tAcc@5 100.00 ( 99.72)\n","Epoch: [97][300/391]\tTime  0.091 ( 0.093)\tLoss 7.6498e-02 (1.4388e-01)\tAcc@1  98.44 ( 96.08)\tAcc@5 100.00 ( 99.72)\n","Epoch: [97][330/391]\tTime  0.092 ( 0.093)\tLoss 2.0467e-01 (1.4524e-01)\tAcc@1  94.53 ( 96.03)\tAcc@5  99.22 ( 99.71)\n","Epoch: [97][360/391]\tTime  0.092 ( 0.093)\tLoss 1.2375e-01 (1.4572e-01)\tAcc@1  96.09 ( 96.01)\tAcc@5  99.22 ( 99.71)\n","Epoch: [97][390/391]\tTime  0.082 ( 0.093)\tLoss 1.3966e-01 (1.4612e-01)\tAcc@1  96.25 ( 95.99)\tAcc@5  98.75 ( 99.71)\n","==> Train Accuracy: Acc@1 95.986 || Acc@5 99.710\n","==> Test Accuracy:  Acc@1 76.980 || Acc@5 94.500\n","==> 39.07 seconds to train this epoch\n","\n","\n","----- epoch: 98, lr: 0.004000000000000001 -----\n","Epoch: [98][  0/391]\tTime  0.266 ( 0.266)\tLoss 1.3692e-01 (1.3692e-01)\tAcc@1  96.09 ( 96.09)\tAcc@5 100.00 (100.00)\n","Epoch: [98][ 30/391]\tTime  0.091 ( 0.100)\tLoss 1.2413e-01 (1.3648e-01)\tAcc@1  96.09 ( 96.42)\tAcc@5 100.00 ( 99.77)\n","Epoch: [98][ 60/391]\tTime  0.092 ( 0.096)\tLoss 1.2501e-01 (1.3250e-01)\tAcc@1  96.09 ( 96.40)\tAcc@5 100.00 ( 99.81)\n","Epoch: [98][ 90/391]\tTime  0.090 ( 0.095)\tLoss 8.9517e-02 (1.2922e-01)\tAcc@1  98.44 ( 96.48)\tAcc@5 100.00 ( 99.79)\n","Epoch: [98][120/391]\tTime  0.098 ( 0.094)\tLoss 1.7115e-01 (1.3231e-01)\tAcc@1  94.53 ( 96.41)\tAcc@5 100.00 ( 99.77)\n","Epoch: [98][150/391]\tTime  0.090 ( 0.094)\tLoss 1.3619e-01 (1.3351e-01)\tAcc@1  95.31 ( 96.33)\tAcc@5 100.00 ( 99.80)\n","Epoch: [98][180/391]\tTime  0.091 ( 0.094)\tLoss 1.1565e-01 (1.3417e-01)\tAcc@1  96.09 ( 96.31)\tAcc@5 100.00 ( 99.79)\n","Epoch: [98][210/391]\tTime  0.087 ( 0.093)\tLoss 1.6456e-01 (1.3543e-01)\tAcc@1  96.88 ( 96.28)\tAcc@5  98.44 ( 99.78)\n","Epoch: [98][240/391]\tTime  0.096 ( 0.093)\tLoss 2.2362e-01 (1.3653e-01)\tAcc@1  93.75 ( 96.31)\tAcc@5 100.00 ( 99.77)\n","Epoch: [98][270/391]\tTime  0.093 ( 0.093)\tLoss 1.2521e-01 (1.3620e-01)\tAcc@1  96.09 ( 96.34)\tAcc@5 100.00 ( 99.76)\n","Epoch: [98][300/391]\tTime  0.091 ( 0.093)\tLoss 1.3626e-01 (1.3572e-01)\tAcc@1  95.31 ( 96.35)\tAcc@5  99.22 ( 99.77)\n","Epoch: [98][330/391]\tTime  0.092 ( 0.093)\tLoss 1.5166e-01 (1.3656e-01)\tAcc@1  96.88 ( 96.32)\tAcc@5 100.00 ( 99.77)\n","Epoch: [98][360/391]\tTime  0.088 ( 0.093)\tLoss 1.7033e-01 (1.3678e-01)\tAcc@1  93.75 ( 96.28)\tAcc@5 100.00 ( 99.78)\n","Epoch: [98][390/391]\tTime  0.082 ( 0.093)\tLoss 1.1833e-01 (1.3725e-01)\tAcc@1  96.25 ( 96.26)\tAcc@5 100.00 ( 99.78)\n","==> Train Accuracy: Acc@1 96.264 || Acc@5 99.782\n","==> Test Accuracy:  Acc@1 76.830 || Acc@5 94.530\n","==> 39.00 seconds to train this epoch\n","\n","\n","----- epoch: 99, lr: 0.004000000000000001 -----\n","Epoch: [99][  0/391]\tTime  0.295 ( 0.295)\tLoss 1.1621e-01 (1.1621e-01)\tAcc@1  96.88 ( 96.88)\tAcc@5 100.00 (100.00)\n","Epoch: [99][ 30/391]\tTime  0.089 ( 0.100)\tLoss 1.2408e-01 (1.4414e-01)\tAcc@1  96.09 ( 95.82)\tAcc@5 100.00 ( 99.77)\n","Epoch: [99][ 60/391]\tTime  0.096 ( 0.096)\tLoss 6.8024e-02 (1.3689e-01)\tAcc@1  99.22 ( 96.11)\tAcc@5 100.00 ( 99.78)\n","Epoch: [99][ 90/391]\tTime  0.090 ( 0.095)\tLoss 1.1065e-01 (1.3883e-01)\tAcc@1  96.88 ( 96.06)\tAcc@5 100.00 ( 99.76)\n","Epoch: [99][120/391]\tTime  0.102 ( 0.094)\tLoss 1.5729e-01 (1.3939e-01)\tAcc@1  92.97 ( 95.93)\tAcc@5 100.00 ( 99.74)\n","Epoch: [99][150/391]\tTime  0.092 ( 0.094)\tLoss 1.2022e-01 (1.3839e-01)\tAcc@1  96.09 ( 96.01)\tAcc@5 100.00 ( 99.76)\n","Epoch: [99][180/391]\tTime  0.090 ( 0.094)\tLoss 1.0208e-01 (1.3736e-01)\tAcc@1  97.66 ( 96.08)\tAcc@5 100.00 ( 99.74)\n","Epoch: [99][210/391]\tTime  0.091 ( 0.094)\tLoss 1.2558e-01 (1.3773e-01)\tAcc@1  97.66 ( 96.08)\tAcc@5  99.22 ( 99.74)\n","Epoch: [99][240/391]\tTime  0.094 ( 0.093)\tLoss 1.6080e-01 (1.3755e-01)\tAcc@1  96.09 ( 96.11)\tAcc@5  99.22 ( 99.72)\n","Epoch: [99][270/391]\tTime  0.082 ( 0.093)\tLoss 1.0394e-01 (1.3736e-01)\tAcc@1  98.44 ( 96.14)\tAcc@5 100.00 ( 99.73)\n","Epoch: [99][300/391]\tTime  0.091 ( 0.093)\tLoss 1.7599e-01 (1.3732e-01)\tAcc@1  94.53 ( 96.16)\tAcc@5 100.00 ( 99.72)\n","Epoch: [99][330/391]\tTime  0.088 ( 0.093)\tLoss 6.8516e-02 (1.3720e-01)\tAcc@1  98.44 ( 96.16)\tAcc@5 100.00 ( 99.73)\n","Epoch: [99][360/391]\tTime  0.092 ( 0.093)\tLoss 1.4256e-01 (1.3800e-01)\tAcc@1  97.66 ( 96.17)\tAcc@5 100.00 ( 99.73)\n","Epoch: [99][390/391]\tTime  0.083 ( 0.093)\tLoss 8.1804e-02 (1.3768e-01)\tAcc@1  98.75 ( 96.20)\tAcc@5 100.00 ( 99.73)\n","==> Train Accuracy: Acc@1 96.200 || Acc@5 99.728\n","==> Test Accuracy:  Acc@1 77.320 || Acc@5 94.290\n","==> 39.06 seconds to train this epoch\n","\n","\n","----- epoch: 100, lr: 0.004000000000000001 -----\n","Epoch: [100][  0/391]\tTime  0.304 ( 0.304)\tLoss 7.6621e-02 (7.6621e-02)\tAcc@1  98.44 ( 98.44)\tAcc@5 100.00 (100.00)\n","Epoch: [100][ 30/391]\tTime  0.092 ( 0.101)\tLoss 2.0527e-01 (1.2604e-01)\tAcc@1  94.53 ( 96.50)\tAcc@5 100.00 ( 99.87)\n","Epoch: [100][ 60/391]\tTime  0.092 ( 0.097)\tLoss 1.2431e-01 (1.3077e-01)\tAcc@1  96.88 ( 96.27)\tAcc@5  99.22 ( 99.83)\n","Epoch: [100][ 90/391]\tTime  0.092 ( 0.095)\tLoss 1.2738e-01 (1.3042e-01)\tAcc@1  96.88 ( 96.33)\tAcc@5 100.00 ( 99.83)\n","Epoch: [100][120/391]\tTime  0.091 ( 0.095)\tLoss 7.8843e-02 (1.2674e-01)\tAcc@1  98.44 ( 96.51)\tAcc@5 100.00 ( 99.84)\n","Epoch: [100][150/391]\tTime  0.095 ( 0.094)\tLoss 7.1893e-02 (1.2885e-01)\tAcc@1  98.44 ( 96.44)\tAcc@5 100.00 ( 99.83)\n","Epoch: [100][180/391]\tTime  0.095 ( 0.094)\tLoss 1.0714e-01 (1.2896e-01)\tAcc@1  97.66 ( 96.43)\tAcc@5  99.22 ( 99.83)\n","Epoch: [100][210/391]\tTime  0.087 ( 0.094)\tLoss 8.1088e-02 (1.2951e-01)\tAcc@1  97.66 ( 96.40)\tAcc@5 100.00 ( 99.82)\n","Epoch: [100][240/391]\tTime  0.092 ( 0.094)\tLoss 1.2274e-01 (1.2926e-01)\tAcc@1  96.09 ( 96.43)\tAcc@5 100.00 ( 99.82)\n","Epoch: [100][270/391]\tTime  0.094 ( 0.093)\tLoss 9.4667e-02 (1.2909e-01)\tAcc@1  96.88 ( 96.45)\tAcc@5 100.00 ( 99.83)\n","Epoch: [100][300/391]\tTime  0.092 ( 0.093)\tLoss 1.0299e-01 (1.2949e-01)\tAcc@1  97.66 ( 96.43)\tAcc@5 100.00 ( 99.82)\n","Epoch: [100][330/391]\tTime  0.092 ( 0.093)\tLoss 8.1138e-02 (1.2927e-01)\tAcc@1  98.44 ( 96.45)\tAcc@5 100.00 ( 99.81)\n","Epoch: [100][360/391]\tTime  0.089 ( 0.093)\tLoss 1.0757e-01 (1.2949e-01)\tAcc@1  96.09 ( 96.43)\tAcc@5 100.00 ( 99.80)\n","Epoch: [100][390/391]\tTime  0.082 ( 0.093)\tLoss 1.1698e-01 (1.2871e-01)\tAcc@1  97.50 ( 96.48)\tAcc@5 100.00 ( 99.80)\n","==> Train Accuracy: Acc@1 96.480 || Acc@5 99.800\n","==> Test Accuracy:  Acc@1 77.090 || Acc@5 94.150\n","==> 39.16 seconds to train this epoch\n","\n","\n","----- epoch: 101, lr: 0.004000000000000001 -----\n","Epoch: [101][  0/391]\tTime  0.299 ( 0.299)\tLoss 1.1215e-01 (1.1215e-01)\tAcc@1  96.88 ( 96.88)\tAcc@5 100.00 (100.00)\n","Epoch: [101][ 30/391]\tTime  0.094 ( 0.100)\tLoss 1.1299e-01 (1.3113e-01)\tAcc@1  96.88 ( 96.45)\tAcc@5  99.22 ( 99.77)\n","Epoch: [101][ 60/391]\tTime  0.094 ( 0.096)\tLoss 1.1663e-01 (1.3094e-01)\tAcc@1  96.88 ( 96.41)\tAcc@5 100.00 ( 99.73)\n","Epoch: [101][ 90/391]\tTime  0.089 ( 0.095)\tLoss 1.5349e-01 (1.3761e-01)\tAcc@1  96.88 ( 96.24)\tAcc@5  99.22 ( 99.67)\n","Epoch: [101][120/391]\tTime  0.088 ( 0.094)\tLoss 1.3413e-01 (1.3646e-01)\tAcc@1  94.53 ( 96.26)\tAcc@5 100.00 ( 99.68)\n","Epoch: [101][150/391]\tTime  0.090 ( 0.094)\tLoss 9.9527e-02 (1.3292e-01)\tAcc@1  94.53 ( 96.36)\tAcc@5 100.00 ( 99.70)\n","Epoch: [101][180/391]\tTime  0.090 ( 0.094)\tLoss 1.1646e-01 (1.3226e-01)\tAcc@1  96.88 ( 96.37)\tAcc@5 100.00 ( 99.68)\n","Epoch: [101][210/391]\tTime  0.090 ( 0.093)\tLoss 1.0673e-01 (1.2989e-01)\tAcc@1  96.88 ( 96.46)\tAcc@5 100.00 ( 99.70)\n","Epoch: [101][240/391]\tTime  0.091 ( 0.093)\tLoss 1.1067e-01 (1.2792e-01)\tAcc@1  97.66 ( 96.51)\tAcc@5 100.00 ( 99.71)\n","Epoch: [101][270/391]\tTime  0.092 ( 0.093)\tLoss 1.2665e-01 (1.2862e-01)\tAcc@1  95.31 ( 96.49)\tAcc@5 100.00 ( 99.71)\n","Epoch: [101][300/391]\tTime  0.092 ( 0.093)\tLoss 1.0899e-01 (1.2868e-01)\tAcc@1  97.66 ( 96.48)\tAcc@5  99.22 ( 99.71)\n","Epoch: [101][330/391]\tTime  0.092 ( 0.093)\tLoss 1.6366e-01 (1.2852e-01)\tAcc@1  95.31 ( 96.50)\tAcc@5 100.00 ( 99.72)\n","Epoch: [101][360/391]\tTime  0.093 ( 0.093)\tLoss 1.1711e-01 (1.2676e-01)\tAcc@1  95.31 ( 96.55)\tAcc@5 100.00 ( 99.74)\n","Epoch: [101][390/391]\tTime  0.082 ( 0.093)\tLoss 1.2913e-01 (1.2684e-01)\tAcc@1  96.25 ( 96.54)\tAcc@5 100.00 ( 99.75)\n","==> Train Accuracy: Acc@1 96.538 || Acc@5 99.746\n","==> Test Accuracy:  Acc@1 76.960 || Acc@5 94.230\n","==> 39.08 seconds to train this epoch\n","\n","\n","----- epoch: 102, lr: 0.004000000000000001 -----\n","Epoch: [102][  0/391]\tTime  0.276 ( 0.276)\tLoss 8.6493e-02 (8.6493e-02)\tAcc@1  96.88 ( 96.88)\tAcc@5 100.00 (100.00)\n","Epoch: [102][ 30/391]\tTime  0.097 ( 0.100)\tLoss 1.0721e-01 (1.2058e-01)\tAcc@1  97.66 ( 96.67)\tAcc@5 100.00 ( 99.72)\n","Epoch: [102][ 60/391]\tTime  0.107 ( 0.096)\tLoss 1.3507e-01 (1.2092e-01)\tAcc@1  96.88 ( 96.89)\tAcc@5 100.00 ( 99.80)\n","Epoch: [102][ 90/391]\tTime  0.087 ( 0.095)\tLoss 7.6111e-02 (1.1851e-01)\tAcc@1  98.44 ( 96.94)\tAcc@5 100.00 ( 99.81)\n","Epoch: [102][120/391]\tTime  0.092 ( 0.094)\tLoss 2.0140e-01 (1.1746e-01)\tAcc@1  95.31 ( 96.97)\tAcc@5  99.22 ( 99.83)\n","Epoch: [102][150/391]\tTime  0.091 ( 0.094)\tLoss 9.1169e-02 (1.2020e-01)\tAcc@1  97.66 ( 96.87)\tAcc@5 100.00 ( 99.81)\n","Epoch: [102][180/391]\tTime  0.093 ( 0.094)\tLoss 1.1760e-01 (1.2072e-01)\tAcc@1  95.31 ( 96.86)\tAcc@5 100.00 ( 99.82)\n","Epoch: [102][210/391]\tTime  0.090 ( 0.094)\tLoss 8.8768e-02 (1.2208e-01)\tAcc@1  96.88 ( 96.76)\tAcc@5 100.00 ( 99.82)\n","Epoch: [102][240/391]\tTime  0.090 ( 0.093)\tLoss 1.2712e-01 (1.2235e-01)\tAcc@1  96.09 ( 96.74)\tAcc@5 100.00 ( 99.82)\n","Epoch: [102][270/391]\tTime  0.090 ( 0.093)\tLoss 1.0891e-01 (1.2356e-01)\tAcc@1  96.88 ( 96.72)\tAcc@5 100.00 ( 99.82)\n","Epoch: [102][300/391]\tTime  0.091 ( 0.093)\tLoss 3.9634e-02 (1.2214e-01)\tAcc@1  99.22 ( 96.76)\tAcc@5 100.00 ( 99.83)\n","Epoch: [102][330/391]\tTime  0.089 ( 0.093)\tLoss 1.1973e-01 (1.2337e-01)\tAcc@1  96.88 ( 96.72)\tAcc@5  99.22 ( 99.81)\n","Epoch: [102][360/391]\tTime  0.089 ( 0.093)\tLoss 1.4986e-01 (1.2299e-01)\tAcc@1  95.31 ( 96.73)\tAcc@5 100.00 ( 99.81)\n","Epoch: [102][390/391]\tTime  0.082 ( 0.093)\tLoss 1.2540e-01 (1.2281e-01)\tAcc@1  96.25 ( 96.74)\tAcc@5 100.00 ( 99.81)\n","==> Train Accuracy: Acc@1 96.744 || Acc@5 99.810\n","==> Test Accuracy:  Acc@1 76.880 || Acc@5 94.360\n","==> 39.10 seconds to train this epoch\n","\n","\n","----- epoch: 103, lr: 0.004000000000000001 -----\n","Epoch: [103][  0/391]\tTime  0.267 ( 0.267)\tLoss 1.9361e-01 (1.9361e-01)\tAcc@1  94.53 ( 94.53)\tAcc@5  98.44 ( 98.44)\n","Epoch: [103][ 30/391]\tTime  0.094 ( 0.100)\tLoss 9.2487e-02 (1.1022e-01)\tAcc@1  98.44 ( 97.25)\tAcc@5  99.22 ( 99.80)\n","Epoch: [103][ 60/391]\tTime  0.092 ( 0.096)\tLoss 1.3381e-01 (1.1784e-01)\tAcc@1  96.09 ( 96.86)\tAcc@5 100.00 ( 99.82)\n","Epoch: [103][ 90/391]\tTime  0.100 ( 0.095)\tLoss 5.8418e-02 (1.1597e-01)\tAcc@1 100.00 ( 96.97)\tAcc@5 100.00 ( 99.79)\n","Epoch: [103][120/391]\tTime  0.092 ( 0.094)\tLoss 8.5013e-02 (1.1379e-01)\tAcc@1  98.44 ( 97.00)\tAcc@5  99.22 ( 99.79)\n","Epoch: [103][150/391]\tTime  0.101 ( 0.094)\tLoss 1.5772e-01 (1.1423e-01)\tAcc@1  95.31 ( 97.00)\tAcc@5  99.22 ( 99.77)\n","Epoch: [103][180/391]\tTime  0.090 ( 0.094)\tLoss 1.1752e-01 (1.1522e-01)\tAcc@1  96.88 ( 96.92)\tAcc@5 100.00 ( 99.77)\n","Epoch: [103][210/391]\tTime  0.089 ( 0.094)\tLoss 1.1245e-01 (1.1547e-01)\tAcc@1  96.88 ( 96.88)\tAcc@5 100.00 ( 99.79)\n","Epoch: [103][240/391]\tTime  0.090 ( 0.093)\tLoss 1.0779e-01 (1.1639e-01)\tAcc@1  98.44 ( 96.86)\tAcc@5  99.22 ( 99.79)\n","Epoch: [103][270/391]\tTime  0.089 ( 0.093)\tLoss 9.4542e-02 (1.1626e-01)\tAcc@1  96.88 ( 96.85)\tAcc@5 100.00 ( 99.80)\n","Epoch: [103][300/391]\tTime  0.093 ( 0.093)\tLoss 1.5709e-01 (1.1687e-01)\tAcc@1  97.66 ( 96.84)\tAcc@5  99.22 ( 99.79)\n","Epoch: [103][330/391]\tTime  0.090 ( 0.093)\tLoss 1.5504e-01 (1.1721e-01)\tAcc@1  94.53 ( 96.83)\tAcc@5 100.00 ( 99.80)\n","Epoch: [103][360/391]\tTime  0.092 ( 0.093)\tLoss 1.8832e-01 (1.1749e-01)\tAcc@1  95.31 ( 96.82)\tAcc@5 100.00 ( 99.81)\n","Epoch: [103][390/391]\tTime  0.082 ( 0.093)\tLoss 9.8068e-02 (1.1699e-01)\tAcc@1  96.25 ( 96.83)\tAcc@5 100.00 ( 99.80)\n","==> Train Accuracy: Acc@1 96.830 || Acc@5 99.804\n","==> Test Accuracy:  Acc@1 77.330 || Acc@5 94.280\n","==> 39.04 seconds to train this epoch\n","\n","\n","----- epoch: 104, lr: 0.004000000000000001 -----\n","Epoch: [104][  0/391]\tTime  0.271 ( 0.271)\tLoss 1.2638e-01 (1.2638e-01)\tAcc@1  96.88 ( 96.88)\tAcc@5 100.00 (100.00)\n","Epoch: [104][ 30/391]\tTime  0.096 ( 0.100)\tLoss 6.8096e-02 (1.1077e-01)\tAcc@1 100.00 ( 97.35)\tAcc@5 100.00 ( 99.72)\n","Epoch: [104][ 60/391]\tTime  0.091 ( 0.096)\tLoss 7.3754e-02 (1.1254e-01)\tAcc@1  99.22 ( 97.07)\tAcc@5  99.22 ( 99.77)\n","Epoch: [104][ 90/391]\tTime  0.092 ( 0.095)\tLoss 6.9628e-02 (1.1117e-01)\tAcc@1  97.66 ( 97.08)\tAcc@5 100.00 ( 99.79)\n","Epoch: [104][120/391]\tTime  0.091 ( 0.094)\tLoss 1.1763e-01 (1.1233e-01)\tAcc@1  95.31 ( 97.07)\tAcc@5 100.00 ( 99.79)\n","Epoch: [104][150/391]\tTime  0.087 ( 0.094)\tLoss 1.2188e-01 (1.1053e-01)\tAcc@1  96.88 ( 97.07)\tAcc@5 100.00 ( 99.81)\n","Epoch: [104][180/391]\tTime  0.079 ( 0.094)\tLoss 7.2264e-02 (1.1040e-01)\tAcc@1  96.88 ( 97.10)\tAcc@5 100.00 ( 99.81)\n","Epoch: [104][210/391]\tTime  0.092 ( 0.093)\tLoss 5.7902e-02 (1.1031e-01)\tAcc@1  99.22 ( 97.09)\tAcc@5 100.00 ( 99.80)\n","Epoch: [104][240/391]\tTime  0.092 ( 0.093)\tLoss 9.2852e-02 (1.1288e-01)\tAcc@1  97.66 ( 97.00)\tAcc@5 100.00 ( 99.80)\n","Epoch: [104][270/391]\tTime  0.095 ( 0.093)\tLoss 1.0185e-01 (1.1313e-01)\tAcc@1  97.66 ( 97.00)\tAcc@5 100.00 ( 99.80)\n","Epoch: [104][300/391]\tTime  0.092 ( 0.093)\tLoss 7.0045e-02 (1.1260e-01)\tAcc@1  99.22 ( 97.00)\tAcc@5 100.00 ( 99.81)\n","Epoch: [104][330/391]\tTime  0.088 ( 0.093)\tLoss 1.2753e-01 (1.1309e-01)\tAcc@1  96.09 ( 97.00)\tAcc@5  99.22 ( 99.81)\n","Epoch: [104][360/391]\tTime  0.086 ( 0.093)\tLoss 1.2849e-01 (1.1273e-01)\tAcc@1  96.88 ( 97.01)\tAcc@5 100.00 ( 99.81)\n","Epoch: [104][390/391]\tTime  0.082 ( 0.093)\tLoss 9.8358e-02 (1.1280e-01)\tAcc@1  97.50 ( 96.99)\tAcc@5 100.00 ( 99.81)\n","==> Train Accuracy: Acc@1 96.986 || Acc@5 99.812\n","==> Test Accuracy:  Acc@1 76.850 || Acc@5 94.530\n","==> 39.03 seconds to train this epoch\n","\n","\n","----- epoch: 105, lr: 0.004000000000000001 -----\n","Epoch: [105][  0/391]\tTime  0.245 ( 0.245)\tLoss 1.5342e-01 (1.5342e-01)\tAcc@1  94.53 ( 94.53)\tAcc@5  99.22 ( 99.22)\n","Epoch: [105][ 30/391]\tTime  0.091 ( 0.099)\tLoss 1.8378e-01 (1.0055e-01)\tAcc@1  95.31 ( 97.40)\tAcc@5  99.22 ( 99.90)\n","Epoch: [105][ 60/391]\tTime  0.090 ( 0.095)\tLoss 4.5795e-02 (1.0762e-01)\tAcc@1  99.22 ( 97.25)\tAcc@5 100.00 ( 99.83)\n","Epoch: [105][ 90/391]\tTime  0.094 ( 0.094)\tLoss 1.0187e-01 (1.0648e-01)\tAcc@1  97.66 ( 97.24)\tAcc@5 100.00 ( 99.85)\n","Epoch: [105][120/391]\tTime  0.091 ( 0.094)\tLoss 9.8380e-02 (1.0671e-01)\tAcc@1  97.66 ( 97.26)\tAcc@5 100.00 ( 99.85)\n","Epoch: [105][150/391]\tTime  0.092 ( 0.093)\tLoss 1.1224e-01 (1.0743e-01)\tAcc@1  97.66 ( 97.26)\tAcc@5 100.00 ( 99.83)\n","Epoch: [105][180/391]\tTime  0.096 ( 0.093)\tLoss 1.0663e-01 (1.0809e-01)\tAcc@1  96.09 ( 97.22)\tAcc@5 100.00 ( 99.83)\n","Epoch: [105][210/391]\tTime  0.101 ( 0.093)\tLoss 8.2054e-02 (1.1122e-01)\tAcc@1  96.09 ( 97.07)\tAcc@5 100.00 ( 99.82)\n","Epoch: [105][240/391]\tTime  0.092 ( 0.093)\tLoss 1.1737e-01 (1.1105e-01)\tAcc@1  95.31 ( 97.06)\tAcc@5 100.00 ( 99.83)\n","Epoch: [105][270/391]\tTime  0.089 ( 0.093)\tLoss 8.1437e-02 (1.1122e-01)\tAcc@1  98.44 ( 97.05)\tAcc@5 100.00 ( 99.82)\n","Epoch: [105][300/391]\tTime  0.095 ( 0.093)\tLoss 9.5512e-02 (1.1206e-01)\tAcc@1  97.66 ( 97.01)\tAcc@5 100.00 ( 99.82)\n","Epoch: [105][330/391]\tTime  0.095 ( 0.093)\tLoss 8.6936e-02 (1.1337e-01)\tAcc@1  96.88 ( 96.97)\tAcc@5 100.00 ( 99.83)\n","Epoch: [105][360/391]\tTime  0.093 ( 0.093)\tLoss 1.1641e-01 (1.1365e-01)\tAcc@1  96.88 ( 96.97)\tAcc@5 100.00 ( 99.82)\n","Epoch: [105][390/391]\tTime  0.082 ( 0.093)\tLoss 8.5940e-02 (1.1373e-01)\tAcc@1  97.50 ( 96.98)\tAcc@5 100.00 ( 99.83)\n","==> Train Accuracy: Acc@1 96.976 || Acc@5 99.826\n","==> Test Accuracy:  Acc@1 77.570 || Acc@5 94.370\n","==> 38.93 seconds to train this epoch\n","\n","\n","----- epoch: 106, lr: 0.004000000000000001 -----\n","Epoch: [106][  0/391]\tTime  0.283 ( 0.283)\tLoss 7.8026e-02 (7.8026e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5 100.00 (100.00)\n","Epoch: [106][ 30/391]\tTime  0.088 ( 0.100)\tLoss 7.7761e-02 (1.1533e-01)\tAcc@1  99.22 ( 97.03)\tAcc@5 100.00 ( 99.75)\n","Epoch: [106][ 60/391]\tTime  0.092 ( 0.096)\tLoss 1.7675e-01 (1.0992e-01)\tAcc@1  93.75 ( 97.23)\tAcc@5  99.22 ( 99.76)\n","Epoch: [106][ 90/391]\tTime  0.087 ( 0.095)\tLoss 9.6588e-02 (1.1078e-01)\tAcc@1  97.66 ( 97.10)\tAcc@5 100.00 ( 99.73)\n","Epoch: [106][120/391]\tTime  0.092 ( 0.094)\tLoss 2.6775e-01 (1.1001e-01)\tAcc@1  92.97 ( 97.08)\tAcc@5  98.44 ( 99.77)\n","Epoch: [106][150/391]\tTime  0.092 ( 0.094)\tLoss 1.3973e-01 (1.1108e-01)\tAcc@1  96.09 ( 96.99)\tAcc@5 100.00 ( 99.78)\n","Epoch: [106][180/391]\tTime  0.089 ( 0.093)\tLoss 6.4597e-02 (1.0966e-01)\tAcc@1  97.66 ( 97.07)\tAcc@5 100.00 ( 99.79)\n","Epoch: [106][210/391]\tTime  0.090 ( 0.093)\tLoss 1.5116e-01 (1.1036e-01)\tAcc@1  96.09 ( 97.02)\tAcc@5 100.00 ( 99.80)\n","Epoch: [106][240/391]\tTime  0.092 ( 0.093)\tLoss 8.0637e-02 (1.1007e-01)\tAcc@1  97.66 ( 97.00)\tAcc@5 100.00 ( 99.81)\n","Epoch: [106][270/391]\tTime  0.095 ( 0.093)\tLoss 1.4654e-01 (1.0878e-01)\tAcc@1  94.53 ( 97.05)\tAcc@5 100.00 ( 99.81)\n","Epoch: [106][300/391]\tTime  0.092 ( 0.093)\tLoss 6.7159e-02 (1.0970e-01)\tAcc@1  98.44 ( 97.03)\tAcc@5 100.00 ( 99.80)\n","Epoch: [106][330/391]\tTime  0.092 ( 0.093)\tLoss 9.0088e-02 (1.0997e-01)\tAcc@1  97.66 ( 97.03)\tAcc@5 100.00 ( 99.81)\n","Epoch: [106][360/391]\tTime  0.091 ( 0.093)\tLoss 9.6384e-02 (1.0947e-01)\tAcc@1  97.66 ( 97.04)\tAcc@5 100.00 ( 99.82)\n","Epoch: [106][390/391]\tTime  0.083 ( 0.093)\tLoss 9.5077e-02 (1.0954e-01)\tAcc@1  96.25 ( 97.05)\tAcc@5 100.00 ( 99.82)\n","==> Train Accuracy: Acc@1 97.052 || Acc@5 99.818\n","==> Test Accuracy:  Acc@1 76.600 || Acc@5 94.230\n","==> 38.90 seconds to train this epoch\n","\n","\n","----- epoch: 107, lr: 0.004000000000000001 -----\n","Epoch: [107][  0/391]\tTime  0.270 ( 0.270)\tLoss 1.1744e-01 (1.1744e-01)\tAcc@1  97.66 ( 97.66)\tAcc@5 100.00 (100.00)\n","Epoch: [107][ 30/391]\tTime  0.091 ( 0.098)\tLoss 9.6566e-02 (9.8385e-02)\tAcc@1  96.88 ( 97.08)\tAcc@5 100.00 ( 99.87)\n","Epoch: [107][ 60/391]\tTime  0.091 ( 0.096)\tLoss 1.0522e-01 (9.9519e-02)\tAcc@1  96.09 ( 97.14)\tAcc@5 100.00 ( 99.81)\n","Epoch: [107][ 90/391]\tTime  0.095 ( 0.094)\tLoss 8.9465e-02 (1.0240e-01)\tAcc@1  97.66 ( 97.08)\tAcc@5 100.00 ( 99.81)\n","Epoch: [107][120/391]\tTime  0.092 ( 0.094)\tLoss 9.9582e-02 (1.0601e-01)\tAcc@1  97.66 ( 97.02)\tAcc@5 100.00 ( 99.79)\n","Epoch: [107][150/391]\tTime  0.090 ( 0.093)\tLoss 6.4957e-02 (1.0571e-01)\tAcc@1  99.22 ( 97.05)\tAcc@5  99.22 ( 99.80)\n","Epoch: [107][180/391]\tTime  0.092 ( 0.093)\tLoss 5.2807e-02 (1.0458e-01)\tAcc@1  97.66 ( 97.14)\tAcc@5 100.00 ( 99.81)\n","Epoch: [107][210/391]\tTime  0.091 ( 0.093)\tLoss 1.5618e-01 (1.0427e-01)\tAcc@1  96.09 ( 97.16)\tAcc@5  99.22 ( 99.80)\n","Epoch: [107][240/391]\tTime  0.095 ( 0.093)\tLoss 1.5420e-01 (1.0359e-01)\tAcc@1  96.09 ( 97.21)\tAcc@5  99.22 ( 99.81)\n","Epoch: [107][270/391]\tTime  0.090 ( 0.093)\tLoss 1.3552e-01 (1.0387e-01)\tAcc@1  96.09 ( 97.14)\tAcc@5 100.00 ( 99.83)\n","Epoch: [107][300/391]\tTime  0.093 ( 0.093)\tLoss 6.3216e-02 (1.0478e-01)\tAcc@1 100.00 ( 97.13)\tAcc@5 100.00 ( 99.82)\n","Epoch: [107][330/391]\tTime  0.093 ( 0.093)\tLoss 8.6177e-02 (1.0551e-01)\tAcc@1  98.44 ( 97.13)\tAcc@5 100.00 ( 99.82)\n","Epoch: [107][360/391]\tTime  0.078 ( 0.093)\tLoss 1.1102e-01 (1.0634e-01)\tAcc@1  96.88 ( 97.11)\tAcc@5 100.00 ( 99.81)\n","Epoch: [107][390/391]\tTime  0.082 ( 0.093)\tLoss 8.6768e-02 (1.0611e-01)\tAcc@1  97.50 ( 97.11)\tAcc@5 100.00 ( 99.82)\n","==> Train Accuracy: Acc@1 97.106 || Acc@5 99.820\n","==> Test Accuracy:  Acc@1 77.070 || Acc@5 94.360\n","==> 38.94 seconds to train this epoch\n","\n","\n","----- epoch: 108, lr: 0.004000000000000001 -----\n","Epoch: [108][  0/391]\tTime  0.270 ( 0.270)\tLoss 9.5033e-02 (9.5033e-02)\tAcc@1  97.66 ( 97.66)\tAcc@5 100.00 (100.00)\n","Epoch: [108][ 30/391]\tTime  0.090 ( 0.098)\tLoss 6.6728e-02 (9.8520e-02)\tAcc@1  98.44 ( 97.30)\tAcc@5 100.00 ( 99.85)\n","Epoch: [108][ 60/391]\tTime  0.094 ( 0.095)\tLoss 1.0689e-01 (1.0002e-01)\tAcc@1  97.66 ( 97.44)\tAcc@5 100.00 ( 99.85)\n","Epoch: [108][ 90/391]\tTime  0.092 ( 0.094)\tLoss 1.1414e-01 (1.0154e-01)\tAcc@1  95.31 ( 97.32)\tAcc@5 100.00 ( 99.85)\n","Epoch: [108][120/391]\tTime  0.095 ( 0.094)\tLoss 6.9284e-02 (9.9105e-02)\tAcc@1  97.66 ( 97.32)\tAcc@5 100.00 ( 99.86)\n","Epoch: [108][150/391]\tTime  0.091 ( 0.093)\tLoss 6.9052e-02 (1.0252e-01)\tAcc@1  98.44 ( 97.23)\tAcc@5 100.00 ( 99.86)\n","Epoch: [108][180/391]\tTime  0.094 ( 0.093)\tLoss 7.3202e-02 (1.0217e-01)\tAcc@1  98.44 ( 97.28)\tAcc@5 100.00 ( 99.85)\n","Epoch: [108][210/391]\tTime  0.092 ( 0.093)\tLoss 1.1635e-01 (1.0289e-01)\tAcc@1  95.31 ( 97.26)\tAcc@5 100.00 ( 99.84)\n","Epoch: [108][240/391]\tTime  0.094 ( 0.093)\tLoss 6.0430e-02 (1.0417e-01)\tAcc@1  98.44 ( 97.24)\tAcc@5  99.22 ( 99.83)\n","Epoch: [108][270/391]\tTime  0.089 ( 0.093)\tLoss 7.1519e-02 (1.0483e-01)\tAcc@1  97.66 ( 97.19)\tAcc@5 100.00 ( 99.84)\n","Epoch: [108][300/391]\tTime  0.093 ( 0.093)\tLoss 1.0046e-01 (1.0499e-01)\tAcc@1  97.66 ( 97.17)\tAcc@5  99.22 ( 99.84)\n","Epoch: [108][330/391]\tTime  0.091 ( 0.093)\tLoss 7.8964e-02 (1.0524e-01)\tAcc@1  98.44 ( 97.16)\tAcc@5 100.00 ( 99.84)\n","Epoch: [108][360/391]\tTime  0.091 ( 0.093)\tLoss 1.3292e-01 (1.0510e-01)\tAcc@1  97.66 ( 97.16)\tAcc@5  99.22 ( 99.85)\n","Epoch: [108][390/391]\tTime  0.081 ( 0.093)\tLoss 6.2072e-02 (1.0560e-01)\tAcc@1 100.00 ( 97.16)\tAcc@5 100.00 ( 99.86)\n","==> Train Accuracy: Acc@1 97.162 || Acc@5 99.856\n","==> Test Accuracy:  Acc@1 76.950 || Acc@5 94.100\n","==> 38.93 seconds to train this epoch\n","\n","\n","----- epoch: 109, lr: 0.004000000000000001 -----\n","Epoch: [109][  0/391]\tTime  0.263 ( 0.263)\tLoss 1.5484e-01 (1.5484e-01)\tAcc@1  97.66 ( 97.66)\tAcc@5 100.00 (100.00)\n","Epoch: [109][ 30/391]\tTime  0.091 ( 0.099)\tLoss 1.2241e-01 (1.0899e-01)\tAcc@1  96.88 ( 96.88)\tAcc@5  99.22 ( 99.90)\n","Epoch: [109][ 60/391]\tTime  0.090 ( 0.096)\tLoss 7.6597e-02 (1.0361e-01)\tAcc@1  97.66 ( 97.13)\tAcc@5 100.00 ( 99.88)\n","Epoch: [109][ 90/391]\tTime  0.092 ( 0.095)\tLoss 8.9116e-02 (1.0520e-01)\tAcc@1  97.66 ( 97.10)\tAcc@5 100.00 ( 99.84)\n","Epoch: [109][120/391]\tTime  0.094 ( 0.094)\tLoss 9.4334e-02 (1.0466e-01)\tAcc@1  96.88 ( 97.08)\tAcc@5 100.00 ( 99.85)\n","Epoch: [109][150/391]\tTime  0.091 ( 0.094)\tLoss 4.4847e-02 (1.0434e-01)\tAcc@1  98.44 ( 97.10)\tAcc@5 100.00 ( 99.84)\n","Epoch: [109][180/391]\tTime  0.094 ( 0.094)\tLoss 1.0566e-01 (1.0361e-01)\tAcc@1  96.09 ( 97.14)\tAcc@5 100.00 ( 99.85)\n","Epoch: [109][210/391]\tTime  0.091 ( 0.094)\tLoss 1.7748e-01 (1.0368e-01)\tAcc@1  95.31 ( 97.15)\tAcc@5  99.22 ( 99.85)\n","Epoch: [109][240/391]\tTime  0.091 ( 0.093)\tLoss 1.1162e-01 (1.0320e-01)\tAcc@1  96.09 ( 97.19)\tAcc@5 100.00 ( 99.85)\n","Epoch: [109][270/391]\tTime  0.091 ( 0.093)\tLoss 2.3385e-01 (1.0456e-01)\tAcc@1  92.97 ( 97.14)\tAcc@5  99.22 ( 99.84)\n","Epoch: [109][300/391]\tTime  0.095 ( 0.093)\tLoss 1.6475e-01 (1.0599e-01)\tAcc@1  96.09 ( 97.12)\tAcc@5  99.22 ( 99.82)\n","Epoch: [109][330/391]\tTime  0.094 ( 0.093)\tLoss 4.6396e-02 (1.0611e-01)\tAcc@1  99.22 ( 97.12)\tAcc@5 100.00 ( 99.82)\n","Epoch: [109][360/391]\tTime  0.092 ( 0.093)\tLoss 1.3715e-01 (1.0591e-01)\tAcc@1  96.09 ( 97.14)\tAcc@5 100.00 ( 99.82)\n","Epoch: [109][390/391]\tTime  0.082 ( 0.093)\tLoss 8.9843e-02 (1.0642e-01)\tAcc@1  97.50 ( 97.11)\tAcc@5 100.00 ( 99.83)\n","==> Train Accuracy: Acc@1 97.108 || Acc@5 99.830\n","==> Test Accuracy:  Acc@1 77.110 || Acc@5 94.170\n","==> 38.95 seconds to train this epoch\n","\n","\n","----- epoch: 110, lr: 0.004000000000000001 -----\n","Epoch: [110][  0/391]\tTime  0.263 ( 0.263)\tLoss 6.6739e-02 (6.6739e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5 100.00 (100.00)\n","Epoch: [110][ 30/391]\tTime  0.095 ( 0.099)\tLoss 1.2835e-01 (9.3251e-02)\tAcc@1  95.31 ( 97.56)\tAcc@5 100.00 ( 99.95)\n","Epoch: [110][ 60/391]\tTime  0.093 ( 0.095)\tLoss 6.4234e-02 (9.9374e-02)\tAcc@1  98.44 ( 97.30)\tAcc@5 100.00 ( 99.88)\n","Epoch: [110][ 90/391]\tTime  0.090 ( 0.094)\tLoss 8.7385e-02 (9.9867e-02)\tAcc@1  98.44 ( 97.38)\tAcc@5 100.00 ( 99.87)\n","Epoch: [110][120/391]\tTime  0.093 ( 0.094)\tLoss 9.5392e-02 (9.8910e-02)\tAcc@1  96.88 ( 97.44)\tAcc@5 100.00 ( 99.86)\n","Epoch: [110][150/391]\tTime  0.095 ( 0.093)\tLoss 9.0637e-02 (9.9696e-02)\tAcc@1  98.44 ( 97.40)\tAcc@5 100.00 ( 99.86)\n","Epoch: [110][180/391]\tTime  0.096 ( 0.093)\tLoss 6.0799e-02 (1.0002e-01)\tAcc@1  99.22 ( 97.40)\tAcc@5 100.00 ( 99.87)\n","Epoch: [110][210/391]\tTime  0.094 ( 0.093)\tLoss 1.7301e-01 (1.0233e-01)\tAcc@1  97.66 ( 97.32)\tAcc@5  99.22 ( 99.85)\n","Epoch: [110][240/391]\tTime  0.093 ( 0.093)\tLoss 1.1237e-01 (1.0232e-01)\tAcc@1  96.88 ( 97.29)\tAcc@5 100.00 ( 99.86)\n","Epoch: [110][270/391]\tTime  0.091 ( 0.093)\tLoss 6.0341e-02 (1.0184e-01)\tAcc@1  98.44 ( 97.28)\tAcc@5  99.22 ( 99.86)\n","Epoch: [110][300/391]\tTime  0.089 ( 0.093)\tLoss 1.6802e-01 (1.0284e-01)\tAcc@1  96.09 ( 97.27)\tAcc@5  99.22 ( 99.84)\n","Epoch: [110][330/391]\tTime  0.091 ( 0.093)\tLoss 7.7483e-02 (1.0305e-01)\tAcc@1  96.88 ( 97.28)\tAcc@5 100.00 ( 99.84)\n","Epoch: [110][360/391]\tTime  0.095 ( 0.093)\tLoss 1.0714e-01 (1.0297e-01)\tAcc@1  96.88 ( 97.29)\tAcc@5 100.00 ( 99.85)\n","Epoch: [110][390/391]\tTime  0.082 ( 0.093)\tLoss 9.7898e-02 (1.0212e-01)\tAcc@1  97.50 ( 97.31)\tAcc@5  98.75 ( 99.85)\n","==> Train Accuracy: Acc@1 97.312 || Acc@5 99.854\n","==> Test Accuracy:  Acc@1 77.150 || Acc@5 94.200\n","==> 38.99 seconds to train this epoch\n","\n","\n","----- epoch: 111, lr: 0.004000000000000001 -----\n","Epoch: [111][  0/391]\tTime  0.297 ( 0.297)\tLoss 9.4928e-02 (9.4928e-02)\tAcc@1  98.44 ( 98.44)\tAcc@5 100.00 (100.00)\n","Epoch: [111][ 30/391]\tTime  0.095 ( 0.099)\tLoss 1.0862e-01 (9.8342e-02)\tAcc@1  95.31 ( 97.30)\tAcc@5 100.00 ( 99.90)\n","Epoch: [111][ 60/391]\tTime  0.094 ( 0.096)\tLoss 7.8180e-02 (9.7358e-02)\tAcc@1  96.88 ( 97.31)\tAcc@5 100.00 ( 99.92)\n","Epoch: [111][ 90/391]\tTime  0.092 ( 0.095)\tLoss 6.9489e-02 (1.0001e-01)\tAcc@1  97.66 ( 97.30)\tAcc@5 100.00 ( 99.89)\n","Epoch: [111][120/391]\tTime  0.093 ( 0.095)\tLoss 6.5525e-02 (1.0005e-01)\tAcc@1  99.22 ( 97.26)\tAcc@5 100.00 ( 99.88)\n","Epoch: [111][150/391]\tTime  0.091 ( 0.094)\tLoss 8.0470e-02 (9.7622e-02)\tAcc@1  98.44 ( 97.37)\tAcc@5 100.00 ( 99.89)\n","Epoch: [111][180/391]\tTime  0.090 ( 0.094)\tLoss 1.2294e-01 (9.7272e-02)\tAcc@1  97.66 ( 97.43)\tAcc@5  99.22 ( 99.89)\n","Epoch: [111][210/391]\tTime  0.091 ( 0.094)\tLoss 8.7113e-02 (9.7793e-02)\tAcc@1  97.66 ( 97.39)\tAcc@5 100.00 ( 99.89)\n","Epoch: [111][240/391]\tTime  0.093 ( 0.093)\tLoss 1.1843e-01 (9.7690e-02)\tAcc@1  96.09 ( 97.37)\tAcc@5 100.00 ( 99.90)\n","Epoch: [111][270/391]\tTime  0.092 ( 0.093)\tLoss 1.7095e-01 (9.8700e-02)\tAcc@1  95.31 ( 97.35)\tAcc@5  99.22 ( 99.90)\n","Epoch: [111][300/391]\tTime  0.091 ( 0.093)\tLoss 1.8171e-01 (1.0052e-01)\tAcc@1  94.53 ( 97.30)\tAcc@5  98.44 ( 99.87)\n","Epoch: [111][330/391]\tTime  0.094 ( 0.093)\tLoss 3.8415e-02 (9.9521e-02)\tAcc@1  99.22 ( 97.33)\tAcc@5 100.00 ( 99.87)\n","Epoch: [111][360/391]\tTime  0.092 ( 0.093)\tLoss 6.4394e-02 (9.9373e-02)\tAcc@1  99.22 ( 97.35)\tAcc@5 100.00 ( 99.87)\n","Epoch: [111][390/391]\tTime  0.082 ( 0.093)\tLoss 1.3680e-01 (1.0012e-01)\tAcc@1  96.25 ( 97.33)\tAcc@5 100.00 ( 99.87)\n","==> Train Accuracy: Acc@1 97.326 || Acc@5 99.868\n","==> Test Accuracy:  Acc@1 77.080 || Acc@5 94.080\n","==> 39.07 seconds to train this epoch\n","\n","\n","----- epoch: 112, lr: 0.004000000000000001 -----\n","Epoch: [112][  0/391]\tTime  0.285 ( 0.285)\tLoss 6.2017e-02 (6.2017e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5 100.00 (100.00)\n","Epoch: [112][ 30/391]\tTime  0.089 ( 0.100)\tLoss 1.2168e-01 (1.0300e-01)\tAcc@1  96.88 ( 97.33)\tAcc@5  99.22 ( 99.82)\n","Epoch: [112][ 60/391]\tTime  0.096 ( 0.096)\tLoss 6.9012e-02 (9.7418e-02)\tAcc@1  98.44 ( 97.44)\tAcc@5 100.00 ( 99.86)\n","Epoch: [112][ 90/391]\tTime  0.092 ( 0.095)\tLoss 7.6426e-02 (9.9235e-02)\tAcc@1  97.66 ( 97.42)\tAcc@5 100.00 ( 99.87)\n","Epoch: [112][120/391]\tTime  0.093 ( 0.095)\tLoss 1.2680e-01 (1.0002e-01)\tAcc@1  94.53 ( 97.38)\tAcc@5 100.00 ( 99.87)\n","Epoch: [112][150/391]\tTime  0.091 ( 0.094)\tLoss 8.0396e-02 (9.9857e-02)\tAcc@1  99.22 ( 97.40)\tAcc@5 100.00 ( 99.86)\n","Epoch: [112][180/391]\tTime  0.091 ( 0.094)\tLoss 1.1018e-01 (9.8797e-02)\tAcc@1  95.31 ( 97.45)\tAcc@5 100.00 ( 99.87)\n","Epoch: [112][210/391]\tTime  0.091 ( 0.094)\tLoss 4.7747e-02 (1.0054e-01)\tAcc@1  98.44 ( 97.37)\tAcc@5 100.00 ( 99.86)\n","Epoch: [112][240/391]\tTime  0.092 ( 0.094)\tLoss 1.7422e-01 (1.0251e-01)\tAcc@1  94.53 ( 97.34)\tAcc@5  99.22 ( 99.85)\n","Epoch: [112][270/391]\tTime  0.096 ( 0.093)\tLoss 1.3263e-01 (1.0375e-01)\tAcc@1  95.31 ( 97.28)\tAcc@5  99.22 ( 99.84)\n","Epoch: [112][300/391]\tTime  0.089 ( 0.093)\tLoss 1.8906e-01 (1.0396e-01)\tAcc@1  93.75 ( 97.27)\tAcc@5  99.22 ( 99.84)\n","Epoch: [112][330/391]\tTime  0.092 ( 0.093)\tLoss 6.5117e-02 (1.0386e-01)\tAcc@1  98.44 ( 97.27)\tAcc@5 100.00 ( 99.83)\n","Epoch: [112][360/391]\tTime  0.092 ( 0.093)\tLoss 1.2257e-01 (1.0523e-01)\tAcc@1  96.88 ( 97.23)\tAcc@5  99.22 ( 99.83)\n","Epoch: [112][390/391]\tTime  0.082 ( 0.093)\tLoss 5.5694e-02 (1.0533e-01)\tAcc@1  97.50 ( 97.24)\tAcc@5 100.00 ( 99.83)\n","==> Train Accuracy: Acc@1 97.238 || Acc@5 99.828\n","==> Test Accuracy:  Acc@1 76.650 || Acc@5 93.860\n","==> 39.10 seconds to train this epoch\n","\n","\n","----- epoch: 113, lr: 0.004000000000000001 -----\n","Epoch: [113][  0/391]\tTime  0.272 ( 0.272)\tLoss 1.2372e-01 (1.2372e-01)\tAcc@1  95.31 ( 95.31)\tAcc@5 100.00 (100.00)\n","Epoch: [113][ 30/391]\tTime  0.093 ( 0.100)\tLoss 7.3748e-02 (9.1489e-02)\tAcc@1  98.44 ( 97.56)\tAcc@5 100.00 ( 99.90)\n","Epoch: [113][ 60/391]\tTime  0.089 ( 0.096)\tLoss 5.7790e-02 (9.0406e-02)\tAcc@1  98.44 ( 97.71)\tAcc@5 100.00 ( 99.86)\n","Epoch: [113][ 90/391]\tTime  0.088 ( 0.095)\tLoss 8.1626e-02 (8.8889e-02)\tAcc@1  99.22 ( 97.67)\tAcc@5  99.22 ( 99.87)\n","Epoch: [113][120/391]\tTime  0.089 ( 0.094)\tLoss 1.1976e-01 (9.1409e-02)\tAcc@1  96.09 ( 97.66)\tAcc@5 100.00 ( 99.87)\n","Epoch: [113][150/391]\tTime  0.088 ( 0.094)\tLoss 1.0553e-01 (9.2706e-02)\tAcc@1  97.66 ( 97.66)\tAcc@5 100.00 ( 99.87)\n","Epoch: [113][180/391]\tTime  0.091 ( 0.094)\tLoss 1.0996e-01 (9.4073e-02)\tAcc@1  97.66 ( 97.63)\tAcc@5 100.00 ( 99.86)\n","Epoch: [113][210/391]\tTime  0.105 ( 0.094)\tLoss 8.2709e-02 (9.4693e-02)\tAcc@1  97.66 ( 97.63)\tAcc@5 100.00 ( 99.85)\n","Epoch: [113][240/391]\tTime  0.092 ( 0.093)\tLoss 1.0002e-01 (9.5144e-02)\tAcc@1  97.66 ( 97.62)\tAcc@5 100.00 ( 99.86)\n","Epoch: [113][270/391]\tTime  0.097 ( 0.093)\tLoss 1.1967e-01 (9.5661e-02)\tAcc@1  96.88 ( 97.58)\tAcc@5 100.00 ( 99.86)\n","Epoch: [113][300/391]\tTime  0.095 ( 0.093)\tLoss 1.2556e-01 (9.5385e-02)\tAcc@1  95.31 ( 97.55)\tAcc@5 100.00 ( 99.87)\n","Epoch: [113][330/391]\tTime  0.087 ( 0.093)\tLoss 1.4109e-01 (9.6723e-02)\tAcc@1  95.31 ( 97.50)\tAcc@5  99.22 ( 99.87)\n","Epoch: [113][360/391]\tTime  0.089 ( 0.093)\tLoss 1.2949e-01 (9.8002e-02)\tAcc@1  96.88 ( 97.48)\tAcc@5 100.00 ( 99.87)\n","Epoch: [113][390/391]\tTime  0.081 ( 0.093)\tLoss 8.8783e-02 (9.9386e-02)\tAcc@1  97.50 ( 97.43)\tAcc@5 100.00 ( 99.86)\n","==> Train Accuracy: Acc@1 97.428 || Acc@5 99.860\n","==> Test Accuracy:  Acc@1 76.840 || Acc@5 93.870\n","==> 39.04 seconds to train this epoch\n","\n","\n","----- epoch: 114, lr: 0.004000000000000001 -----\n","Epoch: [114][  0/391]\tTime  0.282 ( 0.282)\tLoss 1.6166e-01 (1.6166e-01)\tAcc@1  95.31 ( 95.31)\tAcc@5 100.00 (100.00)\n","Epoch: [114][ 30/391]\tTime  0.094 ( 0.100)\tLoss 1.4838e-01 (1.0482e-01)\tAcc@1  95.31 ( 97.25)\tAcc@5  99.22 ( 99.82)\n","Epoch: [114][ 60/391]\tTime  0.095 ( 0.097)\tLoss 8.9574e-02 (1.0199e-01)\tAcc@1  99.22 ( 97.44)\tAcc@5 100.00 ( 99.83)\n","Epoch: [114][ 90/391]\tTime  0.092 ( 0.095)\tLoss 1.0788e-01 (1.0018e-01)\tAcc@1  97.66 ( 97.42)\tAcc@5 100.00 ( 99.86)\n","Epoch: [114][120/391]\tTime  0.089 ( 0.094)\tLoss 8.9328e-02 (9.7944e-02)\tAcc@1  98.44 ( 97.44)\tAcc@5 100.00 ( 99.86)\n","Epoch: [114][150/391]\tTime  0.092 ( 0.094)\tLoss 7.4574e-02 (9.6318e-02)\tAcc@1  97.66 ( 97.51)\tAcc@5 100.00 ( 99.87)\n","Epoch: [114][180/391]\tTime  0.096 ( 0.094)\tLoss 8.8237e-02 (9.5227e-02)\tAcc@1  98.44 ( 97.55)\tAcc@5 100.00 ( 99.88)\n","Epoch: [114][210/391]\tTime  0.092 ( 0.094)\tLoss 5.1227e-02 (9.4951e-02)\tAcc@1 100.00 ( 97.54)\tAcc@5 100.00 ( 99.88)\n","Epoch: [114][240/391]\tTime  0.088 ( 0.093)\tLoss 8.0198e-02 (9.5741e-02)\tAcc@1  97.66 ( 97.52)\tAcc@5 100.00 ( 99.87)\n","Epoch: [114][270/391]\tTime  0.087 ( 0.093)\tLoss 1.2779e-01 (9.7032e-02)\tAcc@1  96.88 ( 97.49)\tAcc@5  99.22 ( 99.86)\n","Epoch: [114][300/391]\tTime  0.097 ( 0.093)\tLoss 6.2480e-02 (9.8825e-02)\tAcc@1  99.22 ( 97.43)\tAcc@5 100.00 ( 99.86)\n","Epoch: [114][330/391]\tTime  0.088 ( 0.093)\tLoss 5.6034e-02 (9.9166e-02)\tAcc@1  99.22 ( 97.42)\tAcc@5 100.00 ( 99.86)\n","Epoch: [114][360/391]\tTime  0.093 ( 0.093)\tLoss 8.4030e-02 (9.8949e-02)\tAcc@1  99.22 ( 97.43)\tAcc@5 100.00 ( 99.86)\n","Epoch: [114][390/391]\tTime  0.082 ( 0.093)\tLoss 9.8769e-02 (9.9644e-02)\tAcc@1  96.25 ( 97.40)\tAcc@5 100.00 ( 99.85)\n","==> Train Accuracy: Acc@1 97.402 || Acc@5 99.846\n","==> Test Accuracy:  Acc@1 76.670 || Acc@5 94.020\n","==> 39.02 seconds to train this epoch\n","\n","\n","----- epoch: 115, lr: 0.004000000000000001 -----\n","Epoch: [115][  0/391]\tTime  0.267 ( 0.267)\tLoss 1.3905e-01 (1.3905e-01)\tAcc@1  96.09 ( 96.09)\tAcc@5  99.22 ( 99.22)\n","Epoch: [115][ 30/391]\tTime  0.093 ( 0.099)\tLoss 1.1976e-01 (9.4853e-02)\tAcc@1  96.88 ( 97.48)\tAcc@5 100.00 ( 99.80)\n","Epoch: [115][ 60/391]\tTime  0.090 ( 0.095)\tLoss 7.8181e-02 (9.5901e-02)\tAcc@1  98.44 ( 97.45)\tAcc@5 100.00 ( 99.85)\n","Epoch: [115][ 90/391]\tTime  0.091 ( 0.094)\tLoss 8.3684e-02 (9.5950e-02)\tAcc@1  97.66 ( 97.47)\tAcc@5 100.00 ( 99.88)\n","Epoch: [115][120/391]\tTime  0.092 ( 0.094)\tLoss 1.5334e-01 (9.5238e-02)\tAcc@1  96.88 ( 97.50)\tAcc@5  98.44 ( 99.86)\n","Epoch: [115][150/391]\tTime  0.094 ( 0.094)\tLoss 9.7133e-02 (9.5735e-02)\tAcc@1  95.31 ( 97.49)\tAcc@5 100.00 ( 99.87)\n","Epoch: [115][180/391]\tTime  0.097 ( 0.093)\tLoss 7.2205e-02 (9.7783e-02)\tAcc@1  97.66 ( 97.38)\tAcc@5 100.00 ( 99.87)\n","Epoch: [115][210/391]\tTime  0.090 ( 0.093)\tLoss 8.6781e-02 (9.9153e-02)\tAcc@1  97.66 ( 97.32)\tAcc@5 100.00 ( 99.87)\n","Epoch: [115][240/391]\tTime  0.093 ( 0.093)\tLoss 1.2756e-01 (1.0059e-01)\tAcc@1  96.09 ( 97.25)\tAcc@5  99.22 ( 99.87)\n","Epoch: [115][270/391]\tTime  0.092 ( 0.093)\tLoss 1.1014e-01 (1.0192e-01)\tAcc@1  98.44 ( 97.22)\tAcc@5 100.00 ( 99.85)\n","Epoch: [115][300/391]\tTime  0.091 ( 0.093)\tLoss 1.0233e-01 (1.0148e-01)\tAcc@1  96.88 ( 97.25)\tAcc@5 100.00 ( 99.86)\n","Epoch: [115][330/391]\tTime  0.087 ( 0.093)\tLoss 5.8740e-02 (1.0184e-01)\tAcc@1  97.66 ( 97.25)\tAcc@5 100.00 ( 99.85)\n","Epoch: [115][360/391]\tTime  0.096 ( 0.093)\tLoss 9.5167e-02 (1.0158e-01)\tAcc@1  98.44 ( 97.25)\tAcc@5 100.00 ( 99.85)\n","Epoch: [115][390/391]\tTime  0.084 ( 0.093)\tLoss 1.0761e-01 (1.0188e-01)\tAcc@1  96.25 ( 97.25)\tAcc@5 100.00 ( 99.86)\n","==> Train Accuracy: Acc@1 97.248 || Acc@5 99.856\n","==> Test Accuracy:  Acc@1 76.850 || Acc@5 93.980\n","==> 39.07 seconds to train this epoch\n","\n","\n","----- epoch: 116, lr: 0.004000000000000001 -----\n","Epoch: [116][  0/391]\tTime  0.308 ( 0.308)\tLoss 8.3235e-02 (8.3235e-02)\tAcc@1  98.44 ( 98.44)\tAcc@5 100.00 (100.00)\n","Epoch: [116][ 30/391]\tTime  0.091 ( 0.100)\tLoss 1.0460e-01 (9.0688e-02)\tAcc@1  98.44 ( 97.58)\tAcc@5 100.00 ( 99.97)\n","Epoch: [116][ 60/391]\tTime  0.096 ( 0.097)\tLoss 8.7920e-02 (9.0442e-02)\tAcc@1  97.66 ( 97.49)\tAcc@5 100.00 ( 99.90)\n","Epoch: [116][ 90/391]\tTime  0.092 ( 0.095)\tLoss 8.4117e-02 (9.1458e-02)\tAcc@1  98.44 ( 97.54)\tAcc@5  99.22 ( 99.91)\n","Epoch: [116][120/391]\tTime  0.094 ( 0.095)\tLoss 1.1088e-01 (9.2453e-02)\tAcc@1  97.66 ( 97.57)\tAcc@5  99.22 ( 99.86)\n","Epoch: [116][150/391]\tTime  0.083 ( 0.094)\tLoss 1.0685e-01 (9.2746e-02)\tAcc@1  97.66 ( 97.51)\tAcc@5 100.00 ( 99.87)\n","Epoch: [116][180/391]\tTime  0.094 ( 0.094)\tLoss 1.0999e-01 (9.2780e-02)\tAcc@1  98.44 ( 97.55)\tAcc@5 100.00 ( 99.89)\n","Epoch: [116][210/391]\tTime  0.086 ( 0.094)\tLoss 7.9355e-02 (9.3577e-02)\tAcc@1  97.66 ( 97.53)\tAcc@5 100.00 ( 99.90)\n","Epoch: [116][240/391]\tTime  0.092 ( 0.093)\tLoss 1.2720e-01 (9.4195e-02)\tAcc@1  96.88 ( 97.54)\tAcc@5  99.22 ( 99.90)\n","Epoch: [116][270/391]\tTime  0.096 ( 0.093)\tLoss 1.0590e-01 (9.3995e-02)\tAcc@1  96.88 ( 97.54)\tAcc@5 100.00 ( 99.88)\n","Epoch: [116][300/391]\tTime  0.091 ( 0.093)\tLoss 1.0142e-01 (9.4901e-02)\tAcc@1  97.66 ( 97.52)\tAcc@5 100.00 ( 99.87)\n","Epoch: [116][330/391]\tTime  0.091 ( 0.093)\tLoss 8.8197e-02 (9.4065e-02)\tAcc@1  97.66 ( 97.56)\tAcc@5 100.00 ( 99.87)\n","Epoch: [116][360/391]\tTime  0.091 ( 0.093)\tLoss 1.2124e-01 (9.5478e-02)\tAcc@1  96.88 ( 97.52)\tAcc@5 100.00 ( 99.86)\n","Epoch: [116][390/391]\tTime  0.083 ( 0.093)\tLoss 1.6548e-01 (9.5684e-02)\tAcc@1  95.00 ( 97.50)\tAcc@5 100.00 ( 99.86)\n","==> Train Accuracy: Acc@1 97.496 || Acc@5 99.862\n","==> Test Accuracy:  Acc@1 77.050 || Acc@5 94.320\n","==> 39.04 seconds to train this epoch\n","\n","\n","----- epoch: 117, lr: 0.004000000000000001 -----\n","Epoch: [117][  0/391]\tTime  0.287 ( 0.287)\tLoss 7.9501e-02 (7.9501e-02)\tAcc@1  96.88 ( 96.88)\tAcc@5 100.00 (100.00)\n","Epoch: [117][ 30/391]\tTime  0.090 ( 0.099)\tLoss 7.4691e-02 (9.1571e-02)\tAcc@1  99.22 ( 97.58)\tAcc@5 100.00 ( 99.85)\n","Epoch: [117][ 60/391]\tTime  0.092 ( 0.096)\tLoss 1.0494e-01 (9.0904e-02)\tAcc@1  96.09 ( 97.68)\tAcc@5 100.00 ( 99.87)\n","Epoch: [117][ 90/391]\tTime  0.105 ( 0.095)\tLoss 6.8103e-02 (9.1712e-02)\tAcc@1  99.22 ( 97.57)\tAcc@5 100.00 ( 99.88)\n","Epoch: [117][120/391]\tTime  0.086 ( 0.094)\tLoss 1.0051e-01 (9.2710e-02)\tAcc@1  98.44 ( 97.59)\tAcc@5 100.00 ( 99.85)\n","Epoch: [117][150/391]\tTime  0.098 ( 0.094)\tLoss 6.3256e-02 (9.3234e-02)\tAcc@1  97.66 ( 97.56)\tAcc@5 100.00 ( 99.87)\n","Epoch: [117][180/391]\tTime  0.087 ( 0.094)\tLoss 6.7241e-02 (9.3996e-02)\tAcc@1  97.66 ( 97.55)\tAcc@5 100.00 ( 99.86)\n","Epoch: [117][210/391]\tTime  0.090 ( 0.093)\tLoss 1.0027e-01 (9.3179e-02)\tAcc@1  97.66 ( 97.58)\tAcc@5 100.00 ( 99.86)\n","Epoch: [117][240/391]\tTime  0.095 ( 0.093)\tLoss 1.2474e-01 (9.3866e-02)\tAcc@1  94.53 ( 97.52)\tAcc@5 100.00 ( 99.86)\n","Epoch: [117][270/391]\tTime  0.093 ( 0.093)\tLoss 1.8624e-01 (9.4421e-02)\tAcc@1  95.31 ( 97.50)\tAcc@5 100.00 ( 99.86)\n","Epoch: [117][300/391]\tTime  0.090 ( 0.093)\tLoss 1.0758e-01 (9.4597e-02)\tAcc@1  97.66 ( 97.49)\tAcc@5 100.00 ( 99.87)\n","Epoch: [117][330/391]\tTime  0.090 ( 0.093)\tLoss 1.0083e-01 (9.4936e-02)\tAcc@1  97.66 ( 97.50)\tAcc@5 100.00 ( 99.87)\n","Epoch: [117][360/391]\tTime  0.088 ( 0.093)\tLoss 1.0296e-01 (9.5116e-02)\tAcc@1  96.09 ( 97.50)\tAcc@5  99.22 ( 99.87)\n","Epoch: [117][390/391]\tTime  0.082 ( 0.093)\tLoss 1.4319e-01 (9.5766e-02)\tAcc@1  95.00 ( 97.49)\tAcc@5  98.75 ( 99.86)\n","==> Train Accuracy: Acc@1 97.490 || Acc@5 99.860\n","==> Test Accuracy:  Acc@1 76.750 || Acc@5 93.900\n","==> 39.07 seconds to train this epoch\n","\n","\n","----- epoch: 118, lr: 0.004000000000000001 -----\n","Epoch: [118][  0/391]\tTime  0.290 ( 0.290)\tLoss 6.6515e-02 (6.6515e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5 100.00 (100.00)\n","Epoch: [118][ 30/391]\tTime  0.091 ( 0.100)\tLoss 1.7757e-01 (8.8199e-02)\tAcc@1  94.53 ( 98.01)\tAcc@5  99.22 ( 99.90)\n","Epoch: [118][ 60/391]\tTime  0.090 ( 0.096)\tLoss 6.9579e-02 (9.3814e-02)\tAcc@1  97.66 ( 97.71)\tAcc@5 100.00 ( 99.85)\n","Epoch: [118][ 90/391]\tTime  0.094 ( 0.095)\tLoss 1.1234e-01 (9.3821e-02)\tAcc@1  96.09 ( 97.65)\tAcc@5 100.00 ( 99.86)\n","Epoch: [118][120/391]\tTime  0.091 ( 0.094)\tLoss 4.7437e-02 (9.3685e-02)\tAcc@1  99.22 ( 97.59)\tAcc@5 100.00 ( 99.85)\n","Epoch: [118][150/391]\tTime  0.091 ( 0.094)\tLoss 6.0165e-02 (9.3990e-02)\tAcc@1  98.44 ( 97.58)\tAcc@5 100.00 ( 99.86)\n","Epoch: [118][180/391]\tTime  0.093 ( 0.094)\tLoss 1.2062e-01 (9.3515e-02)\tAcc@1  96.88 ( 97.58)\tAcc@5 100.00 ( 99.85)\n","Epoch: [118][210/391]\tTime  0.091 ( 0.094)\tLoss 7.6976e-02 (9.4618e-02)\tAcc@1  96.88 ( 97.52)\tAcc@5 100.00 ( 99.87)\n","Epoch: [118][240/391]\tTime  0.093 ( 0.094)\tLoss 1.5151e-01 (9.4899e-02)\tAcc@1  95.31 ( 97.53)\tAcc@5  99.22 ( 99.86)\n","Epoch: [118][270/391]\tTime  0.093 ( 0.093)\tLoss 4.8387e-02 (9.5492e-02)\tAcc@1  98.44 ( 97.51)\tAcc@5 100.00 ( 99.85)\n","Epoch: [118][300/391]\tTime  0.094 ( 0.093)\tLoss 9.5609e-02 (9.5371e-02)\tAcc@1  97.66 ( 97.51)\tAcc@5 100.00 ( 99.86)\n","Epoch: [118][330/391]\tTime  0.093 ( 0.093)\tLoss 8.3459e-02 (9.5280e-02)\tAcc@1  97.66 ( 97.53)\tAcc@5 100.00 ( 99.87)\n","Epoch: [118][360/391]\tTime  0.091 ( 0.093)\tLoss 8.9040e-02 (9.5058e-02)\tAcc@1  98.44 ( 97.55)\tAcc@5 100.00 ( 99.87)\n","Epoch: [118][390/391]\tTime  0.081 ( 0.093)\tLoss 5.9938e-02 (9.4737e-02)\tAcc@1  98.75 ( 97.57)\tAcc@5 100.00 ( 99.87)\n","==> Train Accuracy: Acc@1 97.572 || Acc@5 99.866\n","==> Test Accuracy:  Acc@1 76.830 || Acc@5 93.780\n","==> 39.08 seconds to train this epoch\n","\n","\n","----- epoch: 119, lr: 0.004000000000000001 -----\n","Epoch: [119][  0/391]\tTime  0.264 ( 0.264)\tLoss 1.5488e-01 (1.5488e-01)\tAcc@1  94.53 ( 94.53)\tAcc@5 100.00 (100.00)\n","Epoch: [119][ 30/391]\tTime  0.089 ( 0.099)\tLoss 1.0581e-01 (9.2719e-02)\tAcc@1  95.31 ( 97.33)\tAcc@5 100.00 ( 99.92)\n","Epoch: [119][ 60/391]\tTime  0.090 ( 0.096)\tLoss 1.1048e-01 (9.1900e-02)\tAcc@1  96.88 ( 97.40)\tAcc@5 100.00 ( 99.94)\n","Epoch: [119][ 90/391]\tTime  0.099 ( 0.095)\tLoss 8.5946e-02 (8.9527e-02)\tAcc@1  96.88 ( 97.57)\tAcc@5 100.00 ( 99.91)\n","Epoch: [119][120/391]\tTime  0.090 ( 0.094)\tLoss 9.9234e-02 (8.8483e-02)\tAcc@1  95.31 ( 97.64)\tAcc@5 100.00 ( 99.92)\n","Epoch: [119][150/391]\tTime  0.093 ( 0.094)\tLoss 8.4507e-02 (8.9842e-02)\tAcc@1  98.44 ( 97.63)\tAcc@5 100.00 ( 99.90)\n","Epoch: [119][180/391]\tTime  0.085 ( 0.094)\tLoss 1.5620e-01 (9.0323e-02)\tAcc@1  96.09 ( 97.64)\tAcc@5  99.22 ( 99.88)\n","Epoch: [119][210/391]\tTime  0.088 ( 0.094)\tLoss 1.5067e-01 (9.0230e-02)\tAcc@1  96.88 ( 97.65)\tAcc@5  99.22 ( 99.89)\n","Epoch: [119][240/391]\tTime  0.095 ( 0.094)\tLoss 9.3737e-02 (9.0448e-02)\tAcc@1  98.44 ( 97.65)\tAcc@5  99.22 ( 99.88)\n","Epoch: [119][270/391]\tTime  0.091 ( 0.093)\tLoss 9.1730e-02 (9.0747e-02)\tAcc@1  97.66 ( 97.67)\tAcc@5 100.00 ( 99.88)\n","Epoch: [119][300/391]\tTime  0.094 ( 0.093)\tLoss 1.0254e-01 (9.0581e-02)\tAcc@1  98.44 ( 97.67)\tAcc@5 100.00 ( 99.88)\n","Epoch: [119][330/391]\tTime  0.089 ( 0.093)\tLoss 4.1743e-02 (9.0693e-02)\tAcc@1  99.22 ( 97.66)\tAcc@5 100.00 ( 99.88)\n","Epoch: [119][360/391]\tTime  0.105 ( 0.093)\tLoss 1.9112e-01 (9.1989e-02)\tAcc@1  94.53 ( 97.61)\tAcc@5  99.22 ( 99.87)\n","Epoch: [119][390/391]\tTime  0.082 ( 0.093)\tLoss 6.2533e-02 (9.2920e-02)\tAcc@1  97.50 ( 97.59)\tAcc@5 100.00 ( 99.88)\n","==> Train Accuracy: Acc@1 97.590 || Acc@5 99.876\n","==> Test Accuracy:  Acc@1 76.260 || Acc@5 93.620\n","==> 39.15 seconds to train this epoch\n","\n","\n","----- epoch: 120, lr: 0.0008000000000000003 -----\n","Epoch: [120][  0/391]\tTime  0.287 ( 0.287)\tLoss 1.1837e-01 (1.1837e-01)\tAcc@1  96.09 ( 96.09)\tAcc@5 100.00 (100.00)\n","Epoch: [120][ 30/391]\tTime  0.097 ( 0.100)\tLoss 7.1085e-02 (8.5748e-02)\tAcc@1  97.66 ( 97.93)\tAcc@5 100.00 ( 99.87)\n","Epoch: [120][ 60/391]\tTime  0.092 ( 0.096)\tLoss 7.6398e-02 (8.1075e-02)\tAcc@1  97.66 ( 98.05)\tAcc@5 100.00 ( 99.91)\n","Epoch: [120][ 90/391]\tTime  0.093 ( 0.095)\tLoss 5.9078e-02 (8.2990e-02)\tAcc@1  98.44 ( 97.97)\tAcc@5 100.00 ( 99.87)\n","Epoch: [120][120/391]\tTime  0.093 ( 0.094)\tLoss 4.8031e-02 (8.2215e-02)\tAcc@1  99.22 ( 97.99)\tAcc@5 100.00 ( 99.87)\n","Epoch: [120][150/391]\tTime  0.107 ( 0.094)\tLoss 6.3366e-02 (8.0969e-02)\tAcc@1  98.44 ( 97.95)\tAcc@5 100.00 ( 99.88)\n","Epoch: [120][180/391]\tTime  0.097 ( 0.094)\tLoss 7.3852e-02 (8.0340e-02)\tAcc@1  97.66 ( 97.94)\tAcc@5 100.00 ( 99.89)\n","Epoch: [120][210/391]\tTime  0.092 ( 0.094)\tLoss 2.4871e-02 (8.0178e-02)\tAcc@1 100.00 ( 97.94)\tAcc@5 100.00 ( 99.89)\n","Epoch: [120][240/391]\tTime  0.090 ( 0.093)\tLoss 1.0558e-01 (8.0004e-02)\tAcc@1  98.44 ( 97.97)\tAcc@5  99.22 ( 99.89)\n","Epoch: [120][270/391]\tTime  0.098 ( 0.093)\tLoss 1.2354e-01 (7.8973e-02)\tAcc@1  96.88 ( 97.99)\tAcc@5  99.22 ( 99.89)\n","Epoch: [120][300/391]\tTime  0.085 ( 0.093)\tLoss 6.3998e-02 (7.8308e-02)\tAcc@1  96.88 ( 98.00)\tAcc@5 100.00 ( 99.89)\n","Epoch: [120][330/391]\tTime  0.095 ( 0.093)\tLoss 8.2024e-02 (7.7363e-02)\tAcc@1  97.66 ( 98.04)\tAcc@5 100.00 ( 99.89)\n","Epoch: [120][360/391]\tTime  0.092 ( 0.093)\tLoss 3.3635e-02 (7.6269e-02)\tAcc@1  99.22 ( 98.08)\tAcc@5 100.00 ( 99.89)\n","Epoch: [120][390/391]\tTime  0.082 ( 0.093)\tLoss 2.2838e-02 (7.6333e-02)\tAcc@1 100.00 ( 98.08)\tAcc@5 100.00 ( 99.89)\n","==> Train Accuracy: Acc@1 98.080 || Acc@5 99.888\n","==> Test Accuracy:  Acc@1 77.390 || Acc@5 94.130\n","==> 39.10 seconds to train this epoch\n","\n","\n","----- epoch: 121, lr: 0.0008000000000000003 -----\n","Epoch: [121][  0/391]\tTime  0.262 ( 0.262)\tLoss 7.7507e-02 (7.7507e-02)\tAcc@1  97.66 ( 97.66)\tAcc@5 100.00 (100.00)\n","Epoch: [121][ 30/391]\tTime  0.093 ( 0.099)\tLoss 5.6159e-02 (6.3212e-02)\tAcc@1  99.22 ( 98.61)\tAcc@5 100.00 ( 99.97)\n","Epoch: [121][ 60/391]\tTime  0.091 ( 0.095)\tLoss 6.3344e-02 (6.7348e-02)\tAcc@1  98.44 ( 98.45)\tAcc@5 100.00 ( 99.91)\n","Epoch: [121][ 90/391]\tTime  0.098 ( 0.094)\tLoss 6.8250e-02 (6.7733e-02)\tAcc@1  97.66 ( 98.40)\tAcc@5 100.00 ( 99.91)\n","Epoch: [121][120/391]\tTime  0.092 ( 0.094)\tLoss 6.5628e-02 (6.7991e-02)\tAcc@1  98.44 ( 98.41)\tAcc@5 100.00 ( 99.92)\n","Epoch: [121][150/391]\tTime  0.095 ( 0.094)\tLoss 5.9556e-02 (6.8965e-02)\tAcc@1  98.44 ( 98.35)\tAcc@5 100.00 ( 99.93)\n","Epoch: [121][180/391]\tTime  0.108 ( 0.093)\tLoss 2.8487e-02 (6.8582e-02)\tAcc@1  99.22 ( 98.40)\tAcc@5 100.00 ( 99.91)\n","Epoch: [121][210/391]\tTime  0.090 ( 0.093)\tLoss 6.7536e-02 (6.7704e-02)\tAcc@1  97.66 ( 98.38)\tAcc@5 100.00 ( 99.91)\n","Epoch: [121][240/391]\tTime  0.092 ( 0.093)\tLoss 4.2834e-02 (6.8749e-02)\tAcc@1  98.44 ( 98.33)\tAcc@5 100.00 ( 99.92)\n","Epoch: [121][270/391]\tTime  0.091 ( 0.093)\tLoss 5.3031e-02 (6.7914e-02)\tAcc@1  99.22 ( 98.36)\tAcc@5 100.00 ( 99.92)\n","Epoch: [121][300/391]\tTime  0.093 ( 0.093)\tLoss 7.6860e-02 (6.8316e-02)\tAcc@1  98.44 ( 98.35)\tAcc@5  99.22 ( 99.91)\n","Epoch: [121][330/391]\tTime  0.091 ( 0.093)\tLoss 5.3946e-02 (6.8647e-02)\tAcc@1  99.22 ( 98.32)\tAcc@5 100.00 ( 99.92)\n","Epoch: [121][360/391]\tTime  0.093 ( 0.093)\tLoss 6.1012e-02 (6.8394e-02)\tAcc@1  99.22 ( 98.32)\tAcc@5 100.00 ( 99.92)\n","Epoch: [121][390/391]\tTime  0.082 ( 0.093)\tLoss 5.7084e-02 (6.8234e-02)\tAcc@1  98.75 ( 98.32)\tAcc@5 100.00 ( 99.92)\n","==> Train Accuracy: Acc@1 98.322 || Acc@5 99.918\n","==> Test Accuracy:  Acc@1 77.240 || Acc@5 94.130\n","==> 39.05 seconds to train this epoch\n","\n","\n","----- epoch: 122, lr: 0.0008000000000000003 -----\n","Epoch: [122][  0/391]\tTime  0.266 ( 0.266)\tLoss 5.0962e-02 (5.0962e-02)\tAcc@1  97.66 ( 97.66)\tAcc@5 100.00 (100.00)\n","Epoch: [122][ 30/391]\tTime  0.092 ( 0.099)\tLoss 8.1643e-02 (5.9976e-02)\tAcc@1  98.44 ( 98.41)\tAcc@5  99.22 ( 99.95)\n","Epoch: [122][ 60/391]\tTime  0.092 ( 0.096)\tLoss 2.5190e-02 (6.1643e-02)\tAcc@1 100.00 ( 98.36)\tAcc@5 100.00 ( 99.92)\n","Epoch: [122][ 90/391]\tTime  0.080 ( 0.094)\tLoss 6.6509e-02 (6.0297e-02)\tAcc@1  97.66 ( 98.42)\tAcc@5 100.00 ( 99.90)\n","Epoch: [122][120/391]\tTime  0.091 ( 0.094)\tLoss 3.9463e-02 (6.0240e-02)\tAcc@1  99.22 ( 98.41)\tAcc@5 100.00 ( 99.90)\n","Epoch: [122][150/391]\tTime  0.093 ( 0.094)\tLoss 8.2276e-02 (6.0397e-02)\tAcc@1  98.44 ( 98.43)\tAcc@5 100.00 ( 99.90)\n","Epoch: [122][180/391]\tTime  0.091 ( 0.093)\tLoss 7.8429e-02 (6.0481e-02)\tAcc@1  97.66 ( 98.40)\tAcc@5 100.00 ( 99.91)\n","Epoch: [122][210/391]\tTime  0.091 ( 0.093)\tLoss 6.6236e-02 (6.1702e-02)\tAcc@1  97.66 ( 98.37)\tAcc@5 100.00 ( 99.91)\n","Epoch: [122][240/391]\tTime  0.090 ( 0.093)\tLoss 3.5660e-02 (6.1874e-02)\tAcc@1  99.22 ( 98.40)\tAcc@5 100.00 ( 99.91)\n","Epoch: [122][270/391]\tTime  0.093 ( 0.093)\tLoss 4.8766e-02 (6.1612e-02)\tAcc@1  99.22 ( 98.43)\tAcc@5 100.00 ( 99.92)\n","Epoch: [122][300/391]\tTime  0.092 ( 0.093)\tLoss 1.7069e-02 (6.1653e-02)\tAcc@1 100.00 ( 98.45)\tAcc@5 100.00 ( 99.91)\n","Epoch: [122][330/391]\tTime  0.094 ( 0.093)\tLoss 7.4230e-02 (6.1727e-02)\tAcc@1  97.66 ( 98.45)\tAcc@5 100.00 ( 99.92)\n","Epoch: [122][360/391]\tTime  0.091 ( 0.093)\tLoss 9.3514e-02 (6.1421e-02)\tAcc@1  97.66 ( 98.47)\tAcc@5 100.00 ( 99.92)\n","Epoch: [122][390/391]\tTime  0.082 ( 0.093)\tLoss 6.0486e-02 (6.1384e-02)\tAcc@1  97.50 ( 98.46)\tAcc@5 100.00 ( 99.91)\n","==> Train Accuracy: Acc@1 98.458 || Acc@5 99.914\n","==> Test Accuracy:  Acc@1 77.770 || Acc@5 94.090\n","==> 38.98 seconds to train this epoch\n","\n","\n","----- epoch: 123, lr: 0.0008000000000000003 -----\n","Epoch: [123][  0/391]\tTime  0.285 ( 0.285)\tLoss 4.0079e-02 (4.0079e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5 100.00 (100.00)\n","Epoch: [123][ 30/391]\tTime  0.093 ( 0.101)\tLoss 9.9690e-02 (6.0898e-02)\tAcc@1  96.88 ( 98.39)\tAcc@5 100.00 ( 99.95)\n","Epoch: [123][ 60/391]\tTime  0.100 ( 0.097)\tLoss 7.3999e-02 (6.0366e-02)\tAcc@1  99.22 ( 98.50)\tAcc@5 100.00 ( 99.92)\n","Epoch: [123][ 90/391]\tTime  0.095 ( 0.096)\tLoss 5.6285e-02 (6.0807e-02)\tAcc@1  99.22 ( 98.52)\tAcc@5 100.00 ( 99.91)\n","Epoch: [123][120/391]\tTime  0.097 ( 0.095)\tLoss 3.1848e-02 (5.9538e-02)\tAcc@1  99.22 ( 98.55)\tAcc@5 100.00 ( 99.92)\n","Epoch: [123][150/391]\tTime  0.093 ( 0.094)\tLoss 3.5380e-02 (5.9891e-02)\tAcc@1  99.22 ( 98.54)\tAcc@5 100.00 ( 99.93)\n","Epoch: [123][180/391]\tTime  0.095 ( 0.094)\tLoss 7.5708e-02 (6.0415e-02)\tAcc@1  97.66 ( 98.49)\tAcc@5 100.00 ( 99.93)\n","Epoch: [123][210/391]\tTime  0.091 ( 0.094)\tLoss 4.5361e-02 (6.0079e-02)\tAcc@1  98.44 ( 98.50)\tAcc@5 100.00 ( 99.93)\n","Epoch: [123][240/391]\tTime  0.089 ( 0.094)\tLoss 9.7376e-02 (6.0554e-02)\tAcc@1  98.44 ( 98.50)\tAcc@5  99.22 ( 99.93)\n","Epoch: [123][270/391]\tTime  0.090 ( 0.094)\tLoss 3.5794e-02 (6.0264e-02)\tAcc@1 100.00 ( 98.53)\tAcc@5 100.00 ( 99.93)\n","Epoch: [123][300/391]\tTime  0.098 ( 0.093)\tLoss 4.0097e-02 (6.0444e-02)\tAcc@1 100.00 ( 98.53)\tAcc@5 100.00 ( 99.93)\n","Epoch: [123][330/391]\tTime  0.089 ( 0.093)\tLoss 1.0871e-01 (6.0485e-02)\tAcc@1  97.66 ( 98.52)\tAcc@5  99.22 ( 99.93)\n","Epoch: [123][360/391]\tTime  0.092 ( 0.093)\tLoss 5.9250e-02 (6.0326e-02)\tAcc@1  97.66 ( 98.52)\tAcc@5 100.00 ( 99.93)\n","Epoch: [123][390/391]\tTime  0.082 ( 0.093)\tLoss 6.7976e-02 (6.0469e-02)\tAcc@1  98.75 ( 98.54)\tAcc@5 100.00 ( 99.92)\n","==> Train Accuracy: Acc@1 98.542 || Acc@5 99.924\n","==> Test Accuracy:  Acc@1 77.780 || Acc@5 94.210\n","==> 39.14 seconds to train this epoch\n","\n","\n","----- epoch: 124, lr: 0.0008000000000000003 -----\n","Epoch: [124][  0/391]\tTime  0.288 ( 0.288)\tLoss 6.2191e-02 (6.2191e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5 100.00 (100.00)\n","Epoch: [124][ 30/391]\tTime  0.088 ( 0.100)\tLoss 6.9380e-02 (5.9432e-02)\tAcc@1  96.88 ( 98.51)\tAcc@5 100.00 ( 99.97)\n","Epoch: [124][ 60/391]\tTime  0.094 ( 0.096)\tLoss 5.0510e-02 (6.0337e-02)\tAcc@1  99.22 ( 98.44)\tAcc@5 100.00 ( 99.96)\n","Epoch: [124][ 90/391]\tTime  0.093 ( 0.095)\tLoss 4.4721e-02 (6.1504e-02)\tAcc@1  99.22 ( 98.35)\tAcc@5 100.00 ( 99.92)\n","Epoch: [124][120/391]\tTime  0.092 ( 0.094)\tLoss 2.5416e-02 (6.2832e-02)\tAcc@1 100.00 ( 98.34)\tAcc@5 100.00 ( 99.92)\n","Epoch: [124][150/391]\tTime  0.095 ( 0.094)\tLoss 2.5268e-02 (6.3173e-02)\tAcc@1 100.00 ( 98.32)\tAcc@5 100.00 ( 99.91)\n","Epoch: [124][180/391]\tTime  0.092 ( 0.094)\tLoss 5.9773e-02 (6.2320e-02)\tAcc@1  97.66 ( 98.35)\tAcc@5 100.00 ( 99.91)\n","Epoch: [124][210/391]\tTime  0.092 ( 0.093)\tLoss 6.9812e-02 (6.1582e-02)\tAcc@1  97.66 ( 98.39)\tAcc@5 100.00 ( 99.91)\n","Epoch: [124][240/391]\tTime  0.090 ( 0.093)\tLoss 3.5179e-02 (6.0325e-02)\tAcc@1  99.22 ( 98.43)\tAcc@5 100.00 ( 99.92)\n","Epoch: [124][270/391]\tTime  0.090 ( 0.093)\tLoss 8.0995e-02 (6.0761e-02)\tAcc@1  96.88 ( 98.45)\tAcc@5 100.00 ( 99.91)\n","Epoch: [124][300/391]\tTime  0.094 ( 0.093)\tLoss 4.5163e-02 (5.9852e-02)\tAcc@1  99.22 ( 98.48)\tAcc@5 100.00 ( 99.91)\n","Epoch: [124][330/391]\tTime  0.093 ( 0.093)\tLoss 3.7661e-02 (5.9342e-02)\tAcc@1  99.22 ( 98.52)\tAcc@5 100.00 ( 99.91)\n","Epoch: [124][360/391]\tTime  0.097 ( 0.093)\tLoss 9.3913e-02 (5.8836e-02)\tAcc@1  96.88 ( 98.54)\tAcc@5 100.00 ( 99.92)\n","Epoch: [124][390/391]\tTime  0.082 ( 0.093)\tLoss 4.6237e-02 (5.8512e-02)\tAcc@1 100.00 ( 98.58)\tAcc@5 100.00 ( 99.91)\n","==> Train Accuracy: Acc@1 98.576 || Acc@5 99.912\n","==> Test Accuracy:  Acc@1 77.520 || Acc@5 94.100\n","==> 39.03 seconds to train this epoch\n","\n","\n","----- epoch: 125, lr: 0.0008000000000000003 -----\n","Epoch: [125][  0/391]\tTime  0.271 ( 0.271)\tLoss 7.7193e-02 (7.7193e-02)\tAcc@1  97.66 ( 97.66)\tAcc@5 100.00 (100.00)\n","Epoch: [125][ 30/391]\tTime  0.091 ( 0.098)\tLoss 6.8481e-02 (5.6965e-02)\tAcc@1  97.66 ( 98.46)\tAcc@5 100.00 ( 99.95)\n","Epoch: [125][ 60/391]\tTime  0.093 ( 0.095)\tLoss 9.6936e-02 (5.8705e-02)\tAcc@1  98.44 ( 98.49)\tAcc@5 100.00 ( 99.94)\n","Epoch: [125][ 90/391]\tTime  0.092 ( 0.094)\tLoss 4.9364e-02 (5.7572e-02)\tAcc@1  98.44 ( 98.51)\tAcc@5 100.00 ( 99.95)\n","Epoch: [125][120/391]\tTime  0.096 ( 0.094)\tLoss 5.5851e-02 (5.8632e-02)\tAcc@1 100.00 ( 98.51)\tAcc@5 100.00 ( 99.92)\n","Epoch: [125][150/391]\tTime  0.100 ( 0.093)\tLoss 3.3993e-02 (5.9594e-02)\tAcc@1  99.22 ( 98.51)\tAcc@5 100.00 ( 99.91)\n","Epoch: [125][180/391]\tTime  0.093 ( 0.093)\tLoss 2.9613e-02 (5.8639e-02)\tAcc@1  99.22 ( 98.57)\tAcc@5 100.00 ( 99.92)\n","Epoch: [125][210/391]\tTime  0.087 ( 0.093)\tLoss 9.0422e-02 (5.8260e-02)\tAcc@1  97.66 ( 98.60)\tAcc@5  99.22 ( 99.91)\n","Epoch: [125][240/391]\tTime  0.095 ( 0.093)\tLoss 5.7583e-02 (5.7580e-02)\tAcc@1  98.44 ( 98.61)\tAcc@5 100.00 ( 99.92)\n","Epoch: [125][270/391]\tTime  0.094 ( 0.093)\tLoss 4.5162e-02 (5.7545e-02)\tAcc@1  99.22 ( 98.61)\tAcc@5 100.00 ( 99.93)\n","Epoch: [125][300/391]\tTime  0.092 ( 0.093)\tLoss 5.2973e-02 (5.6953e-02)\tAcc@1  98.44 ( 98.65)\tAcc@5 100.00 ( 99.93)\n","Epoch: [125][330/391]\tTime  0.091 ( 0.093)\tLoss 5.5158e-02 (5.6857e-02)\tAcc@1  98.44 ( 98.66)\tAcc@5 100.00 ( 99.93)\n","Epoch: [125][360/391]\tTime  0.093 ( 0.093)\tLoss 7.2945e-02 (5.7296e-02)\tAcc@1  98.44 ( 98.63)\tAcc@5 100.00 ( 99.94)\n","Epoch: [125][390/391]\tTime  0.081 ( 0.093)\tLoss 6.2445e-02 (5.7221e-02)\tAcc@1  96.25 ( 98.64)\tAcc@5 100.00 ( 99.94)\n","==> Train Accuracy: Acc@1 98.642 || Acc@5 99.938\n","==> Test Accuracy:  Acc@1 77.770 || Acc@5 94.120\n","==> 38.98 seconds to train this epoch\n","\n","\n","----- epoch: 126, lr: 0.0008000000000000003 -----\n","Epoch: [126][  0/391]\tTime  0.277 ( 0.277)\tLoss 5.0016e-02 (5.0016e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5 100.00 (100.00)\n","Epoch: [126][ 30/391]\tTime  0.092 ( 0.099)\tLoss 7.7579e-02 (5.6813e-02)\tAcc@1  96.88 ( 98.84)\tAcc@5 100.00 ( 99.87)\n","Epoch: [126][ 60/391]\tTime  0.089 ( 0.096)\tLoss 6.1198e-02 (5.7750e-02)\tAcc@1  99.22 ( 98.64)\tAcc@5  99.22 ( 99.87)\n","Epoch: [126][ 90/391]\tTime  0.086 ( 0.095)\tLoss 8.3869e-02 (5.8524e-02)\tAcc@1  98.44 ( 98.60)\tAcc@5 100.00 ( 99.89)\n","Epoch: [126][120/391]\tTime  0.092 ( 0.094)\tLoss 1.0028e-01 (5.5890e-02)\tAcc@1  97.66 ( 98.70)\tAcc@5  99.22 ( 99.91)\n","Epoch: [126][150/391]\tTime  0.090 ( 0.094)\tLoss 1.2258e-01 (5.6660e-02)\tAcc@1  96.88 ( 98.68)\tAcc@5 100.00 ( 99.92)\n","Epoch: [126][180/391]\tTime  0.091 ( 0.093)\tLoss 5.2569e-02 (5.5785e-02)\tAcc@1  99.22 ( 98.71)\tAcc@5  99.22 ( 99.91)\n","Epoch: [126][210/391]\tTime  0.094 ( 0.093)\tLoss 5.5076e-02 (5.6283e-02)\tAcc@1  99.22 ( 98.68)\tAcc@5  99.22 ( 99.91)\n","Epoch: [126][240/391]\tTime  0.093 ( 0.093)\tLoss 3.8238e-02 (5.6260e-02)\tAcc@1  99.22 ( 98.66)\tAcc@5 100.00 ( 99.92)\n","Epoch: [126][270/391]\tTime  0.094 ( 0.093)\tLoss 1.2457e-01 (5.6964e-02)\tAcc@1  95.31 ( 98.65)\tAcc@5  99.22 ( 99.91)\n","Epoch: [126][300/391]\tTime  0.091 ( 0.093)\tLoss 8.5782e-02 (5.6466e-02)\tAcc@1  99.22 ( 98.66)\tAcc@5  99.22 ( 99.91)\n","Epoch: [126][330/391]\tTime  0.087 ( 0.093)\tLoss 3.3912e-02 (5.5890e-02)\tAcc@1  99.22 ( 98.69)\tAcc@5 100.00 ( 99.92)\n","Epoch: [126][360/391]\tTime  0.089 ( 0.093)\tLoss 7.0490e-02 (5.6120e-02)\tAcc@1  97.66 ( 98.69)\tAcc@5 100.00 ( 99.92)\n","Epoch: [126][390/391]\tTime  0.083 ( 0.093)\tLoss 4.1090e-02 (5.6265e-02)\tAcc@1 100.00 ( 98.67)\tAcc@5 100.00 ( 99.93)\n","==> Train Accuracy: Acc@1 98.674 || Acc@5 99.928\n","==> Test Accuracy:  Acc@1 77.680 || Acc@5 94.060\n","==> 38.97 seconds to train this epoch\n","\n","\n","----- epoch: 127, lr: 0.0008000000000000003 -----\n","Epoch: [127][  0/391]\tTime  0.276 ( 0.276)\tLoss 2.8773e-02 (2.8773e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5 100.00 (100.00)\n","Epoch: [127][ 30/391]\tTime  0.096 ( 0.100)\tLoss 1.2315e-01 (5.6823e-02)\tAcc@1  96.88 ( 98.59)\tAcc@5  98.44 ( 99.92)\n","Epoch: [127][ 60/391]\tTime  0.091 ( 0.096)\tLoss 4.0723e-02 (5.6806e-02)\tAcc@1  99.22 ( 98.62)\tAcc@5 100.00 ( 99.96)\n","Epoch: [127][ 90/391]\tTime  0.092 ( 0.095)\tLoss 5.7239e-02 (5.2683e-02)\tAcc@1  98.44 ( 98.78)\tAcc@5 100.00 ( 99.97)\n","Epoch: [127][120/391]\tTime  0.098 ( 0.094)\tLoss 3.3089e-02 (5.4643e-02)\tAcc@1  99.22 ( 98.75)\tAcc@5 100.00 ( 99.96)\n","Epoch: [127][150/391]\tTime  0.092 ( 0.094)\tLoss 1.2592e-02 (5.5018e-02)\tAcc@1 100.00 ( 98.75)\tAcc@5 100.00 ( 99.96)\n","Epoch: [127][180/391]\tTime  0.093 ( 0.093)\tLoss 8.3193e-02 (5.5775e-02)\tAcc@1  96.88 ( 98.73)\tAcc@5 100.00 ( 99.95)\n","Epoch: [127][210/391]\tTime  0.098 ( 0.093)\tLoss 5.2557e-02 (5.5781e-02)\tAcc@1  98.44 ( 98.72)\tAcc@5 100.00 ( 99.95)\n","Epoch: [127][240/391]\tTime  0.092 ( 0.093)\tLoss 6.5161e-02 (5.5624e-02)\tAcc@1  98.44 ( 98.72)\tAcc@5 100.00 ( 99.95)\n","Epoch: [127][270/391]\tTime  0.090 ( 0.093)\tLoss 2.1982e-02 (5.4926e-02)\tAcc@1 100.00 ( 98.73)\tAcc@5 100.00 ( 99.95)\n","Epoch: [127][300/391]\tTime  0.090 ( 0.093)\tLoss 4.5663e-02 (5.5035e-02)\tAcc@1  99.22 ( 98.73)\tAcc@5 100.00 ( 99.94)\n","Epoch: [127][330/391]\tTime  0.094 ( 0.093)\tLoss 6.3279e-02 (5.5554e-02)\tAcc@1  97.66 ( 98.71)\tAcc@5 100.00 ( 99.94)\n","Epoch: [127][360/391]\tTime  0.105 ( 0.093)\tLoss 1.4352e-01 (5.5806e-02)\tAcc@1  95.31 ( 98.71)\tAcc@5 100.00 ( 99.95)\n","Epoch: [127][390/391]\tTime  0.078 ( 0.093)\tLoss 3.5794e-02 (5.5973e-02)\tAcc@1 100.00 ( 98.70)\tAcc@5 100.00 ( 99.95)\n","==> Train Accuracy: Acc@1 98.704 || Acc@5 99.948\n","==> Test Accuracy:  Acc@1 77.740 || Acc@5 94.120\n","==> 39.00 seconds to train this epoch\n","\n","\n","----- epoch: 128, lr: 0.0008000000000000003 -----\n","Epoch: [128][  0/391]\tTime  0.277 ( 0.277)\tLoss 8.5441e-02 (8.5441e-02)\tAcc@1  98.44 ( 98.44)\tAcc@5 100.00 (100.00)\n","Epoch: [128][ 30/391]\tTime  0.091 ( 0.100)\tLoss 2.6003e-02 (4.9105e-02)\tAcc@1 100.00 ( 98.94)\tAcc@5 100.00 ( 99.90)\n","Epoch: [128][ 60/391]\tTime  0.090 ( 0.096)\tLoss 5.7644e-02 (5.1665e-02)\tAcc@1  99.22 ( 98.80)\tAcc@5 100.00 ( 99.95)\n","Epoch: [128][ 90/391]\tTime  0.087 ( 0.095)\tLoss 3.6740e-02 (5.4120e-02)\tAcc@1  99.22 ( 98.68)\tAcc@5 100.00 ( 99.96)\n","Epoch: [128][120/391]\tTime  0.089 ( 0.094)\tLoss 6.8421e-02 (5.4019e-02)\tAcc@1  97.66 ( 98.65)\tAcc@5  99.22 ( 99.95)\n","Epoch: [128][150/391]\tTime  0.075 ( 0.094)\tLoss 2.7794e-02 (5.4027e-02)\tAcc@1 100.00 ( 98.65)\tAcc@5 100.00 ( 99.95)\n","Epoch: [128][180/391]\tTime  0.097 ( 0.094)\tLoss 1.3361e-02 (5.3397e-02)\tAcc@1 100.00 ( 98.71)\tAcc@5 100.00 ( 99.96)\n","Epoch: [128][210/391]\tTime  0.091 ( 0.094)\tLoss 7.2144e-02 (5.3765e-02)\tAcc@1  98.44 ( 98.72)\tAcc@5 100.00 ( 99.95)\n","Epoch: [128][240/391]\tTime  0.091 ( 0.094)\tLoss 5.9238e-02 (5.5102e-02)\tAcc@1  97.66 ( 98.69)\tAcc@5 100.00 ( 99.94)\n","Epoch: [128][270/391]\tTime  0.093 ( 0.094)\tLoss 4.6716e-02 (5.5622e-02)\tAcc@1  99.22 ( 98.71)\tAcc@5 100.00 ( 99.94)\n","Epoch: [128][300/391]\tTime  0.091 ( 0.093)\tLoss 2.5207e-02 (5.6206e-02)\tAcc@1 100.00 ( 98.69)\tAcc@5 100.00 ( 99.94)\n","Epoch: [128][330/391]\tTime  0.094 ( 0.093)\tLoss 5.2240e-02 (5.5559e-02)\tAcc@1  98.44 ( 98.70)\tAcc@5 100.00 ( 99.94)\n","Epoch: [128][360/391]\tTime  0.090 ( 0.093)\tLoss 7.3266e-02 (5.5595e-02)\tAcc@1  95.31 ( 98.70)\tAcc@5 100.00 ( 99.94)\n","Epoch: [128][390/391]\tTime  0.083 ( 0.093)\tLoss 4.4443e-02 (5.5197e-02)\tAcc@1 100.00 ( 98.71)\tAcc@5 100.00 ( 99.94)\n","==> Train Accuracy: Acc@1 98.714 || Acc@5 99.938\n","==> Test Accuracy:  Acc@1 78.000 || Acc@5 94.110\n","==> 39.05 seconds to train this epoch\n","\n","\n","----- epoch: 129, lr: 0.0008000000000000003 -----\n","Epoch: [129][  0/391]\tTime  0.277 ( 0.277)\tLoss 1.0771e-01 (1.0771e-01)\tAcc@1  98.44 ( 98.44)\tAcc@5 100.00 (100.00)\n","Epoch: [129][ 30/391]\tTime  0.092 ( 0.101)\tLoss 6.5688e-02 (5.5658e-02)\tAcc@1  98.44 ( 98.54)\tAcc@5 100.00 ( 99.97)\n","Epoch: [129][ 60/391]\tTime  0.105 ( 0.096)\tLoss 9.2481e-02 (5.7983e-02)\tAcc@1  97.66 ( 98.41)\tAcc@5 100.00 ( 99.94)\n","Epoch: [129][ 90/391]\tTime  0.101 ( 0.095)\tLoss 1.9275e-02 (5.6581e-02)\tAcc@1 100.00 ( 98.50)\tAcc@5 100.00 ( 99.93)\n","Epoch: [129][120/391]\tTime  0.084 ( 0.094)\tLoss 6.0494e-02 (5.5169e-02)\tAcc@1  98.44 ( 98.61)\tAcc@5 100.00 ( 99.94)\n","Epoch: [129][150/391]\tTime  0.091 ( 0.094)\tLoss 2.2681e-02 (5.3571e-02)\tAcc@1  99.22 ( 98.69)\tAcc@5 100.00 ( 99.94)\n","Epoch: [129][180/391]\tTime  0.094 ( 0.094)\tLoss 9.2101e-02 (5.3001e-02)\tAcc@1  96.88 ( 98.71)\tAcc@5 100.00 ( 99.94)\n","Epoch: [129][210/391]\tTime  0.090 ( 0.094)\tLoss 6.4498e-02 (5.2574e-02)\tAcc@1  98.44 ( 98.76)\tAcc@5 100.00 ( 99.94)\n","Epoch: [129][240/391]\tTime  0.092 ( 0.094)\tLoss 9.4357e-02 (5.3033e-02)\tAcc@1  96.88 ( 98.75)\tAcc@5 100.00 ( 99.94)\n","Epoch: [129][270/391]\tTime  0.095 ( 0.093)\tLoss 5.7893e-02 (5.3663e-02)\tAcc@1  97.66 ( 98.74)\tAcc@5 100.00 ( 99.94)\n","Epoch: [129][300/391]\tTime  0.094 ( 0.093)\tLoss 3.5130e-02 (5.3242e-02)\tAcc@1 100.00 ( 98.76)\tAcc@5 100.00 ( 99.95)\n","Epoch: [129][330/391]\tTime  0.093 ( 0.093)\tLoss 5.5438e-02 (5.3737e-02)\tAcc@1  98.44 ( 98.76)\tAcc@5 100.00 ( 99.95)\n","Epoch: [129][360/391]\tTime  0.091 ( 0.093)\tLoss 1.1875e-01 (5.3916e-02)\tAcc@1  96.88 ( 98.75)\tAcc@5  99.22 ( 99.94)\n","Epoch: [129][390/391]\tTime  0.082 ( 0.093)\tLoss 5.7457e-02 (5.3547e-02)\tAcc@1  97.50 ( 98.76)\tAcc@5 100.00 ( 99.94)\n","==> Train Accuracy: Acc@1 98.762 || Acc@5 99.940\n","==> Test Accuracy:  Acc@1 77.850 || Acc@5 94.120\n","==> 39.13 seconds to train this epoch\n","\n","\n","----- epoch: 130, lr: 0.0008000000000000003 -----\n","Epoch: [130][  0/391]\tTime  0.294 ( 0.294)\tLoss 1.0171e-01 (1.0171e-01)\tAcc@1  96.09 ( 96.09)\tAcc@5 100.00 (100.00)\n","Epoch: [130][ 30/391]\tTime  0.090 ( 0.099)\tLoss 2.6751e-02 (5.5351e-02)\tAcc@1 100.00 ( 98.66)\tAcc@5 100.00 ( 99.95)\n","Epoch: [130][ 60/391]\tTime  0.091 ( 0.096)\tLoss 4.1457e-02 (5.3150e-02)\tAcc@1  98.44 ( 98.69)\tAcc@5 100.00 ( 99.95)\n","Epoch: [130][ 90/391]\tTime  0.092 ( 0.095)\tLoss 4.4275e-02 (5.3306e-02)\tAcc@1  99.22 ( 98.80)\tAcc@5 100.00 ( 99.94)\n","Epoch: [130][120/391]\tTime  0.090 ( 0.094)\tLoss 7.6049e-02 (5.3146e-02)\tAcc@1  97.66 ( 98.79)\tAcc@5 100.00 ( 99.92)\n","Epoch: [130][150/391]\tTime  0.093 ( 0.094)\tLoss 9.9014e-02 (5.2817e-02)\tAcc@1  97.66 ( 98.72)\tAcc@5  99.22 ( 99.92)\n","Epoch: [130][180/391]\tTime  0.092 ( 0.094)\tLoss 1.4358e-02 (5.2748e-02)\tAcc@1 100.00 ( 98.73)\tAcc@5 100.00 ( 99.92)\n","Epoch: [130][210/391]\tTime  0.090 ( 0.093)\tLoss 5.4409e-02 (5.2999e-02)\tAcc@1  99.22 ( 98.72)\tAcc@5 100.00 ( 99.91)\n","Epoch: [130][240/391]\tTime  0.093 ( 0.093)\tLoss 4.1103e-02 (5.3460e-02)\tAcc@1 100.00 ( 98.70)\tAcc@5 100.00 ( 99.91)\n","Epoch: [130][270/391]\tTime  0.090 ( 0.093)\tLoss 3.4473e-02 (5.3395e-02)\tAcc@1 100.00 ( 98.67)\tAcc@5 100.00 ( 99.92)\n","Epoch: [130][300/391]\tTime  0.087 ( 0.093)\tLoss 5.2921e-02 (5.3209e-02)\tAcc@1  98.44 ( 98.69)\tAcc@5 100.00 ( 99.92)\n","Epoch: [130][330/391]\tTime  0.102 ( 0.093)\tLoss 4.4350e-02 (5.2716e-02)\tAcc@1  98.44 ( 98.71)\tAcc@5 100.00 ( 99.92)\n","Epoch: [130][360/391]\tTime  0.093 ( 0.093)\tLoss 3.8998e-02 (5.2725e-02)\tAcc@1  98.44 ( 98.70)\tAcc@5 100.00 ( 99.92)\n","Epoch: [130][390/391]\tTime  0.081 ( 0.093)\tLoss 5.3392e-02 (5.2305e-02)\tAcc@1  98.75 ( 98.72)\tAcc@5 100.00 ( 99.92)\n","==> Train Accuracy: Acc@1 98.722 || Acc@5 99.924\n","==> Test Accuracy:  Acc@1 77.580 || Acc@5 94.200\n","==> 39.01 seconds to train this epoch\n","\n","\n","----- epoch: 131, lr: 0.0008000000000000003 -----\n","Epoch: [131][  0/391]\tTime  0.293 ( 0.293)\tLoss 3.0843e-02 (3.0843e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [131][ 30/391]\tTime  0.100 ( 0.100)\tLoss 1.6711e-02 (5.4422e-02)\tAcc@1 100.00 ( 98.66)\tAcc@5 100.00 ( 99.90)\n","Epoch: [131][ 60/391]\tTime  0.092 ( 0.096)\tLoss 6.1617e-02 (5.2799e-02)\tAcc@1  98.44 ( 98.69)\tAcc@5 100.00 ( 99.94)\n","Epoch: [131][ 90/391]\tTime  0.089 ( 0.095)\tLoss 1.4618e-01 (5.4181e-02)\tAcc@1  96.88 ( 98.75)\tAcc@5 100.00 ( 99.93)\n","Epoch: [131][120/391]\tTime  0.099 ( 0.094)\tLoss 3.0118e-02 (5.2775e-02)\tAcc@1  98.44 ( 98.76)\tAcc@5 100.00 ( 99.94)\n","Epoch: [131][150/391]\tTime  0.091 ( 0.094)\tLoss 4.4845e-02 (5.3325e-02)\tAcc@1  98.44 ( 98.75)\tAcc@5 100.00 ( 99.92)\n","Epoch: [131][180/391]\tTime  0.096 ( 0.094)\tLoss 1.9969e-02 (5.3003e-02)\tAcc@1 100.00 ( 98.74)\tAcc@5 100.00 ( 99.93)\n","Epoch: [131][210/391]\tTime  0.093 ( 0.094)\tLoss 7.7256e-02 (5.3507e-02)\tAcc@1  97.66 ( 98.69)\tAcc@5 100.00 ( 99.92)\n","Epoch: [131][240/391]\tTime  0.083 ( 0.093)\tLoss 2.5025e-02 (5.2488e-02)\tAcc@1 100.00 ( 98.75)\tAcc@5 100.00 ( 99.93)\n","Epoch: [131][270/391]\tTime  0.098 ( 0.093)\tLoss 2.2382e-02 (5.2316e-02)\tAcc@1 100.00 ( 98.75)\tAcc@5 100.00 ( 99.93)\n","Epoch: [131][300/391]\tTime  0.096 ( 0.093)\tLoss 3.4465e-02 (5.2944e-02)\tAcc@1  98.44 ( 98.73)\tAcc@5 100.00 ( 99.93)\n","Epoch: [131][330/391]\tTime  0.095 ( 0.093)\tLoss 4.1611e-02 (5.2387e-02)\tAcc@1  99.22 ( 98.75)\tAcc@5 100.00 ( 99.93)\n","Epoch: [131][360/391]\tTime  0.096 ( 0.093)\tLoss 6.0355e-02 (5.2605e-02)\tAcc@1  98.44 ( 98.75)\tAcc@5 100.00 ( 99.93)\n","Epoch: [131][390/391]\tTime  0.081 ( 0.093)\tLoss 3.0969e-02 (5.2742e-02)\tAcc@1 100.00 ( 98.74)\tAcc@5 100.00 ( 99.93)\n","==> Train Accuracy: Acc@1 98.742 || Acc@5 99.934\n","==> Test Accuracy:  Acc@1 77.580 || Acc@5 94.270\n","==> 39.06 seconds to train this epoch\n","\n","\n","----- epoch: 132, lr: 0.0008000000000000003 -----\n","Epoch: [132][  0/391]\tTime  0.272 ( 0.272)\tLoss 5.5146e-02 (5.5146e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5 100.00 (100.00)\n","Epoch: [132][ 30/391]\tTime  0.091 ( 0.098)\tLoss 1.0801e-01 (5.3229e-02)\tAcc@1  99.22 ( 99.07)\tAcc@5 100.00 ( 99.97)\n","Epoch: [132][ 60/391]\tTime  0.091 ( 0.096)\tLoss 4.6683e-02 (5.0583e-02)\tAcc@1  98.44 ( 98.94)\tAcc@5 100.00 ( 99.97)\n","Epoch: [132][ 90/391]\tTime  0.096 ( 0.095)\tLoss 4.3745e-02 (5.3000e-02)\tAcc@1  98.44 ( 98.88)\tAcc@5 100.00 ( 99.95)\n","Epoch: [132][120/391]\tTime  0.095 ( 0.094)\tLoss 6.5625e-02 (5.1259e-02)\tAcc@1  98.44 ( 98.91)\tAcc@5 100.00 ( 99.95)\n","Epoch: [132][150/391]\tTime  0.096 ( 0.094)\tLoss 4.9489e-02 (5.1068e-02)\tAcc@1  98.44 ( 98.86)\tAcc@5 100.00 ( 99.95)\n","Epoch: [132][180/391]\tTime  0.095 ( 0.094)\tLoss 7.1663e-02 (5.0364e-02)\tAcc@1  97.66 ( 98.87)\tAcc@5 100.00 ( 99.96)\n","Epoch: [132][210/391]\tTime  0.097 ( 0.093)\tLoss 5.0963e-02 (5.0301e-02)\tAcc@1 100.00 ( 98.87)\tAcc@5 100.00 ( 99.96)\n","Epoch: [132][240/391]\tTime  0.093 ( 0.093)\tLoss 4.0166e-02 (4.9729e-02)\tAcc@1  99.22 ( 98.87)\tAcc@5 100.00 ( 99.96)\n","Epoch: [132][270/391]\tTime  0.088 ( 0.093)\tLoss 5.4444e-02 (4.9076e-02)\tAcc@1  98.44 ( 98.89)\tAcc@5 100.00 ( 99.97)\n","Epoch: [132][300/391]\tTime  0.091 ( 0.093)\tLoss 8.0817e-02 (4.9938e-02)\tAcc@1  96.88 ( 98.85)\tAcc@5 100.00 ( 99.96)\n","Epoch: [132][330/391]\tTime  0.092 ( 0.093)\tLoss 1.9638e-02 (4.9541e-02)\tAcc@1 100.00 ( 98.87)\tAcc@5 100.00 ( 99.96)\n","Epoch: [132][360/391]\tTime  0.087 ( 0.093)\tLoss 6.5759e-02 (4.9760e-02)\tAcc@1  98.44 ( 98.85)\tAcc@5 100.00 ( 99.97)\n","Epoch: [132][390/391]\tTime  0.082 ( 0.093)\tLoss 6.8039e-02 (4.9729e-02)\tAcc@1  98.75 ( 98.84)\tAcc@5 100.00 ( 99.97)\n","==> Train Accuracy: Acc@1 98.842 || Acc@5 99.966\n","==> Test Accuracy:  Acc@1 77.620 || Acc@5 94.230\n","==> 39.06 seconds to train this epoch\n","\n","\n","----- epoch: 133, lr: 0.0008000000000000003 -----\n","Epoch: [133][  0/391]\tTime  0.273 ( 0.273)\tLoss 7.7393e-02 (7.7393e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5  99.22 ( 99.22)\n","Epoch: [133][ 30/391]\tTime  0.096 ( 0.100)\tLoss 5.9236e-02 (5.3308e-02)\tAcc@1  99.22 ( 98.87)\tAcc@5 100.00 ( 99.95)\n","Epoch: [133][ 60/391]\tTime  0.094 ( 0.096)\tLoss 3.1030e-02 (5.2750e-02)\tAcc@1 100.00 ( 98.86)\tAcc@5 100.00 ( 99.95)\n","Epoch: [133][ 90/391]\tTime  0.095 ( 0.095)\tLoss 4.7472e-02 (5.1142e-02)\tAcc@1  97.66 ( 98.87)\tAcc@5 100.00 ( 99.96)\n","Epoch: [133][120/391]\tTime  0.091 ( 0.094)\tLoss 4.0433e-02 (5.0873e-02)\tAcc@1  99.22 ( 98.86)\tAcc@5 100.00 ( 99.95)\n","Epoch: [133][150/391]\tTime  0.092 ( 0.094)\tLoss 3.1659e-02 (5.1049e-02)\tAcc@1 100.00 ( 98.87)\tAcc@5 100.00 ( 99.96)\n","Epoch: [133][180/391]\tTime  0.093 ( 0.094)\tLoss 8.6293e-02 (5.1323e-02)\tAcc@1  97.66 ( 98.86)\tAcc@5 100.00 ( 99.95)\n","Epoch: [133][210/391]\tTime  0.091 ( 0.093)\tLoss 4.4668e-02 (5.2542e-02)\tAcc@1  98.44 ( 98.80)\tAcc@5 100.00 ( 99.94)\n","Epoch: [133][240/391]\tTime  0.091 ( 0.093)\tLoss 3.5621e-02 (5.1708e-02)\tAcc@1  99.22 ( 98.83)\tAcc@5 100.00 ( 99.95)\n","Epoch: [133][270/391]\tTime  0.094 ( 0.093)\tLoss 2.1591e-02 (5.1465e-02)\tAcc@1 100.00 ( 98.84)\tAcc@5 100.00 ( 99.95)\n","Epoch: [133][300/391]\tTime  0.095 ( 0.093)\tLoss 2.7891e-02 (5.1867e-02)\tAcc@1  99.22 ( 98.82)\tAcc@5 100.00 ( 99.95)\n","Epoch: [133][330/391]\tTime  0.092 ( 0.093)\tLoss 4.6165e-02 (5.1865e-02)\tAcc@1  99.22 ( 98.80)\tAcc@5 100.00 ( 99.96)\n","Epoch: [133][360/391]\tTime  0.093 ( 0.093)\tLoss 5.7699e-02 (5.1615e-02)\tAcc@1  99.22 ( 98.82)\tAcc@5 100.00 ( 99.96)\n","Epoch: [133][390/391]\tTime  0.082 ( 0.093)\tLoss 5.7901e-02 (5.2065e-02)\tAcc@1  98.75 ( 98.80)\tAcc@5 100.00 ( 99.96)\n","==> Train Accuracy: Acc@1 98.798 || Acc@5 99.958\n","==> Test Accuracy:  Acc@1 77.740 || Acc@5 94.260\n","==> 39.15 seconds to train this epoch\n","\n","\n","----- epoch: 134, lr: 0.0008000000000000003 -----\n","Epoch: [134][  0/391]\tTime  0.275 ( 0.275)\tLoss 5.4292e-02 (5.4292e-02)\tAcc@1  98.44 ( 98.44)\tAcc@5 100.00 (100.00)\n","Epoch: [134][ 30/391]\tTime  0.097 ( 0.099)\tLoss 2.6007e-02 (4.9485e-02)\tAcc@1 100.00 ( 98.79)\tAcc@5 100.00 (100.00)\n","Epoch: [134][ 60/391]\tTime  0.088 ( 0.096)\tLoss 3.1374e-02 (4.9265e-02)\tAcc@1  99.22 ( 98.83)\tAcc@5 100.00 ( 99.95)\n","Epoch: [134][ 90/391]\tTime  0.096 ( 0.095)\tLoss 6.4624e-02 (4.9076e-02)\tAcc@1  98.44 ( 98.88)\tAcc@5  99.22 ( 99.96)\n","Epoch: [134][120/391]\tTime  0.096 ( 0.094)\tLoss 3.3703e-02 (4.8442e-02)\tAcc@1  99.22 ( 98.92)\tAcc@5 100.00 ( 99.95)\n","Epoch: [134][150/391]\tTime  0.091 ( 0.094)\tLoss 5.1421e-02 (4.9180e-02)\tAcc@1  97.66 ( 98.88)\tAcc@5 100.00 ( 99.95)\n","Epoch: [134][180/391]\tTime  0.094 ( 0.094)\tLoss 1.9005e-02 (4.8720e-02)\tAcc@1 100.00 ( 98.91)\tAcc@5 100.00 ( 99.95)\n","Epoch: [134][210/391]\tTime  0.092 ( 0.093)\tLoss 3.3426e-02 (4.8560e-02)\tAcc@1  99.22 ( 98.92)\tAcc@5 100.00 ( 99.96)\n","Epoch: [134][240/391]\tTime  0.091 ( 0.093)\tLoss 5.6092e-02 (4.8468e-02)\tAcc@1  99.22 ( 98.93)\tAcc@5 100.00 ( 99.95)\n","Epoch: [134][270/391]\tTime  0.091 ( 0.093)\tLoss 5.9146e-02 (4.8325e-02)\tAcc@1  99.22 ( 98.93)\tAcc@5  99.22 ( 99.95)\n","Epoch: [134][300/391]\tTime  0.088 ( 0.093)\tLoss 3.3225e-02 (4.8146e-02)\tAcc@1  99.22 ( 98.93)\tAcc@5 100.00 ( 99.95)\n","Epoch: [134][330/391]\tTime  0.089 ( 0.093)\tLoss 3.9209e-02 (4.8600e-02)\tAcc@1  98.44 ( 98.90)\tAcc@5 100.00 ( 99.95)\n","Epoch: [134][360/391]\tTime  0.093 ( 0.093)\tLoss 8.0182e-02 (4.9476e-02)\tAcc@1  96.88 ( 98.88)\tAcc@5 100.00 ( 99.95)\n","Epoch: [134][390/391]\tTime  0.082 ( 0.093)\tLoss 1.1860e-01 (5.0067e-02)\tAcc@1  96.25 ( 98.85)\tAcc@5  98.75 ( 99.94)\n","==> Train Accuracy: Acc@1 98.850 || Acc@5 99.942\n","==> Test Accuracy:  Acc@1 77.860 || Acc@5 94.150\n","==> 39.03 seconds to train this epoch\n","\n","\n","----- epoch: 135, lr: 0.0008000000000000003 -----\n","Epoch: [135][  0/391]\tTime  0.259 ( 0.259)\tLoss 3.1423e-02 (3.1423e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5 100.00 (100.00)\n","Epoch: [135][ 30/391]\tTime  0.096 ( 0.100)\tLoss 7.3615e-02 (5.5971e-02)\tAcc@1  97.66 ( 98.71)\tAcc@5 100.00 ( 99.92)\n","Epoch: [135][ 60/391]\tTime  0.089 ( 0.096)\tLoss 7.4385e-02 (5.3623e-02)\tAcc@1  99.22 ( 98.77)\tAcc@5 100.00 ( 99.95)\n","Epoch: [135][ 90/391]\tTime  0.091 ( 0.095)\tLoss 8.2314e-02 (5.2479e-02)\tAcc@1  96.88 ( 98.78)\tAcc@5 100.00 ( 99.91)\n","Epoch: [135][120/391]\tTime  0.090 ( 0.094)\tLoss 3.5020e-02 (5.0182e-02)\tAcc@1  99.22 ( 98.85)\tAcc@5 100.00 ( 99.93)\n","Epoch: [135][150/391]\tTime  0.095 ( 0.094)\tLoss 6.3917e-02 (5.1313e-02)\tAcc@1  99.22 ( 98.82)\tAcc@5 100.00 ( 99.94)\n","Epoch: [135][180/391]\tTime  0.091 ( 0.094)\tLoss 3.3446e-02 (5.0278e-02)\tAcc@1 100.00 ( 98.86)\tAcc@5 100.00 ( 99.94)\n","Epoch: [135][210/391]\tTime  0.094 ( 0.093)\tLoss 2.6234e-02 (5.0365e-02)\tAcc@1 100.00 ( 98.83)\tAcc@5 100.00 ( 99.94)\n","Epoch: [135][240/391]\tTime  0.100 ( 0.093)\tLoss 3.4594e-02 (5.0202e-02)\tAcc@1 100.00 ( 98.82)\tAcc@5 100.00 ( 99.94)\n","Epoch: [135][270/391]\tTime  0.076 ( 0.093)\tLoss 6.7247e-02 (4.9638e-02)\tAcc@1  98.44 ( 98.85)\tAcc@5 100.00 ( 99.94)\n","Epoch: [135][300/391]\tTime  0.094 ( 0.093)\tLoss 2.8481e-02 (4.9497e-02)\tAcc@1 100.00 ( 98.86)\tAcc@5 100.00 ( 99.95)\n","Epoch: [135][330/391]\tTime  0.092 ( 0.093)\tLoss 4.9353e-02 (4.9468e-02)\tAcc@1  99.22 ( 98.87)\tAcc@5 100.00 ( 99.95)\n","Epoch: [135][360/391]\tTime  0.091 ( 0.093)\tLoss 1.8911e-02 (4.9598e-02)\tAcc@1 100.00 ( 98.85)\tAcc@5 100.00 ( 99.95)\n","Epoch: [135][390/391]\tTime  0.081 ( 0.093)\tLoss 3.4743e-02 (4.9268e-02)\tAcc@1  98.75 ( 98.87)\tAcc@5 100.00 ( 99.95)\n","==> Train Accuracy: Acc@1 98.868 || Acc@5 99.948\n","==> Test Accuracy:  Acc@1 77.690 || Acc@5 94.300\n","==> 39.00 seconds to train this epoch\n","\n","\n","----- epoch: 136, lr: 0.0008000000000000003 -----\n","Epoch: [136][  0/391]\tTime  0.269 ( 0.269)\tLoss 3.6294e-02 (3.6294e-02)\tAcc@1  98.44 ( 98.44)\tAcc@5 100.00 (100.00)\n","Epoch: [136][ 30/391]\tTime  0.094 ( 0.099)\tLoss 4.1531e-02 (4.9226e-02)\tAcc@1  98.44 ( 98.79)\tAcc@5 100.00 ( 99.95)\n","Epoch: [136][ 60/391]\tTime  0.092 ( 0.096)\tLoss 3.4127e-02 (4.7325e-02)\tAcc@1 100.00 ( 98.96)\tAcc@5 100.00 ( 99.96)\n","Epoch: [136][ 90/391]\tTime  0.094 ( 0.095)\tLoss 3.8055e-02 (4.5682e-02)\tAcc@1  99.22 ( 99.03)\tAcc@5 100.00 ( 99.96)\n","Epoch: [136][120/391]\tTime  0.089 ( 0.094)\tLoss 1.3922e-02 (4.6802e-02)\tAcc@1 100.00 ( 98.97)\tAcc@5 100.00 ( 99.96)\n","Epoch: [136][150/391]\tTime  0.096 ( 0.094)\tLoss 3.0044e-02 (4.6121e-02)\tAcc@1 100.00 ( 98.97)\tAcc@5 100.00 ( 99.97)\n","Epoch: [136][180/391]\tTime  0.094 ( 0.094)\tLoss 5.2233e-02 (4.6000e-02)\tAcc@1  99.22 ( 98.97)\tAcc@5 100.00 ( 99.97)\n","Epoch: [136][210/391]\tTime  0.091 ( 0.094)\tLoss 5.5429e-02 (4.6075e-02)\tAcc@1  97.66 ( 98.96)\tAcc@5 100.00 ( 99.96)\n","Epoch: [136][240/391]\tTime  0.087 ( 0.093)\tLoss 2.7816e-02 (4.6163e-02)\tAcc@1 100.00 ( 98.97)\tAcc@5 100.00 ( 99.96)\n","Epoch: [136][270/391]\tTime  0.092 ( 0.093)\tLoss 3.5732e-02 (4.5805e-02)\tAcc@1  99.22 ( 98.98)\tAcc@5 100.00 ( 99.97)\n","Epoch: [136][300/391]\tTime  0.090 ( 0.093)\tLoss 3.1414e-02 (4.5727e-02)\tAcc@1 100.00 ( 98.98)\tAcc@5 100.00 ( 99.96)\n","Epoch: [136][330/391]\tTime  0.093 ( 0.093)\tLoss 4.4549e-02 (4.6057e-02)\tAcc@1  98.44 ( 98.97)\tAcc@5 100.00 ( 99.96)\n","Epoch: [136][360/391]\tTime  0.095 ( 0.093)\tLoss 6.0285e-02 (4.6037e-02)\tAcc@1  99.22 ( 98.99)\tAcc@5  99.22 ( 99.96)\n","Epoch: [136][390/391]\tTime  0.082 ( 0.093)\tLoss 1.8722e-02 (4.6262e-02)\tAcc@1 100.00 ( 98.98)\tAcc@5 100.00 ( 99.96)\n","==> Train Accuracy: Acc@1 98.976 || Acc@5 99.956\n","==> Test Accuracy:  Acc@1 77.510 || Acc@5 94.120\n","==> 39.08 seconds to train this epoch\n","\n","\n","----- epoch: 137, lr: 0.0008000000000000003 -----\n","Epoch: [137][  0/391]\tTime  0.284 ( 0.284)\tLoss 5.6791e-02 (5.6791e-02)\tAcc@1  97.66 ( 97.66)\tAcc@5 100.00 (100.00)\n","Epoch: [137][ 30/391]\tTime  0.092 ( 0.100)\tLoss 3.7063e-02 (4.8558e-02)\tAcc@1  98.44 ( 98.94)\tAcc@5 100.00 ( 99.95)\n","Epoch: [137][ 60/391]\tTime  0.091 ( 0.096)\tLoss 2.3361e-02 (4.9355e-02)\tAcc@1 100.00 ( 98.94)\tAcc@5 100.00 ( 99.95)\n","Epoch: [137][ 90/391]\tTime  0.089 ( 0.095)\tLoss 4.0991e-02 (4.6580e-02)\tAcc@1 100.00 ( 98.98)\tAcc@5 100.00 ( 99.97)\n","Epoch: [137][120/391]\tTime  0.087 ( 0.094)\tLoss 2.2700e-02 (4.6822e-02)\tAcc@1 100.00 ( 98.99)\tAcc@5 100.00 ( 99.96)\n","Epoch: [137][150/391]\tTime  0.088 ( 0.094)\tLoss 4.5907e-02 (4.6520e-02)\tAcc@1  97.66 ( 99.00)\tAcc@5 100.00 ( 99.96)\n","Epoch: [137][180/391]\tTime  0.084 ( 0.094)\tLoss 4.2365e-02 (4.5371e-02)\tAcc@1  97.66 ( 99.01)\tAcc@5 100.00 ( 99.96)\n","Epoch: [137][210/391]\tTime  0.091 ( 0.093)\tLoss 2.9428e-02 (4.5710e-02)\tAcc@1 100.00 ( 98.99)\tAcc@5 100.00 ( 99.96)\n","Epoch: [137][240/391]\tTime  0.090 ( 0.093)\tLoss 7.7664e-02 (4.6153e-02)\tAcc@1  98.44 ( 98.97)\tAcc@5 100.00 ( 99.96)\n","Epoch: [137][270/391]\tTime  0.093 ( 0.093)\tLoss 3.5976e-02 (4.6295e-02)\tAcc@1  99.22 ( 98.94)\tAcc@5 100.00 ( 99.96)\n","Epoch: [137][300/391]\tTime  0.096 ( 0.093)\tLoss 1.5440e-02 (4.6644e-02)\tAcc@1 100.00 ( 98.93)\tAcc@5 100.00 ( 99.96)\n","Epoch: [137][330/391]\tTime  0.091 ( 0.093)\tLoss 1.8011e-02 (4.7704e-02)\tAcc@1 100.00 ( 98.90)\tAcc@5 100.00 ( 99.94)\n","Epoch: [137][360/391]\tTime  0.092 ( 0.093)\tLoss 6.7095e-02 (4.7466e-02)\tAcc@1  97.66 ( 98.90)\tAcc@5 100.00 ( 99.95)\n","Epoch: [137][390/391]\tTime  0.081 ( 0.093)\tLoss 1.7293e-02 (4.6954e-02)\tAcc@1 100.00 ( 98.92)\tAcc@5 100.00 ( 99.95)\n","==> Train Accuracy: Acc@1 98.920 || Acc@5 99.950\n","==> Test Accuracy:  Acc@1 77.730 || Acc@5 94.340\n","==> 39.01 seconds to train this epoch\n","\n","\n","----- epoch: 138, lr: 0.0008000000000000003 -----\n","Epoch: [138][  0/391]\tTime  0.264 ( 0.264)\tLoss 8.3890e-02 (8.3890e-02)\tAcc@1  98.44 ( 98.44)\tAcc@5 100.00 (100.00)\n","Epoch: [138][ 30/391]\tTime  0.089 ( 0.099)\tLoss 3.2109e-02 (4.9307e-02)\tAcc@1 100.00 ( 98.89)\tAcc@5 100.00 ( 99.92)\n","Epoch: [138][ 60/391]\tTime  0.090 ( 0.096)\tLoss 4.0521e-02 (4.5749e-02)\tAcc@1  98.44 ( 99.00)\tAcc@5 100.00 ( 99.96)\n","Epoch: [138][ 90/391]\tTime  0.093 ( 0.095)\tLoss 5.3272e-02 (4.8533e-02)\tAcc@1  96.88 ( 98.92)\tAcc@5 100.00 ( 99.94)\n","Epoch: [138][120/391]\tTime  0.094 ( 0.094)\tLoss 5.1201e-02 (4.7733e-02)\tAcc@1  99.22 ( 98.96)\tAcc@5 100.00 ( 99.94)\n","Epoch: [138][150/391]\tTime  0.093 ( 0.094)\tLoss 3.4497e-02 (4.8194e-02)\tAcc@1 100.00 ( 98.95)\tAcc@5 100.00 ( 99.94)\n","Epoch: [138][180/391]\tTime  0.095 ( 0.094)\tLoss 2.0649e-02 (4.7477e-02)\tAcc@1 100.00 ( 98.95)\tAcc@5 100.00 ( 99.95)\n","Epoch: [138][210/391]\tTime  0.093 ( 0.094)\tLoss 6.9134e-02 (4.7673e-02)\tAcc@1  98.44 ( 98.95)\tAcc@5 100.00 ( 99.94)\n","Epoch: [138][240/391]\tTime  0.097 ( 0.093)\tLoss 6.6772e-02 (4.8048e-02)\tAcc@1  97.66 ( 98.93)\tAcc@5 100.00 ( 99.95)\n","Epoch: [138][270/391]\tTime  0.098 ( 0.093)\tLoss 6.1495e-02 (4.8058e-02)\tAcc@1  98.44 ( 98.93)\tAcc@5 100.00 ( 99.95)\n","Epoch: [138][300/391]\tTime  0.092 ( 0.093)\tLoss 2.2246e-02 (4.8382e-02)\tAcc@1  99.22 ( 98.90)\tAcc@5 100.00 ( 99.95)\n","Epoch: [138][330/391]\tTime  0.091 ( 0.093)\tLoss 4.4408e-02 (4.8620e-02)\tAcc@1  99.22 ( 98.91)\tAcc@5 100.00 ( 99.94)\n","Epoch: [138][360/391]\tTime  0.095 ( 0.093)\tLoss 6.0856e-02 (4.8796e-02)\tAcc@1  99.22 ( 98.89)\tAcc@5  99.22 ( 99.94)\n","Epoch: [138][390/391]\tTime  0.082 ( 0.093)\tLoss 2.8535e-02 (4.8637e-02)\tAcc@1 100.00 ( 98.89)\tAcc@5 100.00 ( 99.94)\n","==> Train Accuracy: Acc@1 98.892 || Acc@5 99.944\n","==> Test Accuracy:  Acc@1 78.020 || Acc@5 94.350\n","==> 39.04 seconds to train this epoch\n","\n","\n","----- epoch: 139, lr: 0.0008000000000000003 -----\n","Epoch: [139][  0/391]\tTime  0.265 ( 0.265)\tLoss 5.4764e-02 (5.4764e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5 100.00 (100.00)\n","Epoch: [139][ 30/391]\tTime  0.100 ( 0.100)\tLoss 2.9986e-02 (4.7702e-02)\tAcc@1  99.22 ( 98.97)\tAcc@5 100.00 ( 99.97)\n","Epoch: [139][ 60/391]\tTime  0.092 ( 0.096)\tLoss 5.9728e-02 (5.0278e-02)\tAcc@1  99.22 ( 98.77)\tAcc@5 100.00 ( 99.94)\n","Epoch: [139][ 90/391]\tTime  0.087 ( 0.095)\tLoss 1.5093e-02 (4.9411e-02)\tAcc@1 100.00 ( 98.85)\tAcc@5 100.00 ( 99.94)\n","Epoch: [139][120/391]\tTime  0.097 ( 0.095)\tLoss 3.6452e-02 (4.7669e-02)\tAcc@1 100.00 ( 98.90)\tAcc@5 100.00 ( 99.95)\n","Epoch: [139][150/391]\tTime  0.101 ( 0.094)\tLoss 5.0471e-02 (4.8282e-02)\tAcc@1  98.44 ( 98.88)\tAcc@5 100.00 ( 99.94)\n","Epoch: [139][180/391]\tTime  0.100 ( 0.094)\tLoss 6.1767e-02 (4.7364e-02)\tAcc@1  99.22 ( 98.92)\tAcc@5 100.00 ( 99.94)\n","Epoch: [139][210/391]\tTime  0.092 ( 0.094)\tLoss 4.7420e-02 (4.7535e-02)\tAcc@1  99.22 ( 98.90)\tAcc@5 100.00 ( 99.94)\n","Epoch: [139][240/391]\tTime  0.097 ( 0.093)\tLoss 5.5191e-02 (4.6573e-02)\tAcc@1  99.22 ( 98.91)\tAcc@5 100.00 ( 99.95)\n","Epoch: [139][270/391]\tTime  0.089 ( 0.093)\tLoss 5.0324e-02 (4.7628e-02)\tAcc@1  97.66 ( 98.88)\tAcc@5 100.00 ( 99.94)\n","Epoch: [139][300/391]\tTime  0.091 ( 0.093)\tLoss 5.4922e-02 (4.7473e-02)\tAcc@1  98.44 ( 98.89)\tAcc@5 100.00 ( 99.95)\n","Epoch: [139][330/391]\tTime  0.091 ( 0.093)\tLoss 5.4768e-02 (4.7235e-02)\tAcc@1  98.44 ( 98.88)\tAcc@5 100.00 ( 99.95)\n","Epoch: [139][360/391]\tTime  0.093 ( 0.093)\tLoss 7.7458e-02 (4.7248e-02)\tAcc@1  99.22 ( 98.88)\tAcc@5  99.22 ( 99.95)\n","Epoch: [139][390/391]\tTime  0.082 ( 0.093)\tLoss 2.1899e-02 (4.7086e-02)\tAcc@1 100.00 ( 98.88)\tAcc@5 100.00 ( 99.95)\n","==> Train Accuracy: Acc@1 98.882 || Acc@5 99.950\n","==> Test Accuracy:  Acc@1 77.990 || Acc@5 94.180\n","==> 39.16 seconds to train this epoch\n","\n","\n","----- epoch: 140, lr: 0.0008000000000000003 -----\n","Epoch: [140][  0/391]\tTime  0.271 ( 0.271)\tLoss 7.7291e-02 (7.7291e-02)\tAcc@1  96.88 ( 96.88)\tAcc@5 100.00 (100.00)\n","Epoch: [140][ 30/391]\tTime  0.102 ( 0.101)\tLoss 2.0970e-02 (5.0202e-02)\tAcc@1 100.00 ( 98.92)\tAcc@5 100.00 ( 99.90)\n","Epoch: [140][ 60/391]\tTime  0.092 ( 0.097)\tLoss 6.5407e-02 (4.9734e-02)\tAcc@1  97.66 ( 98.87)\tAcc@5 100.00 ( 99.92)\n","Epoch: [140][ 90/391]\tTime  0.094 ( 0.095)\tLoss 3.3575e-02 (4.9709e-02)\tAcc@1 100.00 ( 98.89)\tAcc@5 100.00 ( 99.93)\n","Epoch: [140][120/391]\tTime  0.100 ( 0.095)\tLoss 5.6150e-02 (4.8258e-02)\tAcc@1  98.44 ( 98.89)\tAcc@5 100.00 ( 99.95)\n","Epoch: [140][150/391]\tTime  0.094 ( 0.094)\tLoss 4.7502e-02 (4.7811e-02)\tAcc@1  99.22 ( 98.89)\tAcc@5 100.00 ( 99.94)\n","Epoch: [140][180/391]\tTime  0.091 ( 0.094)\tLoss 6.3068e-02 (4.8019e-02)\tAcc@1  98.44 ( 98.84)\tAcc@5 100.00 ( 99.94)\n","Epoch: [140][210/391]\tTime  0.089 ( 0.094)\tLoss 1.5027e-02 (4.8519e-02)\tAcc@1 100.00 ( 98.80)\tAcc@5 100.00 ( 99.95)\n","Epoch: [140][240/391]\tTime  0.092 ( 0.093)\tLoss 6.6210e-02 (4.8229e-02)\tAcc@1  99.22 ( 98.84)\tAcc@5 100.00 ( 99.95)\n","Epoch: [140][270/391]\tTime  0.090 ( 0.093)\tLoss 6.6450e-02 (4.8504e-02)\tAcc@1  97.66 ( 98.84)\tAcc@5 100.00 ( 99.95)\n","Epoch: [140][300/391]\tTime  0.092 ( 0.093)\tLoss 2.5590e-02 (4.8582e-02)\tAcc@1 100.00 ( 98.86)\tAcc@5 100.00 ( 99.95)\n","Epoch: [140][330/391]\tTime  0.090 ( 0.093)\tLoss 3.2638e-02 (4.8443e-02)\tAcc@1  98.44 ( 98.88)\tAcc@5 100.00 ( 99.95)\n","Epoch: [140][360/391]\tTime  0.091 ( 0.092)\tLoss 3.1242e-02 (4.7415e-02)\tAcc@1 100.00 ( 98.92)\tAcc@5 100.00 ( 99.95)\n","Epoch: [140][390/391]\tTime  0.085 ( 0.092)\tLoss 9.0919e-02 (4.7109e-02)\tAcc@1  98.75 ( 98.92)\tAcc@5  98.75 ( 99.95)\n","==> Train Accuracy: Acc@1 98.924 || Acc@5 99.952\n","==> Test Accuracy:  Acc@1 77.710 || Acc@5 94.120\n","==> 38.67 seconds to train this epoch\n","\n","\n","----- epoch: 141, lr: 0.0008000000000000003 -----\n","Epoch: [141][  0/391]\tTime  0.248 ( 0.248)\tLoss 4.8731e-02 (4.8731e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5 100.00 (100.00)\n","Epoch: [141][ 30/391]\tTime  0.094 ( 0.096)\tLoss 5.7879e-02 (5.5305e-02)\tAcc@1  99.22 ( 98.59)\tAcc@5 100.00 ( 99.97)\n","Epoch: [141][ 60/391]\tTime  0.090 ( 0.094)\tLoss 2.1495e-02 (5.0329e-02)\tAcc@1 100.00 ( 98.82)\tAcc@5 100.00 ( 99.97)\n","Epoch: [141][ 90/391]\tTime  0.090 ( 0.093)\tLoss 3.3926e-02 (4.9265e-02)\tAcc@1 100.00 ( 98.86)\tAcc@5 100.00 ( 99.97)\n","Epoch: [141][120/391]\tTime  0.088 ( 0.092)\tLoss 2.8685e-02 (4.9128e-02)\tAcc@1  99.22 ( 98.86)\tAcc@5 100.00 ( 99.97)\n","Epoch: [141][150/391]\tTime  0.089 ( 0.092)\tLoss 1.8551e-02 (4.9006e-02)\tAcc@1  99.22 ( 98.88)\tAcc@5 100.00 ( 99.97)\n","Epoch: [141][180/391]\tTime  0.089 ( 0.092)\tLoss 2.1812e-02 (4.9072e-02)\tAcc@1 100.00 ( 98.88)\tAcc@5 100.00 ( 99.97)\n","Epoch: [141][210/391]\tTime  0.086 ( 0.092)\tLoss 2.7571e-02 (4.8251e-02)\tAcc@1 100.00 ( 98.91)\tAcc@5 100.00 ( 99.96)\n","Epoch: [141][240/391]\tTime  0.088 ( 0.092)\tLoss 5.6810e-02 (4.7848e-02)\tAcc@1  99.22 ( 98.92)\tAcc@5 100.00 ( 99.96)\n","Epoch: [141][270/391]\tTime  0.093 ( 0.092)\tLoss 3.3115e-02 (4.7925e-02)\tAcc@1  98.44 ( 98.90)\tAcc@5 100.00 ( 99.96)\n","Epoch: [141][300/391]\tTime  0.091 ( 0.091)\tLoss 2.9696e-02 (4.7819e-02)\tAcc@1 100.00 ( 98.92)\tAcc@5 100.00 ( 99.96)\n","Epoch: [141][330/391]\tTime  0.090 ( 0.091)\tLoss 3.8074e-02 (4.7488e-02)\tAcc@1  99.22 ( 98.93)\tAcc@5 100.00 ( 99.96)\n","Epoch: [141][360/391]\tTime  0.090 ( 0.091)\tLoss 6.0037e-02 (4.7059e-02)\tAcc@1  98.44 ( 98.93)\tAcc@5 100.00 ( 99.96)\n","Epoch: [141][390/391]\tTime  0.081 ( 0.091)\tLoss 2.9037e-02 (4.7220e-02)\tAcc@1  98.75 ( 98.93)\tAcc@5 100.00 ( 99.96)\n","==> Train Accuracy: Acc@1 98.934 || Acc@5 99.960\n","==> Test Accuracy:  Acc@1 77.770 || Acc@5 94.220\n","==> 38.19 seconds to train this epoch\n","\n","\n","----- epoch: 142, lr: 0.0008000000000000003 -----\n","Epoch: [142][  0/391]\tTime  0.244 ( 0.244)\tLoss 7.1130e-02 (7.1130e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5 100.00 (100.00)\n","Epoch: [142][ 30/391]\tTime  0.090 ( 0.096)\tLoss 4.6544e-02 (4.7665e-02)\tAcc@1  98.44 ( 98.89)\tAcc@5 100.00 ( 99.97)\n","Epoch: [142][ 60/391]\tTime  0.090 ( 0.093)\tLoss 2.7096e-02 (4.8455e-02)\tAcc@1  99.22 ( 98.80)\tAcc@5 100.00 ( 99.94)\n","Epoch: [142][ 90/391]\tTime  0.097 ( 0.092)\tLoss 3.2008e-02 (4.9336e-02)\tAcc@1  99.22 ( 98.78)\tAcc@5 100.00 ( 99.95)\n","Epoch: [142][120/391]\tTime  0.089 ( 0.092)\tLoss 8.3918e-02 (5.1007e-02)\tAcc@1  99.22 ( 98.76)\tAcc@5  99.22 ( 99.94)\n","Epoch: [142][150/391]\tTime  0.090 ( 0.092)\tLoss 3.9230e-02 (5.0379e-02)\tAcc@1 100.00 ( 98.75)\tAcc@5 100.00 ( 99.95)\n","Epoch: [142][180/391]\tTime  0.098 ( 0.092)\tLoss 3.6331e-02 (4.8558e-02)\tAcc@1 100.00 ( 98.82)\tAcc@5 100.00 ( 99.95)\n","Epoch: [142][210/391]\tTime  0.090 ( 0.092)\tLoss 3.3600e-02 (4.8283e-02)\tAcc@1 100.00 ( 98.84)\tAcc@5 100.00 ( 99.96)\n","Epoch: [142][240/391]\tTime  0.088 ( 0.091)\tLoss 5.6986e-02 (4.8536e-02)\tAcc@1  97.66 ( 98.86)\tAcc@5 100.00 ( 99.95)\n","Epoch: [142][270/391]\tTime  0.091 ( 0.091)\tLoss 1.7646e-02 (4.8001e-02)\tAcc@1 100.00 ( 98.87)\tAcc@5 100.00 ( 99.95)\n","Epoch: [142][300/391]\tTime  0.089 ( 0.091)\tLoss 2.9102e-02 (4.7688e-02)\tAcc@1 100.00 ( 98.87)\tAcc@5 100.00 ( 99.95)\n","Epoch: [142][330/391]\tTime  0.090 ( 0.091)\tLoss 5.9305e-02 (4.8114e-02)\tAcc@1  98.44 ( 98.86)\tAcc@5 100.00 ( 99.94)\n","Epoch: [142][360/391]\tTime  0.088 ( 0.091)\tLoss 3.2908e-02 (4.8566e-02)\tAcc@1 100.00 ( 98.86)\tAcc@5 100.00 ( 99.94)\n","Epoch: [142][390/391]\tTime  0.083 ( 0.091)\tLoss 6.5317e-02 (4.8278e-02)\tAcc@1  98.75 ( 98.87)\tAcc@5 100.00 ( 99.94)\n","==> Train Accuracy: Acc@1 98.866 || Acc@5 99.936\n","==> Test Accuracy:  Acc@1 77.670 || Acc@5 94.300\n","==> 38.27 seconds to train this epoch\n","\n","\n","----- epoch: 143, lr: 0.0008000000000000003 -----\n","Epoch: [143][  0/391]\tTime  0.243 ( 0.243)\tLoss 4.3511e-02 (4.3511e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [143][ 30/391]\tTime  0.090 ( 0.096)\tLoss 5.1473e-02 (4.9358e-02)\tAcc@1  97.66 ( 98.69)\tAcc@5 100.00 ( 99.95)\n","Epoch: [143][ 60/391]\tTime  0.100 ( 0.094)\tLoss 5.6146e-02 (4.9129e-02)\tAcc@1  98.44 ( 98.74)\tAcc@5 100.00 ( 99.96)\n","Epoch: [143][ 90/391]\tTime  0.091 ( 0.093)\tLoss 5.9521e-02 (4.6523e-02)\tAcc@1  98.44 ( 98.92)\tAcc@5 100.00 ( 99.97)\n","Epoch: [143][120/391]\tTime  0.090 ( 0.092)\tLoss 6.1789e-02 (4.6063e-02)\tAcc@1  99.22 ( 98.98)\tAcc@5 100.00 ( 99.96)\n","Epoch: [143][150/391]\tTime  0.094 ( 0.092)\tLoss 1.9256e-02 (4.4779e-02)\tAcc@1 100.00 ( 99.03)\tAcc@5 100.00 ( 99.96)\n","Epoch: [143][180/391]\tTime  0.090 ( 0.092)\tLoss 2.4135e-02 (4.5244e-02)\tAcc@1 100.00 ( 99.01)\tAcc@5 100.00 ( 99.96)\n","Epoch: [143][210/391]\tTime  0.092 ( 0.092)\tLoss 6.5818e-02 (4.5058e-02)\tAcc@1  98.44 ( 99.01)\tAcc@5 100.00 ( 99.96)\n","Epoch: [143][240/391]\tTime  0.090 ( 0.092)\tLoss 2.3267e-02 (4.4065e-02)\tAcc@1  99.22 ( 99.03)\tAcc@5 100.00 ( 99.96)\n","Epoch: [143][270/391]\tTime  0.093 ( 0.091)\tLoss 1.0685e-01 (4.4532e-02)\tAcc@1  97.66 ( 99.02)\tAcc@5 100.00 ( 99.97)\n","Epoch: [143][300/391]\tTime  0.089 ( 0.091)\tLoss 8.8733e-02 (4.4690e-02)\tAcc@1  98.44 ( 99.01)\tAcc@5  99.22 ( 99.97)\n","Epoch: [143][330/391]\tTime  0.087 ( 0.091)\tLoss 2.3462e-02 (4.4599e-02)\tAcc@1 100.00 ( 99.03)\tAcc@5 100.00 ( 99.97)\n","Epoch: [143][360/391]\tTime  0.087 ( 0.091)\tLoss 2.9709e-02 (4.4655e-02)\tAcc@1  98.44 ( 99.02)\tAcc@5 100.00 ( 99.96)\n","Epoch: [143][390/391]\tTime  0.081 ( 0.091)\tLoss 7.3039e-02 (4.5169e-02)\tAcc@1  98.75 ( 98.99)\tAcc@5 100.00 ( 99.96)\n","==> Train Accuracy: Acc@1 98.992 || Acc@5 99.960\n","==> Test Accuracy:  Acc@1 77.670 || Acc@5 94.210\n","==> 38.24 seconds to train this epoch\n","\n","\n","----- epoch: 144, lr: 0.0008000000000000003 -----\n","Epoch: [144][  0/391]\tTime  0.250 ( 0.250)\tLoss 9.8316e-02 (9.8316e-02)\tAcc@1  96.88 ( 96.88)\tAcc@5 100.00 (100.00)\n","Epoch: [144][ 30/391]\tTime  0.090 ( 0.096)\tLoss 4.5806e-02 (4.9578e-02)\tAcc@1  99.22 ( 98.77)\tAcc@5 100.00 ( 99.92)\n","Epoch: [144][ 60/391]\tTime  0.093 ( 0.094)\tLoss 3.7253e-02 (4.8939e-02)\tAcc@1 100.00 ( 98.80)\tAcc@5 100.00 ( 99.90)\n","Epoch: [144][ 90/391]\tTime  0.091 ( 0.093)\tLoss 5.2482e-02 (4.9459e-02)\tAcc@1  99.22 ( 98.80)\tAcc@5 100.00 ( 99.91)\n","Epoch: [144][120/391]\tTime  0.098 ( 0.092)\tLoss 2.5261e-02 (4.7523e-02)\tAcc@1 100.00 ( 98.88)\tAcc@5 100.00 ( 99.92)\n","Epoch: [144][150/391]\tTime  0.092 ( 0.092)\tLoss 3.0335e-02 (4.6282e-02)\tAcc@1 100.00 ( 98.92)\tAcc@5 100.00 ( 99.94)\n","Epoch: [144][180/391]\tTime  0.091 ( 0.092)\tLoss 3.1862e-02 (4.6993e-02)\tAcc@1  99.22 ( 98.87)\tAcc@5 100.00 ( 99.93)\n","Epoch: [144][210/391]\tTime  0.094 ( 0.092)\tLoss 3.0662e-02 (4.6138e-02)\tAcc@1 100.00 ( 98.93)\tAcc@5 100.00 ( 99.94)\n","Epoch: [144][240/391]\tTime  0.090 ( 0.092)\tLoss 7.6990e-02 (4.6616e-02)\tAcc@1  98.44 ( 98.91)\tAcc@5 100.00 ( 99.94)\n","Epoch: [144][270/391]\tTime  0.089 ( 0.092)\tLoss 3.9662e-02 (4.6225e-02)\tAcc@1  99.22 ( 98.94)\tAcc@5 100.00 ( 99.94)\n","Epoch: [144][300/391]\tTime  0.091 ( 0.091)\tLoss 2.4022e-02 (4.6485e-02)\tAcc@1 100.00 ( 98.95)\tAcc@5 100.00 ( 99.94)\n","Epoch: [144][330/391]\tTime  0.087 ( 0.091)\tLoss 1.5857e-02 (4.6476e-02)\tAcc@1 100.00 ( 98.94)\tAcc@5 100.00 ( 99.95)\n","Epoch: [144][360/391]\tTime  0.091 ( 0.091)\tLoss 2.3259e-02 (4.5859e-02)\tAcc@1  99.22 ( 98.94)\tAcc@5 100.00 ( 99.95)\n","Epoch: [144][390/391]\tTime  0.082 ( 0.091)\tLoss 3.7204e-02 (4.5808e-02)\tAcc@1 100.00 ( 98.95)\tAcc@5 100.00 ( 99.95)\n","==> Train Accuracy: Acc@1 98.950 || Acc@5 99.950\n","==> Test Accuracy:  Acc@1 77.800 || Acc@5 94.130\n","==> 38.29 seconds to train this epoch\n","\n","\n","----- epoch: 145, lr: 0.0008000000000000003 -----\n","Epoch: [145][  0/391]\tTime  0.240 ( 0.240)\tLoss 6.2632e-02 (6.2632e-02)\tAcc@1  97.66 ( 97.66)\tAcc@5 100.00 (100.00)\n","Epoch: [145][ 30/391]\tTime  0.092 ( 0.097)\tLoss 4.9911e-02 (4.7564e-02)\tAcc@1 100.00 ( 98.94)\tAcc@5 100.00 ( 99.92)\n","Epoch: [145][ 60/391]\tTime  0.090 ( 0.094)\tLoss 4.1757e-02 (4.5450e-02)\tAcc@1  99.22 ( 98.89)\tAcc@5 100.00 ( 99.95)\n","Epoch: [145][ 90/391]\tTime  0.094 ( 0.093)\tLoss 3.1689e-02 (4.6382e-02)\tAcc@1  99.22 ( 98.88)\tAcc@5 100.00 ( 99.95)\n","Epoch: [145][120/391]\tTime  0.089 ( 0.093)\tLoss 4.0263e-02 (4.5237e-02)\tAcc@1  98.44 ( 98.92)\tAcc@5 100.00 ( 99.95)\n","Epoch: [145][150/391]\tTime  0.094 ( 0.092)\tLoss 2.4093e-02 (4.5336e-02)\tAcc@1 100.00 ( 98.93)\tAcc@5 100.00 ( 99.95)\n","Epoch: [145][180/391]\tTime  0.086 ( 0.092)\tLoss 2.0087e-02 (4.4228e-02)\tAcc@1 100.00 ( 98.98)\tAcc@5 100.00 ( 99.96)\n","Epoch: [145][210/391]\tTime  0.089 ( 0.092)\tLoss 5.9840e-02 (4.4418e-02)\tAcc@1  99.22 ( 98.99)\tAcc@5  99.22 ( 99.96)\n","Epoch: [145][240/391]\tTime  0.090 ( 0.092)\tLoss 4.0125e-02 (4.4506e-02)\tAcc@1  98.44 ( 98.99)\tAcc@5 100.00 ( 99.96)\n","Epoch: [145][270/391]\tTime  0.093 ( 0.092)\tLoss 5.6794e-02 (4.4549e-02)\tAcc@1  98.44 ( 98.97)\tAcc@5 100.00 ( 99.96)\n","Epoch: [145][300/391]\tTime  0.090 ( 0.092)\tLoss 5.4418e-02 (4.4494e-02)\tAcc@1  98.44 ( 98.97)\tAcc@5 100.00 ( 99.96)\n","Epoch: [145][330/391]\tTime  0.090 ( 0.091)\tLoss 3.5004e-02 (4.4831e-02)\tAcc@1  99.22 ( 98.96)\tAcc@5 100.00 ( 99.96)\n","Epoch: [145][360/391]\tTime  0.092 ( 0.091)\tLoss 5.5845e-02 (4.5041e-02)\tAcc@1  99.22 ( 98.96)\tAcc@5 100.00 ( 99.95)\n","Epoch: [145][390/391]\tTime  0.081 ( 0.091)\tLoss 6.8045e-02 (4.4892e-02)\tAcc@1  97.50 ( 98.94)\tAcc@5  98.75 ( 99.95)\n","==> Train Accuracy: Acc@1 98.944 || Acc@5 99.954\n","==> Test Accuracy:  Acc@1 78.140 || Acc@5 94.180\n","==> 38.29 seconds to train this epoch\n","\n","\n","----- epoch: 146, lr: 0.0008000000000000003 -----\n","Epoch: [146][  0/391]\tTime  0.252 ( 0.252)\tLoss 3.5065e-02 (3.5065e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5 100.00 (100.00)\n","Epoch: [146][ 30/391]\tTime  0.095 ( 0.096)\tLoss 3.1492e-02 (4.3309e-02)\tAcc@1  99.22 ( 99.12)\tAcc@5 100.00 ( 99.97)\n","Epoch: [146][ 60/391]\tTime  0.091 ( 0.094)\tLoss 5.2736e-02 (4.3188e-02)\tAcc@1  98.44 ( 99.08)\tAcc@5 100.00 ( 99.97)\n","Epoch: [146][ 90/391]\tTime  0.090 ( 0.093)\tLoss 5.8234e-02 (4.5973e-02)\tAcc@1  98.44 ( 99.00)\tAcc@5 100.00 ( 99.97)\n","Epoch: [146][120/391]\tTime  0.092 ( 0.092)\tLoss 2.0398e-02 (4.5799e-02)\tAcc@1 100.00 ( 98.96)\tAcc@5 100.00 ( 99.96)\n","Epoch: [146][150/391]\tTime  0.093 ( 0.092)\tLoss 3.1125e-02 (4.5317e-02)\tAcc@1 100.00 ( 99.01)\tAcc@5 100.00 ( 99.96)\n","Epoch: [146][180/391]\tTime  0.092 ( 0.092)\tLoss 3.7935e-02 (4.5114e-02)\tAcc@1  99.22 ( 98.99)\tAcc@5 100.00 ( 99.96)\n","Epoch: [146][210/391]\tTime  0.093 ( 0.092)\tLoss 4.9881e-02 (4.5234e-02)\tAcc@1  97.66 ( 98.99)\tAcc@5 100.00 ( 99.96)\n","Epoch: [146][240/391]\tTime  0.091 ( 0.092)\tLoss 4.4112e-02 (4.5040e-02)\tAcc@1  98.44 ( 98.98)\tAcc@5 100.00 ( 99.96)\n","Epoch: [146][270/391]\tTime  0.094 ( 0.092)\tLoss 3.5396e-02 (4.5779e-02)\tAcc@1  99.22 ( 98.96)\tAcc@5 100.00 ( 99.96)\n","Epoch: [146][300/391]\tTime  0.088 ( 0.092)\tLoss 3.6418e-02 (4.5031e-02)\tAcc@1 100.00 ( 98.99)\tAcc@5 100.00 ( 99.97)\n","Epoch: [146][330/391]\tTime  0.091 ( 0.092)\tLoss 3.5181e-02 (4.4639e-02)\tAcc@1 100.00 ( 98.98)\tAcc@5 100.00 ( 99.97)\n","Epoch: [146][360/391]\tTime  0.090 ( 0.092)\tLoss 8.0028e-02 (4.4854e-02)\tAcc@1  99.22 ( 98.99)\tAcc@5 100.00 ( 99.97)\n","Epoch: [146][390/391]\tTime  0.081 ( 0.091)\tLoss 3.9564e-02 (4.4561e-02)\tAcc@1 100.00 ( 99.00)\tAcc@5 100.00 ( 99.96)\n","==> Train Accuracy: Acc@1 99.002 || Acc@5 99.964\n","==> Test Accuracy:  Acc@1 77.770 || Acc@5 94.190\n","==> 38.36 seconds to train this epoch\n","\n","\n","----- epoch: 147, lr: 0.0008000000000000003 -----\n","Epoch: [147][  0/391]\tTime  0.247 ( 0.247)\tLoss 6.2565e-02 (6.2565e-02)\tAcc@1  97.66 ( 97.66)\tAcc@5 100.00 (100.00)\n","Epoch: [147][ 30/391]\tTime  0.088 ( 0.097)\tLoss 1.8963e-01 (4.6358e-02)\tAcc@1  96.09 ( 98.97)\tAcc@5  99.22 ( 99.92)\n","Epoch: [147][ 60/391]\tTime  0.091 ( 0.094)\tLoss 3.0953e-02 (4.5895e-02)\tAcc@1  99.22 ( 99.03)\tAcc@5 100.00 ( 99.96)\n","Epoch: [147][ 90/391]\tTime  0.089 ( 0.093)\tLoss 2.7066e-02 (4.6405e-02)\tAcc@1 100.00 ( 98.97)\tAcc@5 100.00 ( 99.96)\n","Epoch: [147][120/391]\tTime  0.092 ( 0.092)\tLoss 1.9723e-02 (4.5103e-02)\tAcc@1 100.00 ( 99.04)\tAcc@5 100.00 ( 99.97)\n","Epoch: [147][150/391]\tTime  0.095 ( 0.092)\tLoss 3.4469e-02 (4.4545e-02)\tAcc@1  98.44 ( 99.06)\tAcc@5 100.00 ( 99.96)\n","Epoch: [147][180/391]\tTime  0.088 ( 0.092)\tLoss 5.0520e-02 (4.3711e-02)\tAcc@1  99.22 ( 99.09)\tAcc@5 100.00 ( 99.96)\n","Epoch: [147][210/391]\tTime  0.090 ( 0.092)\tLoss 1.8959e-02 (4.4039e-02)\tAcc@1 100.00 ( 99.07)\tAcc@5 100.00 ( 99.96)\n","Epoch: [147][240/391]\tTime  0.095 ( 0.092)\tLoss 6.4510e-02 (4.4000e-02)\tAcc@1  99.22 ( 99.04)\tAcc@5 100.00 ( 99.96)\n","Epoch: [147][270/391]\tTime  0.090 ( 0.092)\tLoss 2.5006e-02 (4.4134e-02)\tAcc@1  99.22 ( 99.02)\tAcc@5 100.00 ( 99.96)\n","Epoch: [147][300/391]\tTime  0.091 ( 0.092)\tLoss 6.5411e-02 (4.4186e-02)\tAcc@1  97.66 ( 99.03)\tAcc@5 100.00 ( 99.96)\n","Epoch: [147][330/391]\tTime  0.093 ( 0.091)\tLoss 6.9236e-02 (4.3711e-02)\tAcc@1  99.22 ( 99.05)\tAcc@5 100.00 ( 99.95)\n","Epoch: [147][360/391]\tTime  0.090 ( 0.091)\tLoss 5.3415e-02 (4.3831e-02)\tAcc@1  99.22 ( 99.04)\tAcc@5 100.00 ( 99.95)\n","Epoch: [147][390/391]\tTime  0.081 ( 0.091)\tLoss 6.7964e-02 (4.3412e-02)\tAcc@1  98.75 ( 99.06)\tAcc@5 100.00 ( 99.96)\n","==> Train Accuracy: Acc@1 99.064 || Acc@5 99.956\n","==> Test Accuracy:  Acc@1 77.810 || Acc@5 94.130\n","==> 38.33 seconds to train this epoch\n","\n","\n","----- epoch: 148, lr: 0.0008000000000000003 -----\n","Epoch: [148][  0/391]\tTime  0.233 ( 0.233)\tLoss 5.6333e-02 (5.6333e-02)\tAcc@1  97.66 ( 97.66)\tAcc@5 100.00 (100.00)\n","Epoch: [148][ 30/391]\tTime  0.089 ( 0.096)\tLoss 1.9806e-02 (4.2636e-02)\tAcc@1 100.00 ( 99.07)\tAcc@5 100.00 ( 99.97)\n","Epoch: [148][ 60/391]\tTime  0.093 ( 0.093)\tLoss 2.0855e-02 (4.0252e-02)\tAcc@1 100.00 ( 99.17)\tAcc@5 100.00 ( 99.95)\n","Epoch: [148][ 90/391]\tTime  0.094 ( 0.093)\tLoss 3.1057e-02 (3.9838e-02)\tAcc@1  99.22 ( 99.18)\tAcc@5 100.00 ( 99.96)\n","Epoch: [148][120/391]\tTime  0.092 ( 0.092)\tLoss 3.7089e-02 (4.0335e-02)\tAcc@1  99.22 ( 99.13)\tAcc@5 100.00 ( 99.95)\n","Epoch: [148][150/391]\tTime  0.091 ( 0.092)\tLoss 3.2684e-02 (4.0863e-02)\tAcc@1  98.44 ( 99.11)\tAcc@5 100.00 ( 99.96)\n","Epoch: [148][180/391]\tTime  0.093 ( 0.092)\tLoss 4.0936e-02 (4.2230e-02)\tAcc@1  99.22 ( 99.05)\tAcc@5 100.00 ( 99.96)\n","Epoch: [148][210/391]\tTime  0.092 ( 0.092)\tLoss 3.6998e-02 (4.2504e-02)\tAcc@1  99.22 ( 99.05)\tAcc@5 100.00 ( 99.96)\n","Epoch: [148][240/391]\tTime  0.087 ( 0.092)\tLoss 6.4608e-02 (4.3646e-02)\tAcc@1  97.66 ( 99.01)\tAcc@5 100.00 ( 99.96)\n","Epoch: [148][270/391]\tTime  0.091 ( 0.092)\tLoss 9.3332e-02 (4.3145e-02)\tAcc@1  96.88 ( 99.02)\tAcc@5  99.22 ( 99.96)\n","Epoch: [148][300/391]\tTime  0.086 ( 0.091)\tLoss 2.0132e-02 (4.3872e-02)\tAcc@1  99.22 ( 98.99)\tAcc@5 100.00 ( 99.95)\n","Epoch: [148][330/391]\tTime  0.089 ( 0.091)\tLoss 7.8835e-02 (4.4402e-02)\tAcc@1  97.66 ( 98.97)\tAcc@5  99.22 ( 99.95)\n","Epoch: [148][360/391]\tTime  0.091 ( 0.091)\tLoss 3.8011e-02 (4.4614e-02)\tAcc@1 100.00 ( 98.95)\tAcc@5 100.00 ( 99.95)\n","Epoch: [148][390/391]\tTime  0.082 ( 0.091)\tLoss 3.7034e-02 (4.4752e-02)\tAcc@1 100.00 ( 98.93)\tAcc@5 100.00 ( 99.95)\n","==> Train Accuracy: Acc@1 98.934 || Acc@5 99.954\n","==> Test Accuracy:  Acc@1 77.670 || Acc@5 94.050\n","==> 38.32 seconds to train this epoch\n","\n","\n","----- epoch: 149, lr: 0.0008000000000000003 -----\n","Epoch: [149][  0/391]\tTime  0.247 ( 0.247)\tLoss 4.5758e-02 (4.5758e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5  99.22 ( 99.22)\n","Epoch: [149][ 30/391]\tTime  0.091 ( 0.096)\tLoss 3.6454e-02 (4.4499e-02)\tAcc@1  99.22 ( 98.97)\tAcc@5 100.00 ( 99.90)\n","Epoch: [149][ 60/391]\tTime  0.091 ( 0.094)\tLoss 1.0061e-01 (4.5436e-02)\tAcc@1  96.88 ( 98.89)\tAcc@5 100.00 ( 99.90)\n","Epoch: [149][ 90/391]\tTime  0.094 ( 0.093)\tLoss 2.9957e-02 (4.2190e-02)\tAcc@1 100.00 ( 99.00)\tAcc@5 100.00 ( 99.92)\n","Epoch: [149][120/391]\tTime  0.091 ( 0.092)\tLoss 2.8976e-02 (4.2173e-02)\tAcc@1  99.22 ( 99.01)\tAcc@5 100.00 ( 99.94)\n","Epoch: [149][150/391]\tTime  0.089 ( 0.092)\tLoss 3.0244e-02 (4.2578e-02)\tAcc@1 100.00 ( 99.00)\tAcc@5 100.00 ( 99.93)\n","Epoch: [149][180/391]\tTime  0.092 ( 0.092)\tLoss 2.0351e-02 (4.1132e-02)\tAcc@1 100.00 ( 99.06)\tAcc@5 100.00 ( 99.94)\n","Epoch: [149][210/391]\tTime  0.090 ( 0.092)\tLoss 2.1315e-02 (4.1255e-02)\tAcc@1 100.00 ( 99.05)\tAcc@5 100.00 ( 99.94)\n","Epoch: [149][240/391]\tTime  0.089 ( 0.092)\tLoss 9.5250e-02 (4.2021e-02)\tAcc@1  96.88 ( 99.03)\tAcc@5 100.00 ( 99.94)\n","Epoch: [149][270/391]\tTime  0.088 ( 0.092)\tLoss 3.3543e-02 (4.2528e-02)\tAcc@1 100.00 ( 99.05)\tAcc@5 100.00 ( 99.94)\n","Epoch: [149][300/391]\tTime  0.090 ( 0.092)\tLoss 5.8697e-02 (4.2686e-02)\tAcc@1  98.44 ( 99.03)\tAcc@5 100.00 ( 99.95)\n","Epoch: [149][330/391]\tTime  0.092 ( 0.091)\tLoss 1.4290e-02 (4.3131e-02)\tAcc@1 100.00 ( 99.01)\tAcc@5 100.00 ( 99.95)\n","Epoch: [149][360/391]\tTime  0.089 ( 0.091)\tLoss 6.5759e-02 (4.2930e-02)\tAcc@1  98.44 ( 99.03)\tAcc@5 100.00 ( 99.95)\n","Epoch: [149][390/391]\tTime  0.082 ( 0.091)\tLoss 4.1827e-02 (4.2832e-02)\tAcc@1 100.00 ( 99.04)\tAcc@5 100.00 ( 99.95)\n","==> Train Accuracy: Acc@1 99.036 || Acc@5 99.952\n","==> Test Accuracy:  Acc@1 77.830 || Acc@5 94.100\n","==> 38.28 seconds to train this epoch\n","\n","Best Top-1 Accuracy: 78.14\n"],"name":"stdout"}]}]}