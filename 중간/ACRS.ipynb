{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ACRS.ipynb","provenance":[{"file_id":"1NBZijnKI-JSQWMRa_ie6TIGgiCotY-oF","timestamp":1620412557373},{"file_id":"18b80wpeQD1Wj6NjwZFFhZGDuMWWW1TIZ","timestamp":1619286623952},{"file_id":"1pSPNcOLDPSKONSeGALl1Jplu9ET9z-1q","timestamp":1619081668342}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"75d6baf1f9ec4d239abc7fa6be6621a6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_3134012ec8994071b4cafabe92d036ee","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_672987e229f04fdc868b703da835773c","IPY_MODEL_cba50a585e6d435f9902de418ea2cf5b"]}},"3134012ec8994071b4cafabe92d036ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"672987e229f04fdc868b703da835773c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_b792c6e307c8462588ef08d4fd73dd6b","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":169001437,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":169001437,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d54d05f638614d2398adb6f936f58e41"}},"cba50a585e6d435f9902de418ea2cf5b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7c4737e1f82841f99cbf245b436d7cf2","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 169001984/? [01:28&lt;00:00, 1899179.57it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_eb756e77d722477e9c89d2adb9ccb5ba"}},"b792c6e307c8462588ef08d4fd73dd6b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"d54d05f638614d2398adb6f936f58e41":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7c4737e1f82841f99cbf245b436d7cf2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"eb756e77d722477e9c89d2adb9ccb5ba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"OSFGYaIDG6f0"},"source":["Cutout Data Augmentation.\n","\n","This code is implmented by following the official code (https://github.com/uoguelph-mlrg/Cutout)\n"]},{"cell_type":"markdown","metadata":{"id":"vCVSE5-UboYl"},"source":["##**Import all neceassary packages**"]},{"cell_type":"code","metadata":{"id":"5YBMwPsubsbX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620877931662,"user_tz":-540,"elapsed":33521,"user":{"displayName":"장예원","photoUrl":"","userId":"13687321636375718239"}},"outputId":"2502a2a8-3d81-436c-8027-66afc3c843e9"},"source":["import numpy as np\n","import time\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","import torch.backends.cudnn as cudnn\n","from torch.optim.lr_scheduler import MultiStepLR\n","\n","import torchvision\n","from torchvision import datasets, transforms\n","\n","from tqdm.notebook import tqdm as tqdm\n","\n","from PIL import Image, ImageEnhance, ImageOps, ImageChops\n","import matplotlib.pyplot as plt\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"L88afYXKMSdL"},"source":["##**Model - Define ResNet Model**\n"]},{"cell_type":"code","metadata":{"id":"eMFSLTnkMQdq"},"source":["'''ResNet18/34/50/101/152 in Pytorch.'''\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = conv3x3(3,64)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = F.avg_pool2d(out, 4)\n","        out = out.view(out.size(0), -1)\n","        out = self.linear(out)\n","        return out\n","\n","\n","def ResNet18(num_classes=10):\n","    return ResNet(BasicBlock, [2,2,2,2], num_classes)\n","\n","def ResNet34(num_classes=10):\n","    return ResNet(BasicBlock, [3,4,6,3], num_classes)\n","\n","def ResNet50(num_classes=10):\n","    return ResNet(Bottleneck, [3,4,6,3], num_classes)\n","\n","def ResNet101(num_classes=10):\n","    return ResNet(Bottleneck, [3,4,23,3], num_classes)\n","\n","def ResNet152(num_classes=10):\n","    return ResNet(Bottleneck, [3,8,36,3], num_classes)\n","\n","def test_resnet():\n","    net = ResNet50()\n","    y = net(Variable(torch.randn(1,3,32,32)))\n","    print(y.size())\n","\n","# test_resnet()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qjM3cl279Lvg"},"source":["##**Utils**"]},{"cell_type":"code","metadata":{"id":"gIvuSgE49Kvu"},"source":["class AverageMeter(object):\n","    r\"\"\"Computes and stores the average and current value\n","    \"\"\"\n","    def __init__(self, name, fmt=':f'):\n","        self.name = name\n","        self.fmt = fmt\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","    def __str__(self):\n","        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n","        return fmtstr.format(**self.__dict__)\n","\n","\n","class ProgressMeter(object):\n","    def __init__(self, num_batches, *meters, prefix=\"\"):\n","        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n","        self.meters = meters\n","        self.prefix = prefix\n","\n","    def print(self, batch):\n","        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n","        entries += [str(meter) for meter in self.meters]\n","        print('\\t'.join(entries))\n","\n","    def _get_batch_fmtstr(self, num_batches):\n","        num_digits = len(str(num_batches // 1))\n","        fmt = '{:' + str(num_digits) + 'd}'\n","        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n","\n","\n","def accuracy(output, target, topk=(1,)):\n","    r\"\"\"Computes the accuracy over the $k$ top predictions for the specified values of k\n","    \"\"\"\n","    with torch.no_grad():\n","        maxk = max(topk)\n","        batch_size = target.size(0)\n","\n","        # _, pred = output.topk(maxk, 1, True, True)\n","        # pred = pred.t()\n","        # correct = pred.eq(target.view(1, -1).expand_as(pred))\n","\n","        # faster topk (ref: https://github.com/pytorch/pytorch/issues/22812)\n","        _, idx = output.sort(descending=True)\n","        pred = idx[:,:maxk]\n","        pred = pred.t()\n","        correct = pred.eq(target.view(1, -1).expand_as(pred))\n","\n","        res = []\n","        for k in topk:\n","            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n","            res.append(correct_k.mul_(100.0 / batch_size))\n","        return res\n","\n","class DiskAugmenter(object):\n","    def __init__(self, local_mask=(120, 160), global_mask=(40, 80),\n","                 flip_and_noise=False, augmenting_prob=0.67):\n","\n","        self.augmenting_prob = augmenting_prob\n","        self.local_mask = local_mask\n","        self.global_mask = global_mask\n","        self.flip_and_noise = flip_and_noise\n","        self.augment_illumination = any(x > 0 for x in list(local_mask) + list(global_mask))\n","\n","    def __call__(self, img):\n","        if random.uniform(0, 1) < self.augmenting_prob:\n","            img = illumination_augmenter(img, self.global_mask, self.local_mask)\n","            return img\n","        else:\n","            return img\n","\n","class RandomGamma(object):\n","    def __init__(self, gamma_p = 0.5, gamma_ratio=(0,1.5)):\n","        self.gamma_p = gamma_p\n","        self.gamma_ratio = gamma_ratio\n","\n","    def __call__(self,img):\n","        if random.uniform(0, 1) < self.gamma_p:\n","            gamma = random.uniform(self.gamma_ratio[0], self.gamma_ratio[1])\n","            img = TF.adjust_gamma(img, gamma, gain=1)\n","            return img\n","        else:\n","            return img\n","\n","class RandomColorJitter(object):\n","    def __init__(self, p = 0.5, brightness_ratio=(0,2), contrast_ratio=(0,2), \\\n","                saturation_ratio=(0,2), hue_ratio=(-0.5,0.5)):\n","        self.p = p\n","        self.brightness_ratio = brightness_ratio\n","        self.contrast_ratio = contrast_ratio\n","        self.saturation_ratio = saturation_ratio\n","        self.hue_ratio = hue_ratio\n","\n","    @staticmethod\n","    def process(img, brightness_ratio, contrast_ratio, saturation_ratio, hue_ratio):\n","        brightness = random.uniform(brightness_ratio[0], brightness_ratio[1])\n","        contrast = random.uniform(contrast_ratio[0], contrast_ratio[1])\n","        saturation = random.uniform(saturation_ratio[0], saturation_ratio[1])\n","        hue = random.uniform(hue_ratio[0], hue_ratio[1])\n","\n","        img = TF.adjust_brightness(img, brightness)\n","        img = TF.adjust_contrast(img, contrast)\n","        img = TF.adjust_saturation(img, saturation)\n","        img = TF.adjust_hue(img, hue)\n","\n","        return img\n","\n","    def __call__(self,img):\n","        if random.uniform(0, 1) < self.p:\n","            img = self.process(img, self.brightness_ratio, self.contrast_ratio, \\\n","                                self.saturation_ratio, self.hue_ratio)\n","            return img\n","        else:\n","            return img"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o6y3zhSfMbdC"},"source":["##**Cutout: Main Code for Applying Cutout data augmentation**"]},{"cell_type":"code","metadata":{"id":"iMQI2K4AMopg"},"source":["class Cutout(object):\n","    \"\"\"Randomly mask out one or more patches from an image.\n","\n","    Args:\n","        n_holes (int): Number of patches to cut out of each image.\n","        length (int): The length (in pixels) of each square patch.\n","    \"\"\"\n","    def __init__(self, n_holes, length):\n","        self.n_holes = n_holes\n","        self.length = length\n","\n","    def __call__(self, img):\n","        \"\"\"\n","        Args:\n","            img (Tensor): Tensor image of size (C, H, W).\n","        Returns:\n","            Tensor: Image with n_holes of dimension length x length cut out of it.\n","        \"\"\"\n","        h = img.size(1)\n","        w = img.size(2)\n","\n","        mask = np.ones((h, w), np.float32)\n","\n","        for n in range(self.n_holes):\n","            y = np.random.randint(h)\n","            x = np.random.randint(w)\n","\n","            y1 = np.clip(y - self.length // 2, 0, h)\n","            y2 = np.clip(y + self.length // 2, 0, h)\n","            x1 = np.clip(x - self.length // 2, 0, w)\n","            x2 = np.clip(x + self.length // 2, 0, w)\n","\n","            mask[y1: y2, x1: x2] = 0.\n","\n","        mask = torch.from_numpy(mask)\n","        mask = mask.expand_as(img)\n","        img = img * mask\n","\n","        return img"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o1zGEvEi9W_g"},"source":["##**Random-Shadows-Highlights**"]},{"cell_type":"code","metadata":{"id":"kElVGH4g9WoB"},"source":["import random\n","import numpy as np\n","import cv2\n","from PIL import Image, ImageChops\n","import torchvision.transforms.functional as TF\n","\n","class RandomShadows(object):\n","    def __init__(self, p=0.5, high_ratio=(1,2), low_ratio=(0.01, 0.5), left_low_ratio=(0.4,0.6), \\\n","    left_high_ratio=(0,0.2), right_low_ratio=(0.4,0.6), right_high_ratio = (0,0.2)):\n","        self.p = p\n","        self.high_ratio = high_ratio\n","        self.low_ratio = low_ratio\n","        self.left_low_ratio = left_low_ratio\n","        self.left_high_ratio = left_high_ratio\n","        self.right_low_ratio = right_low_ratio\n","        self.right_high_ratio = right_high_ratio\n","\n","    @staticmethod\n","    def process(img, high_ratio, low_ratio, left_low_ratio, left_high_ratio, \\\n","            right_low_ratio, right_high_ratio):\n","\n","        w, h = img.size\n","        high_contrast_factor = random.uniform(high_ratio[0], high_ratio[1])\n","        low_contrast_factor = random.uniform(low_ratio[0], low_ratio[1])\n","\n","        left_low_factor = random.uniform(left_low_ratio[0]*h, left_low_ratio[1]*h)\n","        left_high_factor = random.uniform(left_high_ratio[0]*h, left_high_ratio[1]*h)\n","        right_low_factor = random.uniform(right_low_ratio[0]*h, right_low_ratio[1]*h)\n","        right_high_factor = random.uniform(right_high_ratio[0]*h, right_high_ratio[1]*h)\n","\n","        tl = (0, left_high_factor)\n","        bl = (0, left_high_factor+left_low_factor)\n","\n","        tr = (w, right_high_factor)\n","        br = (w, right_high_factor+right_low_factor)\n","\n","        contour = np.array([tl, tr, br, bl], dtype=np.int32)\n","        mask = np.zeros([h, w, 3],np.uint8)\n","        cv2.fillPoly(mask,[contour],(255, 255, 255))\n","\n","        #선택된 영역의 평균 RGB 구하기\n","        rgb_img = img.convert('RGB')\n","        red=0;green=0;blue=0;pixel=0\n","        for x in range(0, w):\n","          for y in range(0, h):\n","            if mask[h-y-1][x][0]==0:\n","              continue\n","            r, g, b = rgb_img.getpixel((x, y))\n","            red+=r\n","            green+=g\n","            blue+=b\n","            pixel+=1\n","        red/=pixel\n","        green/=pixel\n","        blue/=pixel\n","        print(red, green, blue)\n","\n","        #평균 RGB 적용\n","        cv2.fillPoly(mask,[contour],(red, green, blue))\n","\n","        inverted_mask = cv2.bitwise_not(mask)\n","        mask_pil = Image.fromarray(mask)\n","        inverted_mask_pil = Image.fromarray(inverted_mask)\n","\n","        low_contrast = TF.adjust_brightness(img, low_contrast_factor)\n","        low_contrast_masked = ImageChops.multiply(low_contrast, mask_pil)\n","        high_contrast = TF.adjust_brightness(img, high_contrast_factor)\n","        high_contrast_masked = ImageChops.multiply(high_contrast, inverted_mask_pil)\n","\n","        return ImageChops.add(low_contrast_masked, high_contrast_masked)\n","        \n","        \n","\n","    def __call__(self, img):\n","        if random.uniform(0, 1) < self.p:\n","            img = self.process(img, self.high_ratio, self.low_ratio, \\\n","            self.left_low_ratio, self.left_high_ratio, self.right_low_ratio, \\\n","            self.right_high_ratio)\n","            return img\n","        else:\n","            return img\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9s8oXpzdMvol"},"source":["##**Parameter Settings**"]},{"cell_type":"code","metadata":{"id":"Pjeqawi9cNK6"},"source":["dataset = 'cifar100' # cifar10 or cifar100\n","model = 'resnet34' # resnet18, resnet50, resnet101\n","batch_size = 128  # Input batch size for training (default: 128)\n","epochs = 150 # Number of epochs to train (default: 200)\n","learning_rate = 0.1 # Learning rate\n","data_augmentation = True # Traditional data augmentation such as augmantation by flipping and cropping?\n","cutout = False # Apply Cutout?\n","n_holes = 1 # Number of holes to cut out from image\n","length = 16 # Length of the holes\n","seed = 0 # Random seed (default: 0)\n","print_freq = 30\n","cuda = torch.cuda.is_available()\n","cudnn.benchmark = True  # Should make training should go faster for large models\n","\n","# What we need for our data augmentation\n","randomshadows = True\n","\n","torch.manual_seed(seed)\n","if cuda:\n","    torch.cuda.manual_seed(seed)\n","\n","test_id = dataset + '_' + model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eXL_PBj6cVoe"},"source":["##**Load and preprocess data**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":313,"referenced_widgets":["75d6baf1f9ec4d239abc7fa6be6621a6","3134012ec8994071b4cafabe92d036ee","672987e229f04fdc868b703da835773c","cba50a585e6d435f9902de418ea2cf5b","b792c6e307c8462588ef08d4fd73dd6b","d54d05f638614d2398adb6f936f58e41","7c4737e1f82841f99cbf245b436d7cf2","eb756e77d722477e9c89d2adb9ccb5ba"]},"id":"dvQjH3T9caYs","executionInfo":{"status":"ok","timestamp":1620877949680,"user_tz":-540,"elapsed":6366,"user":{"displayName":"장예원","photoUrl":"","userId":"13687321636375718239"}},"outputId":"a77118db-c4af-4400-e12d-11c7000fdf58"},"source":["# Image Preprocessing\n","normalize = transforms.Normalize(mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n","                                     std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n","\n","train_transform = transforms.Compose([])\n","if data_augmentation:\n","    train_transform.transforms.append(transforms.RandomCrop(32, padding=4))\n","    train_transform.transforms.append(transforms.RandomHorizontalFlip())\n","if randomshadows:\n","    p = np.round(np.arange(0, 1.1, 0.1), 2)\n","    for i_p in p:\n","        print('RSH p value: ', i_p)\n","        data_transforms = {\n","            'train': transforms.Compose([\n","                # For CIFAR-10 and CIFAR100, either change the model or resize images to 64x64 (uncomment the transform below)\n","                # transforms.Resize(64),\n","                DiskAugmenter(local_mask=(120, 160), global_mask=(40, 80), augmenting_prob=0),\n","                RandomShadows(p=i_p, high_ratio=(1,2), low_ratio=(0,1), \\\n","                left_low_ratio=(0.4,0.8), left_high_ratio=(0,0.3), right_low_ratio=(0.4,0.8),\n","                right_high_ratio = (0,0.3)), ## high means from top of image, low means from top to bottom low\n","                RandomGamma(gamma_p = 0, gamma_ratio=(0, 1.5)),\n","                RandomColorJitter(p = 0, brightness_ratio=(0,2), contrast_ratio=(0,2), \\\n","                           saturation_ratio=(0,2), hue_ratio=(-0.5,0.5)),\n","                \n","                #transforms.ToTensor(),\n","                #transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","                # transforms.RandomErasing(p=i_p)\n","            ]),\n","            'val': transforms.Compose([\n","                # For CIFAR-10 and CIFAR100, either change the model or resize images to 64x64 (uncomment the transform below)\n","                # transforms.Resize(64),\n","                RandomShadows(p=1, high_ratio=(1,2), low_ratio=(0,1), \\\n","                left_low_ratio=(0.4,0.8), left_high_ratio=(0,0.3), right_low_ratio=(0.4,0.8),\n","                right_high_ratio = (0,0.3)), ## high means from top of image, low means from top to bottom low\n","                #transforms.ToTensor(),\n","                #transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","            ]),\n","            'test': transforms.Compose([\n","                #transforms.ToTensor(),\n","            ])\n","        }\n","train_transform.transforms.append(transforms.ToTensor())\n","train_transform.transforms.append(normalize)\n","if cutout:\n","    train_transform.transforms.append(Cutout(n_holes=n_holes, length=length))\n","\n","\n","test_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    normalize])\n","\n","if dataset == 'cifar10':\n","    num_classes = 10\n","    train_dataset = datasets.CIFAR10(root='data/',\n","                                     train=True,\n","                                     transform=train_transform,\n","                                     download=True)\n","\n","    test_dataset = datasets.CIFAR10(root='data/',\n","                                    train=False,\n","                                    transform=test_transform,\n","                                    download=True)\n","elif dataset == 'cifar100':\n","    num_classes = 100\n","    train_dataset = datasets.CIFAR100(root='data/',\n","                                      train=True,\n","                                      transform=train_transform,\n","                                      download=True)\n","\n","    test_dataset = datasets.CIFAR100(root='data/',\n","                                     train=False,\n","                                     transform=test_transform,\n","                                     download=True)\n","\n","\n","# Data Loader (Input Pipeline)\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                           batch_size=batch_size,\n","                                           shuffle=True,\n","                                           pin_memory=True,\n","                                           num_workers=2)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n","                                          batch_size=batch_size,\n","                                          shuffle=False,\n","                                          pin_memory=True,\n","                                          num_workers=2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["RSH p value:  0.0\n","RSH p value:  0.1\n","RSH p value:  0.2\n","RSH p value:  0.3\n","RSH p value:  0.4\n","RSH p value:  0.5\n","RSH p value:  0.6\n","RSH p value:  0.7\n","RSH p value:  0.8\n","RSH p value:  0.9\n","RSH p value:  1.0\n","Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to data/cifar-100-python.tar.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"75d6baf1f9ec4d239abc7fa6be6621a6","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=169001437.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Extracting data/cifar-100-python.tar.gz to data/\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YjC0ghDLcryS"},"source":["model_save_name = 'ACRS_best.pt'\n","path = F\"/content/drive/MyDrive/Colab Notebooks/{model_save_name}\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gITLQIAr9lAZ"},"source":["##**Main Training**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kMLQDD_ZcKYB","outputId":"7bea81f3-5d06-4c58-db1c-cfdfcfcd484b"},"source":["def train(train_loader, epoch, model, optimizer, criterion):\n","    batch_time = AverageMeter('Time', ':6.3f')\n","    losses = AverageMeter('Loss', ':.4e')\n","    top1 = AverageMeter('Acc@1', ':6.2f')\n","    top5 = AverageMeter('Acc@5', ':6.2f')\n","    progress = ProgressMeter(len(train_loader), batch_time, losses,\n","                             top1, top5, prefix=\"Epoch: [{}]\".format(epoch))\n","    # switch to train mode\n","    model.train()\n","\n","    end = time.time()\n","    for i, (input, target) in enumerate(train_loader):\n","        # measure data loading time\n","        input = input.cuda()\n","        target = target.cuda()\n","\n","        # compute output\n","        output = model(input)\n","        loss = criterion(output, target)\n","\n","        # measure accuracy and record loss, accuracy \n","        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n","        losses.update(loss.item(), input.size(0))\n","        top1.update(acc1[0].item(), input.size(0))\n","        top5.update(acc5[0].item(), input.size(0))\n","\n","        # compute gradient and do SGD step\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # measure elapsed time\n","        batch_time.update(time.time() - end)\n","        end = time.time()\n","\n","        if i % print_freq == 0:\n","            progress.print(i)\n","\n","    print('==> Train Accuracy: Acc@1 {top1.avg:.3f} || Acc@5 {top5.avg:.3f}'.format(top1=top1, top5=top5))\n","    return top1.avg\n","\n","def test(test_loader,epoch, model):\n","    top1 = AverageMeter('Acc@1', ':6.2f')\n","    top5 = AverageMeter('Acc@5', ':6.2f')\n","    model.eval()\n","    for i,(input,target) in enumerate(test_loader):\n","        input = input.cuda()\n","        target = target.cuda()\n","\n","        output = model(input)\n","        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n","        top1.update(acc1[0].item(), input.size(0))\n","        top5.update(acc5[0].item(), input.size(0))\n","    print('==> Test Accuracy:  Acc@1 {top1.avg:.3f} || Acc@5 {top5.avg:.3f}'.format(top1=top1, top5=top5))\n","    return top1.avg\n","\n","model = ResNet34(num_classes=num_classes).cuda()\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate,momentum=0.9, nesterov=True, weight_decay=5e-4)\n","\n","scheduler = MultiStepLR(optimizer, milestones=[60, 90, 120], gamma=0.2)\n","\n","criterion = torch.nn.CrossEntropyLoss().cuda()\n","###########################################################\n","best_acc = 0\n","for epoch in range(epochs):\n","    print(\"\\n----- epoch: {}, lr: {} -----\".format(\n","        epoch, optimizer.param_groups[0][\"lr\"]))\n","\n","    # train for one epoch\n","    start_time = time.time()\n","    train(train_loader, epoch, model, optimizer, criterion)\n","    test_acc = test(test_loader,epoch,model)\n","\n","    elapsed_time = time.time() - start_time\n","    print('==> {:.2f} seconds to train this epoch\\n'.format(elapsed_time))\n","    # learning rate scheduling\n","    scheduler.step()\n","    \n","    # Save model for best accuracy\n","    if best_acc < test_acc:\n","        best_acc = test_acc\n","        torch.save(model.state_dict(), path)\n","\n","torch.save(model.state_dict(), \"/content/drive/MyDrive/Colab Notebooks/ACRS_latest.pt\")\n","print(f\"Best Top-1 Accuracy: {best_acc}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","----- epoch: 0, lr: 0.1 -----\n","Epoch: [0][  0/391]\tTime  1.236 ( 1.236)\tLoss 4.7548e+00 (4.7548e+00)\tAcc@1   1.56 (  1.56)\tAcc@5   5.47 (  5.47)\n","Epoch: [0][ 30/391]\tTime  0.157 ( 0.192)\tLoss 4.5985e+00 (5.2545e+00)\tAcc@1   1.56 (  1.11)\tAcc@5  10.16 (  5.82)\n","Epoch: [0][ 60/391]\tTime  0.161 ( 0.177)\tLoss 4.4360e+00 (4.8964e+00)\tAcc@1   3.91 (  1.83)\tAcc@5  10.94 (  7.94)\n","Epoch: [0][ 90/391]\tTime  0.161 ( 0.172)\tLoss 4.2655e+00 (4.7141e+00)\tAcc@1   1.56 (  2.33)\tAcc@5  19.53 ( 10.64)\n","Epoch: [0][120/391]\tTime  0.163 ( 0.169)\tLoss 4.1408e+00 (4.5977e+00)\tAcc@1   5.47 (  2.89)\tAcc@5  25.00 ( 12.55)\n","Epoch: [0][150/391]\tTime  0.162 ( 0.168)\tLoss 4.1073e+00 (4.5007e+00)\tAcc@1   9.38 (  3.63)\tAcc@5  25.00 ( 14.68)\n","Epoch: [0][180/391]\tTime  0.162 ( 0.167)\tLoss 4.0436e+00 (4.4240e+00)\tAcc@1   4.69 (  4.09)\tAcc@5  21.88 ( 16.24)\n","Epoch: [0][210/391]\tTime  0.164 ( 0.167)\tLoss 3.9671e+00 (4.3585e+00)\tAcc@1   9.38 (  4.65)\tAcc@5  29.69 ( 17.87)\n","Epoch: [0][240/391]\tTime  0.165 ( 0.166)\tLoss 3.9573e+00 (4.3033e+00)\tAcc@1   9.38 (  5.17)\tAcc@5  31.25 ( 19.25)\n","Epoch: [0][270/391]\tTime  0.165 ( 0.166)\tLoss 3.9673e+00 (4.2565e+00)\tAcc@1   7.81 (  5.60)\tAcc@5  26.56 ( 20.34)\n","Epoch: [0][300/391]\tTime  0.165 ( 0.166)\tLoss 3.6941e+00 (4.2151e+00)\tAcc@1  14.84 (  6.02)\tAcc@5  37.50 ( 21.45)\n","Epoch: [0][330/391]\tTime  0.162 ( 0.166)\tLoss 4.0693e+00 (4.1808e+00)\tAcc@1   6.25 (  6.33)\tAcc@5  26.56 ( 22.32)\n","Epoch: [0][360/391]\tTime  0.167 ( 0.166)\tLoss 3.7359e+00 (4.1456e+00)\tAcc@1   6.25 (  6.67)\tAcc@5  33.59 ( 23.28)\n","Epoch: [0][390/391]\tTime  0.700 ( 0.167)\tLoss 3.7673e+00 (4.1154e+00)\tAcc@1  16.25 (  7.04)\tAcc@5  35.00 ( 24.10)\n","==> Train Accuracy: Acc@1 7.038 || Acc@5 24.096\n","==> Test Accuracy:  Acc@1 12.440 || Acc@5 36.680\n","==> 69.57 seconds to train this epoch\n","\n","\n","----- epoch: 1, lr: 0.1 -----\n","Epoch: [1][  0/391]\tTime  0.222 ( 0.222)\tLoss 3.8065e+00 (3.8065e+00)\tAcc@1   7.03 (  7.03)\tAcc@5  31.25 ( 31.25)\n","Epoch: [1][ 30/391]\tTime  0.168 ( 0.168)\tLoss 3.3925e+00 (3.6379e+00)\tAcc@1  14.06 ( 12.95)\tAcc@5  41.41 ( 38.81)\n","Epoch: [1][ 60/391]\tTime  0.168 ( 0.168)\tLoss 3.5923e+00 (3.6180e+00)\tAcc@1  11.72 ( 13.56)\tAcc@5  33.59 ( 38.54)\n","Epoch: [1][ 90/391]\tTime  0.167 ( 0.168)\tLoss 3.5786e+00 (3.6048e+00)\tAcc@1  11.72 ( 13.59)\tAcc@5  39.84 ( 38.88)\n","Epoch: [1][120/391]\tTime  0.168 ( 0.168)\tLoss 3.6390e+00 (3.5876e+00)\tAcc@1  16.41 ( 14.15)\tAcc@5  36.72 ( 39.46)\n","Epoch: [1][150/391]\tTime  0.168 ( 0.168)\tLoss 3.3552e+00 (3.5729e+00)\tAcc@1  16.41 ( 14.28)\tAcc@5  42.97 ( 39.84)\n","Epoch: [1][180/391]\tTime  0.168 ( 0.168)\tLoss 3.4362e+00 (3.5642e+00)\tAcc@1  14.06 ( 14.39)\tAcc@5  43.75 ( 39.96)\n","Epoch: [1][210/391]\tTime  0.169 ( 0.168)\tLoss 3.2674e+00 (3.5443e+00)\tAcc@1  14.84 ( 14.54)\tAcc@5  50.78 ( 40.52)\n","Epoch: [1][240/391]\tTime  0.168 ( 0.168)\tLoss 3.1979e+00 (3.5271e+00)\tAcc@1  19.53 ( 14.98)\tAcc@5  50.78 ( 40.90)\n","Epoch: [1][270/391]\tTime  0.170 ( 0.168)\tLoss 3.3751e+00 (3.5057e+00)\tAcc@1  13.28 ( 15.26)\tAcc@5  47.66 ( 41.54)\n","Epoch: [1][300/391]\tTime  0.171 ( 0.169)\tLoss 3.2673e+00 (3.4871e+00)\tAcc@1  16.41 ( 15.61)\tAcc@5  48.44 ( 42.10)\n","Epoch: [1][330/391]\tTime  0.166 ( 0.169)\tLoss 3.3196e+00 (3.4721e+00)\tAcc@1  17.97 ( 15.89)\tAcc@5  46.09 ( 42.61)\n","Epoch: [1][360/391]\tTime  0.170 ( 0.169)\tLoss 3.2605e+00 (3.4559e+00)\tAcc@1  21.09 ( 16.22)\tAcc@5  47.66 ( 43.04)\n","Epoch: [1][390/391]\tTime  0.153 ( 0.169)\tLoss 3.1766e+00 (3.4375e+00)\tAcc@1  23.75 ( 16.60)\tAcc@5  46.25 ( 43.52)\n","==> Train Accuracy: Acc@1 16.600 || Acc@5 43.518\n","==> Test Accuracy:  Acc@1 22.140 || Acc@5 51.320\n","==> 70.27 seconds to train this epoch\n","\n","\n","----- epoch: 2, lr: 0.1 -----\n","Epoch: [2][  0/391]\tTime  0.214 ( 0.214)\tLoss 3.0722e+00 (3.0722e+00)\tAcc@1  28.91 ( 28.91)\tAcc@5  57.81 ( 57.81)\n","Epoch: [2][ 30/391]\tTime  0.171 ( 0.172)\tLoss 3.1887e+00 (3.1646e+00)\tAcc@1  17.97 ( 22.18)\tAcc@5  49.22 ( 51.49)\n","Epoch: [2][ 60/391]\tTime  0.169 ( 0.172)\tLoss 2.8830e+00 (3.1388e+00)\tAcc@1  26.56 ( 22.54)\tAcc@5  53.91 ( 51.61)\n","Epoch: [2][ 90/391]\tTime  0.172 ( 0.172)\tLoss 3.2789e+00 (3.1253e+00)\tAcc@1  21.09 ( 22.90)\tAcc@5  46.88 ( 51.87)\n","Epoch: [2][120/391]\tTime  0.174 ( 0.172)\tLoss 3.2537e+00 (3.1120e+00)\tAcc@1  21.09 ( 23.25)\tAcc@5  51.56 ( 52.36)\n","Epoch: [2][150/391]\tTime  0.173 ( 0.172)\tLoss 2.9332e+00 (3.0972e+00)\tAcc@1  28.91 ( 23.36)\tAcc@5  58.59 ( 52.86)\n","Epoch: [2][180/391]\tTime  0.171 ( 0.172)\tLoss 3.0151e+00 (3.0895e+00)\tAcc@1  28.12 ( 23.47)\tAcc@5  52.34 ( 53.02)\n","Epoch: [2][210/391]\tTime  0.173 ( 0.172)\tLoss 3.0029e+00 (3.0839e+00)\tAcc@1  28.91 ( 23.62)\tAcc@5  53.12 ( 53.26)\n","Epoch: [2][240/391]\tTime  0.172 ( 0.172)\tLoss 2.9579e+00 (3.0681e+00)\tAcc@1  23.44 ( 23.95)\tAcc@5  56.25 ( 53.68)\n","Epoch: [2][270/391]\tTime  0.176 ( 0.172)\tLoss 2.7192e+00 (3.0524e+00)\tAcc@1  29.69 ( 24.28)\tAcc@5  62.50 ( 54.09)\n","Epoch: [2][300/391]\tTime  0.172 ( 0.172)\tLoss 2.9483e+00 (3.0382e+00)\tAcc@1  28.91 ( 24.45)\tAcc@5  54.69 ( 54.39)\n","Epoch: [2][330/391]\tTime  0.171 ( 0.172)\tLoss 2.8149e+00 (3.0264e+00)\tAcc@1  32.81 ( 24.74)\tAcc@5  63.28 ( 54.70)\n","Epoch: [2][360/391]\tTime  0.177 ( 0.172)\tLoss 2.6370e+00 (3.0140e+00)\tAcc@1  25.78 ( 24.93)\tAcc@5  67.97 ( 55.05)\n","Epoch: [2][390/391]\tTime  0.156 ( 0.172)\tLoss 2.7535e+00 (3.0001e+00)\tAcc@1  32.50 ( 25.17)\tAcc@5  58.75 ( 55.42)\n","==> Train Accuracy: Acc@1 25.174 || Acc@5 55.422\n","==> Test Accuracy:  Acc@1 28.480 || Acc@5 60.060\n","==> 71.64 seconds to train this epoch\n","\n","\n","----- epoch: 3, lr: 0.1 -----\n","Epoch: [3][  0/391]\tTime  0.223 ( 0.223)\tLoss 2.6169e+00 (2.6169e+00)\tAcc@1  33.59 ( 33.59)\tAcc@5  63.28 ( 63.28)\n","Epoch: [3][ 30/391]\tTime  0.174 ( 0.175)\tLoss 2.6174e+00 (2.7415e+00)\tAcc@1  28.12 ( 28.86)\tAcc@5  64.84 ( 62.50)\n","Epoch: [3][ 60/391]\tTime  0.171 ( 0.174)\tLoss 2.7178e+00 (2.7289e+00)\tAcc@1  27.34 ( 29.53)\tAcc@5  63.28 ( 62.38)\n","Epoch: [3][ 90/391]\tTime  0.172 ( 0.174)\tLoss 2.7999e+00 (2.7059e+00)\tAcc@1  28.91 ( 30.63)\tAcc@5  58.59 ( 62.37)\n","Epoch: [3][120/391]\tTime  0.175 ( 0.174)\tLoss 2.9592e+00 (2.7190e+00)\tAcc@1  24.22 ( 30.29)\tAcc@5  55.47 ( 62.29)\n","Epoch: [3][150/391]\tTime  0.173 ( 0.174)\tLoss 2.5607e+00 (2.7185e+00)\tAcc@1  30.47 ( 30.42)\tAcc@5  65.62 ( 62.28)\n","Epoch: [3][180/391]\tTime  0.175 ( 0.174)\tLoss 2.8123e+00 (2.7101e+00)\tAcc@1  28.12 ( 30.62)\tAcc@5  53.12 ( 62.45)\n","Epoch: [3][210/391]\tTime  0.174 ( 0.174)\tLoss 2.7987e+00 (2.7025e+00)\tAcc@1  25.78 ( 30.78)\tAcc@5  61.72 ( 62.77)\n","Epoch: [3][240/391]\tTime  0.175 ( 0.174)\tLoss 2.9215e+00 (2.6914e+00)\tAcc@1  24.22 ( 30.94)\tAcc@5  62.50 ( 63.09)\n","Epoch: [3][270/391]\tTime  0.175 ( 0.174)\tLoss 2.9035e+00 (2.6795e+00)\tAcc@1  25.78 ( 31.19)\tAcc@5  60.94 ( 63.30)\n","Epoch: [3][300/391]\tTime  0.175 ( 0.174)\tLoss 2.5716e+00 (2.6679e+00)\tAcc@1  28.91 ( 31.30)\tAcc@5  68.75 ( 63.53)\n","Epoch: [3][330/391]\tTime  0.174 ( 0.174)\tLoss 2.4563e+00 (2.6538e+00)\tAcc@1  36.72 ( 31.62)\tAcc@5  71.88 ( 63.76)\n","Epoch: [3][360/391]\tTime  0.174 ( 0.174)\tLoss 2.4775e+00 (2.6443e+00)\tAcc@1  33.59 ( 31.82)\tAcc@5  67.19 ( 63.97)\n","Epoch: [3][390/391]\tTime  0.158 ( 0.174)\tLoss 2.2894e+00 (2.6307e+00)\tAcc@1  38.75 ( 32.15)\tAcc@5  75.00 ( 64.34)\n","==> Train Accuracy: Acc@1 32.154 || Acc@5 64.340\n","==> Test Accuracy:  Acc@1 33.360 || Acc@5 66.730\n","==> 72.45 seconds to train this epoch\n","\n","\n","----- epoch: 4, lr: 0.1 -----\n","Epoch: [4][  0/391]\tTime  0.219 ( 0.219)\tLoss 2.3777e+00 (2.3777e+00)\tAcc@1  37.50 ( 37.50)\tAcc@5  70.31 ( 70.31)\n","Epoch: [4][ 30/391]\tTime  0.174 ( 0.176)\tLoss 2.4805e+00 (2.3644e+00)\tAcc@1  32.81 ( 37.30)\tAcc@5  69.53 ( 70.56)\n","Epoch: [4][ 60/391]\tTime  0.176 ( 0.176)\tLoss 2.3827e+00 (2.3732e+00)\tAcc@1  37.50 ( 37.18)\tAcc@5  66.41 ( 70.31)\n","Epoch: [4][ 90/391]\tTime  0.176 ( 0.176)\tLoss 2.2522e+00 (2.3677e+00)\tAcc@1  47.66 ( 37.73)\tAcc@5  71.09 ( 70.24)\n","Epoch: [4][120/391]\tTime  0.175 ( 0.176)\tLoss 2.1989e+00 (2.3551e+00)\tAcc@1  38.28 ( 37.96)\tAcc@5  71.88 ( 70.44)\n","Epoch: [4][150/391]\tTime  0.177 ( 0.176)\tLoss 2.3194e+00 (2.3483e+00)\tAcc@1  39.06 ( 37.98)\tAcc@5  69.53 ( 70.59)\n","Epoch: [4][180/391]\tTime  0.175 ( 0.176)\tLoss 2.4942e+00 (2.3417e+00)\tAcc@1  38.28 ( 38.25)\tAcc@5  65.62 ( 70.63)\n","Epoch: [4][210/391]\tTime  0.175 ( 0.176)\tLoss 2.1581e+00 (2.3343e+00)\tAcc@1  40.62 ( 38.32)\tAcc@5  71.09 ( 70.77)\n","Epoch: [4][240/391]\tTime  0.174 ( 0.176)\tLoss 2.1410e+00 (2.3281e+00)\tAcc@1  40.62 ( 38.50)\tAcc@5  76.56 ( 70.99)\n","Epoch: [4][270/391]\tTime  0.173 ( 0.176)\tLoss 2.3404e+00 (2.3224e+00)\tAcc@1  35.16 ( 38.57)\tAcc@5  74.22 ( 71.24)\n","Epoch: [4][300/391]\tTime  0.174 ( 0.176)\tLoss 2.4515e+00 (2.3181e+00)\tAcc@1  32.03 ( 38.71)\tAcc@5  68.75 ( 71.29)\n","Epoch: [4][330/391]\tTime  0.177 ( 0.176)\tLoss 2.0312e+00 (2.3093e+00)\tAcc@1  46.88 ( 38.92)\tAcc@5  78.91 ( 71.49)\n","Epoch: [4][360/391]\tTime  0.178 ( 0.176)\tLoss 2.4709e+00 (2.3073e+00)\tAcc@1  36.72 ( 38.97)\tAcc@5  65.62 ( 71.53)\n","Epoch: [4][390/391]\tTime  0.158 ( 0.176)\tLoss 2.1629e+00 (2.2969e+00)\tAcc@1  40.00 ( 39.19)\tAcc@5  75.00 ( 71.81)\n","==> Train Accuracy: Acc@1 39.190 || Acc@5 71.810\n","==> Test Accuracy:  Acc@1 39.030 || Acc@5 71.400\n","==> 73.05 seconds to train this epoch\n","\n","\n","----- epoch: 5, lr: 0.1 -----\n","Epoch: [5][  0/391]\tTime  0.233 ( 0.233)\tLoss 2.1248e+00 (2.1248e+00)\tAcc@1  43.75 ( 43.75)\tAcc@5  75.78 ( 75.78)\n","Epoch: [5][ 30/391]\tTime  0.177 ( 0.177)\tLoss 1.8299e+00 (2.0575e+00)\tAcc@1  50.00 ( 44.61)\tAcc@5  80.47 ( 76.79)\n","Epoch: [5][ 60/391]\tTime  0.174 ( 0.176)\tLoss 2.1041e+00 (2.0671e+00)\tAcc@1  42.97 ( 44.07)\tAcc@5  75.00 ( 76.10)\n","Epoch: [5][ 90/391]\tTime  0.177 ( 0.176)\tLoss 2.0859e+00 (2.0907e+00)\tAcc@1  37.50 ( 43.69)\tAcc@5  80.47 ( 75.90)\n","Epoch: [5][120/391]\tTime  0.177 ( 0.176)\tLoss 2.2843e+00 (2.0951e+00)\tAcc@1  36.72 ( 43.49)\tAcc@5  71.88 ( 75.85)\n","Epoch: [5][150/391]\tTime  0.176 ( 0.176)\tLoss 1.8340e+00 (2.0897e+00)\tAcc@1  52.34 ( 43.63)\tAcc@5  81.25 ( 76.14)\n","Epoch: [5][180/391]\tTime  0.176 ( 0.176)\tLoss 1.8663e+00 (2.0827e+00)\tAcc@1  50.00 ( 43.81)\tAcc@5  82.03 ( 76.36)\n","Epoch: [5][210/391]\tTime  0.177 ( 0.176)\tLoss 2.0981e+00 (2.0772e+00)\tAcc@1  42.97 ( 43.81)\tAcc@5  71.88 ( 76.38)\n","Epoch: [5][240/391]\tTime  0.177 ( 0.176)\tLoss 2.1441e+00 (2.0763e+00)\tAcc@1  44.53 ( 43.82)\tAcc@5  73.44 ( 76.44)\n","Epoch: [5][270/391]\tTime  0.177 ( 0.176)\tLoss 2.1158e+00 (2.0742e+00)\tAcc@1  39.84 ( 43.97)\tAcc@5  75.00 ( 76.54)\n","Epoch: [5][300/391]\tTime  0.176 ( 0.176)\tLoss 2.0452e+00 (2.0623e+00)\tAcc@1  44.53 ( 44.17)\tAcc@5  78.91 ( 76.79)\n","Epoch: [5][330/391]\tTime  0.177 ( 0.176)\tLoss 2.0228e+00 (2.0564e+00)\tAcc@1  42.97 ( 44.26)\tAcc@5  76.56 ( 76.90)\n","Epoch: [5][360/391]\tTime  0.174 ( 0.176)\tLoss 1.8188e+00 (2.0497e+00)\tAcc@1  50.00 ( 44.49)\tAcc@5  82.03 ( 77.02)\n","Epoch: [5][390/391]\tTime  0.160 ( 0.176)\tLoss 2.2291e+00 (2.0426e+00)\tAcc@1  41.25 ( 44.72)\tAcc@5  70.00 ( 77.14)\n","==> Train Accuracy: Acc@1 44.716 || Acc@5 77.144\n","==> Test Accuracy:  Acc@1 44.320 || Acc@5 76.730\n","==> 73.13 seconds to train this epoch\n","\n","\n","----- epoch: 6, lr: 0.1 -----\n","Epoch: [6][  0/391]\tTime  0.224 ( 0.224)\tLoss 1.9995e+00 (1.9995e+00)\tAcc@1  53.12 ( 53.12)\tAcc@5  77.34 ( 77.34)\n","Epoch: [6][ 30/391]\tTime  0.176 ( 0.177)\tLoss 1.8101e+00 (1.8788e+00)\tAcc@1  46.88 ( 48.24)\tAcc@5  76.56 ( 80.04)\n","Epoch: [6][ 60/391]\tTime  0.172 ( 0.176)\tLoss 1.8441e+00 (1.8656e+00)\tAcc@1  45.31 ( 48.46)\tAcc@5  82.03 ( 80.33)\n","Epoch: [6][ 90/391]\tTime  0.177 ( 0.176)\tLoss 1.9042e+00 (1.8800e+00)\tAcc@1  43.75 ( 48.24)\tAcc@5  82.03 ( 79.95)\n","Epoch: [6][120/391]\tTime  0.176 ( 0.176)\tLoss 1.7680e+00 (1.8692e+00)\tAcc@1  51.56 ( 48.39)\tAcc@5  82.81 ( 80.03)\n","Epoch: [6][150/391]\tTime  0.177 ( 0.176)\tLoss 1.6634e+00 (1.8592e+00)\tAcc@1  53.91 ( 48.71)\tAcc@5  84.38 ( 80.28)\n","Epoch: [6][180/391]\tTime  0.174 ( 0.176)\tLoss 1.7810e+00 (1.8557e+00)\tAcc@1  49.22 ( 48.88)\tAcc@5  83.59 ( 80.25)\n","Epoch: [6][210/391]\tTime  0.176 ( 0.176)\tLoss 1.9866e+00 (1.8563e+00)\tAcc@1  46.88 ( 48.95)\tAcc@5  77.34 ( 80.25)\n","Epoch: [6][240/391]\tTime  0.177 ( 0.176)\tLoss 1.6142e+00 (1.8613e+00)\tAcc@1  53.12 ( 48.77)\tAcc@5  86.72 ( 80.29)\n","Epoch: [6][270/391]\tTime  0.176 ( 0.176)\tLoss 1.6848e+00 (1.8601e+00)\tAcc@1  56.25 ( 48.80)\tAcc@5  80.47 ( 80.35)\n","Epoch: [6][300/391]\tTime  0.176 ( 0.176)\tLoss 1.8334e+00 (1.8559e+00)\tAcc@1  51.56 ( 48.90)\tAcc@5  78.91 ( 80.39)\n","Epoch: [6][330/391]\tTime  0.176 ( 0.176)\tLoss 2.1802e+00 (1.8543e+00)\tAcc@1  43.75 ( 48.85)\tAcc@5  72.66 ( 80.47)\n","Epoch: [6][360/391]\tTime  0.175 ( 0.176)\tLoss 1.9867e+00 (1.8516e+00)\tAcc@1  40.62 ( 48.92)\tAcc@5  80.47 ( 80.48)\n","Epoch: [6][390/391]\tTime  0.161 ( 0.176)\tLoss 1.9390e+00 (1.8470e+00)\tAcc@1  42.50 ( 49.04)\tAcc@5  78.75 ( 80.55)\n","==> Train Accuracy: Acc@1 49.036 || Acc@5 80.552\n","==> Test Accuracy:  Acc@1 41.890 || Acc@5 73.350\n","==> 73.06 seconds to train this epoch\n","\n","\n","----- epoch: 7, lr: 0.1 -----\n","Epoch: [7][  0/391]\tTime  0.223 ( 0.223)\tLoss 1.6325e+00 (1.6325e+00)\tAcc@1  50.00 ( 50.00)\tAcc@5  82.81 ( 82.81)\n","Epoch: [7][ 30/391]\tTime  0.175 ( 0.177)\tLoss 1.8798e+00 (1.6676e+00)\tAcc@1  47.66 ( 52.85)\tAcc@5  81.25 ( 84.05)\n","Epoch: [7][ 60/391]\tTime  0.176 ( 0.176)\tLoss 1.9425e+00 (1.6641e+00)\tAcc@1  46.88 ( 52.83)\tAcc@5  79.69 ( 84.13)\n","Epoch: [7][ 90/391]\tTime  0.175 ( 0.176)\tLoss 1.5980e+00 (1.6848e+00)\tAcc@1  54.69 ( 52.74)\tAcc@5  89.06 ( 83.50)\n","Epoch: [7][120/391]\tTime  0.178 ( 0.176)\tLoss 1.7379e+00 (1.6879e+00)\tAcc@1  50.00 ( 52.85)\tAcc@5  85.16 ( 83.44)\n","Epoch: [7][150/391]\tTime  0.176 ( 0.176)\tLoss 1.8936e+00 (1.6946e+00)\tAcc@1  45.31 ( 52.69)\tAcc@5  78.12 ( 83.37)\n","Epoch: [7][180/391]\tTime  0.176 ( 0.176)\tLoss 1.6730e+00 (1.6944e+00)\tAcc@1  52.34 ( 52.67)\tAcc@5  84.38 ( 83.37)\n","Epoch: [7][210/391]\tTime  0.177 ( 0.176)\tLoss 1.6475e+00 (1.6984e+00)\tAcc@1  52.34 ( 52.56)\tAcc@5  86.72 ( 83.31)\n","Epoch: [7][240/391]\tTime  0.178 ( 0.176)\tLoss 1.8125e+00 (1.7011e+00)\tAcc@1  46.09 ( 52.43)\tAcc@5  78.12 ( 83.23)\n","Epoch: [7][270/391]\tTime  0.181 ( 0.176)\tLoss 1.6214e+00 (1.7053e+00)\tAcc@1  57.03 ( 52.30)\tAcc@5  85.16 ( 83.11)\n","Epoch: [7][300/391]\tTime  0.177 ( 0.176)\tLoss 1.8844e+00 (1.7082e+00)\tAcc@1  46.09 ( 52.29)\tAcc@5  82.03 ( 83.05)\n","Epoch: [7][330/391]\tTime  0.176 ( 0.176)\tLoss 1.5816e+00 (1.7123e+00)\tAcc@1  54.69 ( 52.16)\tAcc@5  85.16 ( 83.01)\n","Epoch: [7][360/391]\tTime  0.177 ( 0.176)\tLoss 1.7678e+00 (1.7124e+00)\tAcc@1  54.69 ( 52.19)\tAcc@5  85.16 ( 83.02)\n","Epoch: [7][390/391]\tTime  0.158 ( 0.176)\tLoss 1.5428e+00 (1.7127e+00)\tAcc@1  61.25 ( 52.21)\tAcc@5  83.75 ( 83.00)\n","==> Train Accuracy: Acc@1 52.214 || Acc@5 83.004\n","==> Test Accuracy:  Acc@1 46.850 || Acc@5 77.530\n","==> 73.12 seconds to train this epoch\n","\n","\n","----- epoch: 8, lr: 0.1 -----\n","Epoch: [8][  0/391]\tTime  0.240 ( 0.240)\tLoss 1.6823e+00 (1.6823e+00)\tAcc@1  49.22 ( 49.22)\tAcc@5  85.16 ( 85.16)\n","Epoch: [8][ 30/391]\tTime  0.177 ( 0.178)\tLoss 1.8482e+00 (1.5977e+00)\tAcc@1  46.88 ( 54.64)\tAcc@5  79.69 ( 85.23)\n","Epoch: [8][ 60/391]\tTime  0.175 ( 0.177)\tLoss 1.5543e+00 (1.5785e+00)\tAcc@1  48.44 ( 55.19)\tAcc@5  89.06 ( 85.80)\n","Epoch: [8][ 90/391]\tTime  0.173 ( 0.177)\tLoss 1.5280e+00 (1.5687e+00)\tAcc@1  61.72 ( 55.38)\tAcc@5  85.94 ( 85.82)\n","Epoch: [8][120/391]\tTime  0.177 ( 0.176)\tLoss 1.5632e+00 (1.5892e+00)\tAcc@1  54.69 ( 55.00)\tAcc@5  85.94 ( 85.48)\n","Epoch: [8][150/391]\tTime  0.176 ( 0.176)\tLoss 1.5215e+00 (1.5945e+00)\tAcc@1  55.47 ( 54.94)\tAcc@5  85.94 ( 85.29)\n","Epoch: [8][180/391]\tTime  0.176 ( 0.176)\tLoss 1.4009e+00 (1.5898e+00)\tAcc@1  60.16 ( 55.13)\tAcc@5  85.16 ( 85.21)\n","Epoch: [8][210/391]\tTime  0.175 ( 0.176)\tLoss 1.7227e+00 (1.6001e+00)\tAcc@1  53.91 ( 54.94)\tAcc@5  82.81 ( 84.96)\n","Epoch: [8][240/391]\tTime  0.174 ( 0.176)\tLoss 1.6320e+00 (1.6014e+00)\tAcc@1  47.66 ( 54.91)\tAcc@5  85.94 ( 84.98)\n","Epoch: [8][270/391]\tTime  0.176 ( 0.176)\tLoss 1.9249e+00 (1.6090e+00)\tAcc@1  48.44 ( 54.70)\tAcc@5  75.78 ( 84.79)\n","Epoch: [8][300/391]\tTime  0.178 ( 0.176)\tLoss 1.5775e+00 (1.6065e+00)\tAcc@1  52.34 ( 54.72)\tAcc@5  83.59 ( 84.81)\n","Epoch: [8][330/391]\tTime  0.176 ( 0.176)\tLoss 1.7801e+00 (1.6124e+00)\tAcc@1  53.12 ( 54.55)\tAcc@5  82.03 ( 84.77)\n","Epoch: [8][360/391]\tTime  0.177 ( 0.176)\tLoss 1.7935e+00 (1.6142e+00)\tAcc@1  51.56 ( 54.51)\tAcc@5  80.47 ( 84.77)\n","Epoch: [8][390/391]\tTime  0.163 ( 0.176)\tLoss 1.4773e+00 (1.6153e+00)\tAcc@1  60.00 ( 54.48)\tAcc@5  85.00 ( 84.77)\n","==> Train Accuracy: Acc@1 54.478 || Acc@5 84.770\n","==> Test Accuracy:  Acc@1 38.330 || Acc@5 68.290\n","==> 73.17 seconds to train this epoch\n","\n","\n","----- epoch: 9, lr: 0.1 -----\n","Epoch: [9][  0/391]\tTime  0.223 ( 0.223)\tLoss 1.6383e+00 (1.6383e+00)\tAcc@1  53.91 ( 53.91)\tAcc@5  82.03 ( 82.03)\n","Epoch: [9][ 30/391]\tTime  0.175 ( 0.177)\tLoss 1.3440e+00 (1.5099e+00)\tAcc@1  60.94 ( 56.55)\tAcc@5  90.62 ( 86.34)\n","Epoch: [9][ 60/391]\tTime  0.175 ( 0.176)\tLoss 1.6267e+00 (1.5021e+00)\tAcc@1  54.69 ( 56.24)\tAcc@5  86.72 ( 86.83)\n","Epoch: [9][ 90/391]\tTime  0.177 ( 0.176)\tLoss 1.4649e+00 (1.5108e+00)\tAcc@1  59.38 ( 55.94)\tAcc@5  89.84 ( 86.95)\n","Epoch: [9][120/391]\tTime  0.177 ( 0.176)\tLoss 1.5446e+00 (1.5189e+00)\tAcc@1  56.25 ( 56.00)\tAcc@5  86.72 ( 86.71)\n","Epoch: [9][150/391]\tTime  0.176 ( 0.176)\tLoss 1.5677e+00 (1.5156e+00)\tAcc@1  60.16 ( 56.37)\tAcc@5  84.38 ( 86.73)\n","Epoch: [9][180/391]\tTime  0.175 ( 0.176)\tLoss 1.7264e+00 (1.5163e+00)\tAcc@1  50.78 ( 56.57)\tAcc@5  79.69 ( 86.56)\n","Epoch: [9][210/391]\tTime  0.175 ( 0.176)\tLoss 1.7133e+00 (1.5157e+00)\tAcc@1  53.12 ( 56.63)\tAcc@5  85.16 ( 86.60)\n","Epoch: [9][240/391]\tTime  0.178 ( 0.176)\tLoss 1.5028e+00 (1.5243e+00)\tAcc@1  62.50 ( 56.50)\tAcc@5  85.16 ( 86.40)\n","Epoch: [9][270/391]\tTime  0.177 ( 0.176)\tLoss 1.3990e+00 (1.5286e+00)\tAcc@1  60.94 ( 56.41)\tAcc@5  88.28 ( 86.32)\n","Epoch: [9][300/391]\tTime  0.177 ( 0.176)\tLoss 1.5816e+00 (1.5284e+00)\tAcc@1  56.25 ( 56.43)\tAcc@5  82.81 ( 86.28)\n","Epoch: [9][330/391]\tTime  0.177 ( 0.176)\tLoss 1.6146e+00 (1.5292e+00)\tAcc@1  55.47 ( 56.44)\tAcc@5  83.59 ( 86.23)\n","Epoch: [9][360/391]\tTime  0.175 ( 0.176)\tLoss 1.4589e+00 (1.5284e+00)\tAcc@1  58.59 ( 56.50)\tAcc@5  86.72 ( 86.28)\n","Epoch: [9][390/391]\tTime  0.160 ( 0.176)\tLoss 1.7097e+00 (1.5293e+00)\tAcc@1  48.75 ( 56.48)\tAcc@5  86.25 ( 86.25)\n","==> Train Accuracy: Acc@1 56.478 || Acc@5 86.246\n","==> Test Accuracy:  Acc@1 52.300 || Acc@5 82.490\n","==> 73.13 seconds to train this epoch\n","\n","\n","----- epoch: 10, lr: 0.1 -----\n","Epoch: [10][  0/391]\tTime  0.238 ( 0.238)\tLoss 1.3171e+00 (1.3171e+00)\tAcc@1  59.38 ( 59.38)\tAcc@5  89.84 ( 89.84)\n","Epoch: [10][ 30/391]\tTime  0.174 ( 0.177)\tLoss 1.5579e+00 (1.4353e+00)\tAcc@1  57.03 ( 59.35)\tAcc@5  84.38 ( 87.53)\n","Epoch: [10][ 60/391]\tTime  0.169 ( 0.177)\tLoss 1.4511e+00 (1.4225e+00)\tAcc@1  57.03 ( 59.45)\tAcc@5  89.84 ( 87.96)\n","Epoch: [10][ 90/391]\tTime  0.175 ( 0.176)\tLoss 1.2821e+00 (1.4346e+00)\tAcc@1  64.06 ( 59.14)\tAcc@5  89.84 ( 87.56)\n","Epoch: [10][120/391]\tTime  0.174 ( 0.176)\tLoss 1.4369e+00 (1.4488e+00)\tAcc@1  54.69 ( 58.80)\tAcc@5  92.19 ( 87.31)\n","Epoch: [10][150/391]\tTime  0.174 ( 0.176)\tLoss 1.9217e+00 (1.4503e+00)\tAcc@1  48.44 ( 58.79)\tAcc@5  79.69 ( 87.31)\n","Epoch: [10][180/391]\tTime  0.175 ( 0.176)\tLoss 1.2248e+00 (1.4422e+00)\tAcc@1  67.19 ( 59.07)\tAcc@5  87.50 ( 87.44)\n","Epoch: [10][210/391]\tTime  0.174 ( 0.176)\tLoss 1.3519e+00 (1.4475e+00)\tAcc@1  64.06 ( 59.00)\tAcc@5  89.06 ( 87.34)\n","Epoch: [10][240/391]\tTime  0.174 ( 0.176)\tLoss 1.6373e+00 (1.4505e+00)\tAcc@1  55.47 ( 58.92)\tAcc@5  85.16 ( 87.29)\n","Epoch: [10][270/391]\tTime  0.175 ( 0.176)\tLoss 1.3171e+00 (1.4562e+00)\tAcc@1  61.72 ( 58.77)\tAcc@5  89.06 ( 87.21)\n","Epoch: [10][300/391]\tTime  0.174 ( 0.176)\tLoss 1.3150e+00 (1.4614e+00)\tAcc@1  60.16 ( 58.67)\tAcc@5  89.84 ( 87.14)\n","Epoch: [10][330/391]\tTime  0.174 ( 0.176)\tLoss 1.4764e+00 (1.4647e+00)\tAcc@1  61.72 ( 58.60)\tAcc@5  89.84 ( 87.04)\n","Epoch: [10][360/391]\tTime  0.174 ( 0.176)\tLoss 1.3207e+00 (1.4652e+00)\tAcc@1  63.28 ( 58.52)\tAcc@5  85.16 ( 87.02)\n","Epoch: [10][390/391]\tTime  0.160 ( 0.176)\tLoss 1.6908e+00 (1.4671e+00)\tAcc@1  48.75 ( 58.42)\tAcc@5  82.50 ( 86.99)\n","==> Train Accuracy: Acc@1 58.422 || Acc@5 86.986\n","==> Test Accuracy:  Acc@1 52.420 || Acc@5 81.580\n","==> 73.19 seconds to train this epoch\n","\n","\n","----- epoch: 11, lr: 0.1 -----\n","Epoch: [11][  0/391]\tTime  0.233 ( 0.233)\tLoss 1.5778e+00 (1.5778e+00)\tAcc@1  53.91 ( 53.91)\tAcc@5  85.16 ( 85.16)\n","Epoch: [11][ 30/391]\tTime  0.175 ( 0.177)\tLoss 1.4195e+00 (1.3474e+00)\tAcc@1  57.03 ( 61.29)\tAcc@5  91.41 ( 88.81)\n","Epoch: [11][ 60/391]\tTime  0.174 ( 0.177)\tLoss 1.2047e+00 (1.3245e+00)\tAcc@1  62.50 ( 61.78)\tAcc@5  89.84 ( 89.23)\n","Epoch: [11][ 90/391]\tTime  0.174 ( 0.176)\tLoss 1.3399e+00 (1.3547e+00)\tAcc@1  62.50 ( 61.19)\tAcc@5  91.41 ( 88.74)\n","Epoch: [11][120/391]\tTime  0.176 ( 0.176)\tLoss 1.6944e+00 (1.3834e+00)\tAcc@1  50.00 ( 60.40)\tAcc@5  87.50 ( 88.37)\n","Epoch: [11][150/391]\tTime  0.176 ( 0.176)\tLoss 1.2985e+00 (1.3810e+00)\tAcc@1  62.50 ( 60.37)\tAcc@5  88.28 ( 88.45)\n","Epoch: [11][180/391]\tTime  0.177 ( 0.176)\tLoss 1.5917e+00 (1.3885e+00)\tAcc@1  53.12 ( 60.16)\tAcc@5  88.28 ( 88.32)\n","Epoch: [11][210/391]\tTime  0.177 ( 0.176)\tLoss 1.2426e+00 (1.3903e+00)\tAcc@1  63.28 ( 60.16)\tAcc@5  93.75 ( 88.30)\n","Epoch: [11][240/391]\tTime  0.176 ( 0.176)\tLoss 1.5497e+00 (1.3951e+00)\tAcc@1  56.25 ( 60.12)\tAcc@5  84.38 ( 88.21)\n","Epoch: [11][270/391]\tTime  0.176 ( 0.176)\tLoss 1.5480e+00 (1.3984e+00)\tAcc@1  57.03 ( 60.09)\tAcc@5  85.94 ( 88.14)\n","Epoch: [11][300/391]\tTime  0.176 ( 0.176)\tLoss 1.4293e+00 (1.4009e+00)\tAcc@1  59.38 ( 60.02)\tAcc@5  89.84 ( 88.12)\n","Epoch: [11][330/391]\tTime  0.176 ( 0.176)\tLoss 1.5661e+00 (1.4043e+00)\tAcc@1  54.69 ( 59.88)\tAcc@5  84.38 ( 88.07)\n","Epoch: [11][360/391]\tTime  0.176 ( 0.176)\tLoss 1.6572e+00 (1.4084e+00)\tAcc@1  55.47 ( 59.77)\tAcc@5  85.16 ( 88.01)\n","Epoch: [11][390/391]\tTime  0.164 ( 0.176)\tLoss 1.6839e+00 (1.4091e+00)\tAcc@1  52.50 ( 59.78)\tAcc@5  83.75 ( 87.96)\n","==> Train Accuracy: Acc@1 59.780 || Acc@5 87.962\n","==> Test Accuracy:  Acc@1 52.690 || Acc@5 82.320\n","==> 73.14 seconds to train this epoch\n","\n","\n","----- epoch: 12, lr: 0.1 -----\n","Epoch: [12][  0/391]\tTime  0.219 ( 0.219)\tLoss 1.0932e+00 (1.0932e+00)\tAcc@1  66.41 ( 66.41)\tAcc@5  96.88 ( 96.88)\n","Epoch: [12][ 30/391]\tTime  0.176 ( 0.177)\tLoss 1.3759e+00 (1.3209e+00)\tAcc@1  59.38 ( 62.22)\tAcc@5  85.94 ( 88.96)\n","Epoch: [12][ 60/391]\tTime  0.175 ( 0.176)\tLoss 1.4070e+00 (1.3039e+00)\tAcc@1  57.81 ( 62.67)\tAcc@5  91.41 ( 89.33)\n","Epoch: [12][ 90/391]\tTime  0.177 ( 0.176)\tLoss 1.4269e+00 (1.3132e+00)\tAcc@1  64.06 ( 62.41)\tAcc@5  85.94 ( 89.13)\n","Epoch: [12][120/391]\tTime  0.178 ( 0.176)\tLoss 1.3558e+00 (1.3297e+00)\tAcc@1  57.81 ( 62.02)\tAcc@5  89.84 ( 89.04)\n","Epoch: [12][150/391]\tTime  0.176 ( 0.176)\tLoss 1.6079e+00 (1.3312e+00)\tAcc@1  50.78 ( 61.86)\tAcc@5  86.72 ( 88.90)\n","Epoch: [12][180/391]\tTime  0.176 ( 0.176)\tLoss 1.1196e+00 (1.3340e+00)\tAcc@1  66.41 ( 61.72)\tAcc@5  93.75 ( 88.88)\n","Epoch: [12][210/391]\tTime  0.179 ( 0.176)\tLoss 1.2533e+00 (1.3389e+00)\tAcc@1  60.16 ( 61.46)\tAcc@5  92.19 ( 88.87)\n","Epoch: [12][240/391]\tTime  0.177 ( 0.176)\tLoss 1.4119e+00 (1.3445e+00)\tAcc@1  64.06 ( 61.40)\tAcc@5  83.59 ( 88.72)\n","Epoch: [12][270/391]\tTime  0.175 ( 0.176)\tLoss 1.3148e+00 (1.3562e+00)\tAcc@1  67.19 ( 61.08)\tAcc@5  85.94 ( 88.54)\n","Epoch: [12][300/391]\tTime  0.176 ( 0.176)\tLoss 1.4228e+00 (1.3577e+00)\tAcc@1  60.16 ( 61.03)\tAcc@5  86.72 ( 88.56)\n","Epoch: [12][330/391]\tTime  0.176 ( 0.176)\tLoss 1.2421e+00 (1.3609e+00)\tAcc@1  64.06 ( 60.89)\tAcc@5  91.41 ( 88.55)\n","Epoch: [12][360/391]\tTime  0.177 ( 0.176)\tLoss 1.5452e+00 (1.3667e+00)\tAcc@1  58.59 ( 60.74)\tAcc@5  84.38 ( 88.48)\n","Epoch: [12][390/391]\tTime  0.160 ( 0.176)\tLoss 1.5354e+00 (1.3663e+00)\tAcc@1  57.50 ( 60.79)\tAcc@5  82.50 ( 88.47)\n","==> Train Accuracy: Acc@1 60.786 || Acc@5 88.472\n","==> Test Accuracy:  Acc@1 55.180 || Acc@5 84.650\n","==> 73.15 seconds to train this epoch\n","\n","\n","----- epoch: 13, lr: 0.1 -----\n","Epoch: [13][  0/391]\tTime  0.236 ( 0.236)\tLoss 1.3114e+00 (1.3114e+00)\tAcc@1  60.94 ( 60.94)\tAcc@5  88.28 ( 88.28)\n","Epoch: [13][ 30/391]\tTime  0.177 ( 0.178)\tLoss 1.3585e+00 (1.2599e+00)\tAcc@1  64.06 ( 64.01)\tAcc@5  89.06 ( 89.84)\n","Epoch: [13][ 60/391]\tTime  0.175 ( 0.177)\tLoss 1.1831e+00 (1.2585e+00)\tAcc@1  66.41 ( 63.56)\tAcc@5  89.84 ( 90.20)\n","Epoch: [13][ 90/391]\tTime  0.178 ( 0.176)\tLoss 1.3638e+00 (1.2711e+00)\tAcc@1  60.94 ( 63.15)\tAcc@5  84.38 ( 90.03)\n","Epoch: [13][120/391]\tTime  0.175 ( 0.176)\tLoss 1.4486e+00 (1.2872e+00)\tAcc@1  57.03 ( 62.66)\tAcc@5  87.50 ( 89.78)\n","Epoch: [13][150/391]\tTime  0.175 ( 0.176)\tLoss 1.0805e+00 (1.2871e+00)\tAcc@1  67.97 ( 62.67)\tAcc@5  92.97 ( 89.79)\n","Epoch: [13][180/391]\tTime  0.177 ( 0.176)\tLoss 1.4423e+00 (1.2919e+00)\tAcc@1  62.50 ( 62.56)\tAcc@5  89.06 ( 89.71)\n","Epoch: [13][210/391]\tTime  0.176 ( 0.176)\tLoss 1.2485e+00 (1.2987e+00)\tAcc@1  60.16 ( 62.36)\tAcc@5  92.97 ( 89.61)\n","Epoch: [13][240/391]\tTime  0.177 ( 0.176)\tLoss 1.1033e+00 (1.3067e+00)\tAcc@1  67.97 ( 62.09)\tAcc@5  92.19 ( 89.50)\n","Epoch: [13][270/391]\tTime  0.176 ( 0.176)\tLoss 1.7490e+00 (1.3101e+00)\tAcc@1  59.38 ( 62.03)\tAcc@5  82.81 ( 89.46)\n","Epoch: [13][300/391]\tTime  0.173 ( 0.176)\tLoss 1.4304e+00 (1.3102e+00)\tAcc@1  60.16 ( 62.16)\tAcc@5  85.16 ( 89.43)\n","Epoch: [13][330/391]\tTime  0.176 ( 0.176)\tLoss 1.4182e+00 (1.3157e+00)\tAcc@1  60.94 ( 62.04)\tAcc@5  86.72 ( 89.30)\n","Epoch: [13][360/391]\tTime  0.177 ( 0.176)\tLoss 1.3960e+00 (1.3174e+00)\tAcc@1  60.94 ( 62.07)\tAcc@5  87.50 ( 89.25)\n","Epoch: [13][390/391]\tTime  0.160 ( 0.176)\tLoss 1.3029e+00 (1.3213e+00)\tAcc@1  62.50 ( 62.06)\tAcc@5  86.25 ( 89.23)\n","==> Train Accuracy: Acc@1 62.064 || Acc@5 89.228\n","==> Test Accuracy:  Acc@1 52.980 || Acc@5 82.220\n","==> 73.14 seconds to train this epoch\n","\n","\n","----- epoch: 14, lr: 0.1 -----\n","Epoch: [14][  0/391]\tTime  0.237 ( 0.237)\tLoss 1.3138e+00 (1.3138e+00)\tAcc@1  60.94 ( 60.94)\tAcc@5  87.50 ( 87.50)\n","Epoch: [14][ 30/391]\tTime  0.176 ( 0.177)\tLoss 1.2089e+00 (1.2331e+00)\tAcc@1  65.62 ( 64.34)\tAcc@5  89.84 ( 90.50)\n","Epoch: [14][ 60/391]\tTime  0.176 ( 0.177)\tLoss 1.2485e+00 (1.2504e+00)\tAcc@1  64.84 ( 63.97)\tAcc@5  92.19 ( 90.06)\n","Epoch: [14][ 90/391]\tTime  0.176 ( 0.176)\tLoss 1.4382e+00 (1.2386e+00)\tAcc@1  61.72 ( 64.47)\tAcc@5  86.72 ( 90.39)\n","Epoch: [14][120/391]\tTime  0.175 ( 0.176)\tLoss 1.1829e+00 (1.2577e+00)\tAcc@1  64.06 ( 63.88)\tAcc@5  90.62 ( 90.06)\n","Epoch: [14][150/391]\tTime  0.168 ( 0.176)\tLoss 1.1834e+00 (1.2641e+00)\tAcc@1  65.62 ( 63.66)\tAcc@5  92.19 ( 90.03)\n","Epoch: [14][180/391]\tTime  0.170 ( 0.176)\tLoss 1.1188e+00 (1.2708e+00)\tAcc@1  63.28 ( 63.45)\tAcc@5  92.19 ( 89.93)\n","Epoch: [14][210/391]\tTime  0.175 ( 0.176)\tLoss 1.4916e+00 (1.2731e+00)\tAcc@1  58.59 ( 63.51)\tAcc@5  87.50 ( 89.83)\n","Epoch: [14][240/391]\tTime  0.174 ( 0.176)\tLoss 1.5359e+00 (1.2766e+00)\tAcc@1  55.47 ( 63.33)\tAcc@5  84.38 ( 89.82)\n","Epoch: [14][270/391]\tTime  0.175 ( 0.176)\tLoss 1.3052e+00 (1.2798e+00)\tAcc@1  69.53 ( 63.26)\tAcc@5  88.28 ( 89.80)\n","Epoch: [14][300/391]\tTime  0.174 ( 0.176)\tLoss 1.4784e+00 (1.2832e+00)\tAcc@1  53.12 ( 63.15)\tAcc@5  89.84 ( 89.73)\n","Epoch: [14][330/391]\tTime  0.173 ( 0.176)\tLoss 1.4803e+00 (1.2897e+00)\tAcc@1  60.16 ( 63.02)\tAcc@5  82.03 ( 89.64)\n","Epoch: [14][360/391]\tTime  0.175 ( 0.176)\tLoss 1.4296e+00 (1.2894e+00)\tAcc@1  53.12 ( 63.00)\tAcc@5  87.50 ( 89.62)\n","Epoch: [14][390/391]\tTime  0.157 ( 0.176)\tLoss 1.3272e+00 (1.2918e+00)\tAcc@1  63.75 ( 62.90)\tAcc@5  86.25 ( 89.57)\n","==> Train Accuracy: Acc@1 62.904 || Acc@5 89.568\n","==> Test Accuracy:  Acc@1 54.240 || Acc@5 83.750\n","==> 73.15 seconds to train this epoch\n","\n","\n","----- epoch: 15, lr: 0.1 -----\n","Epoch: [15][  0/391]\tTime  0.226 ( 0.226)\tLoss 1.1886e+00 (1.1886e+00)\tAcc@1  67.97 ( 67.97)\tAcc@5  89.06 ( 89.06)\n","Epoch: [15][ 30/391]\tTime  0.176 ( 0.177)\tLoss 1.3347e+00 (1.1927e+00)\tAcc@1  60.16 ( 65.45)\tAcc@5  91.41 ( 91.23)\n","Epoch: [15][ 60/391]\tTime  0.174 ( 0.176)\tLoss 1.0700e+00 (1.1762e+00)\tAcc@1  69.53 ( 65.61)\tAcc@5  91.41 ( 91.33)\n","Epoch: [15][ 90/391]\tTime  0.174 ( 0.176)\tLoss 1.3779e+00 (1.1985e+00)\tAcc@1  61.72 ( 64.79)\tAcc@5  94.53 ( 91.17)\n","Epoch: [15][120/391]\tTime  0.176 ( 0.176)\tLoss 1.2678e+00 (1.2271e+00)\tAcc@1  60.94 ( 64.09)\tAcc@5  93.75 ( 90.84)\n","Epoch: [15][150/391]\tTime  0.175 ( 0.176)\tLoss 1.4521e+00 (1.2346e+00)\tAcc@1  54.69 ( 63.96)\tAcc@5  85.16 ( 90.71)\n","Epoch: [15][180/391]\tTime  0.175 ( 0.176)\tLoss 1.2438e+00 (1.2356e+00)\tAcc@1  60.16 ( 63.98)\tAcc@5  92.19 ( 90.61)\n","Epoch: [15][210/391]\tTime  0.174 ( 0.176)\tLoss 1.3495e+00 (1.2396e+00)\tAcc@1  60.16 ( 63.98)\tAcc@5  85.94 ( 90.48)\n","Epoch: [15][240/391]\tTime  0.175 ( 0.176)\tLoss 1.2196e+00 (1.2442e+00)\tAcc@1  60.94 ( 63.95)\tAcc@5  89.84 ( 90.41)\n","Epoch: [15][270/391]\tTime  0.176 ( 0.176)\tLoss 1.3122e+00 (1.2445e+00)\tAcc@1  62.50 ( 63.94)\tAcc@5  89.06 ( 90.38)\n","Epoch: [15][300/391]\tTime  0.177 ( 0.176)\tLoss 1.2297e+00 (1.2488e+00)\tAcc@1  63.28 ( 63.80)\tAcc@5  89.84 ( 90.24)\n","Epoch: [15][330/391]\tTime  0.177 ( 0.176)\tLoss 1.4567e+00 (1.2542e+00)\tAcc@1  60.16 ( 63.69)\tAcc@5  88.28 ( 90.16)\n","Epoch: [15][360/391]\tTime  0.177 ( 0.176)\tLoss 1.3080e+00 (1.2554e+00)\tAcc@1  60.94 ( 63.62)\tAcc@5  89.84 ( 90.20)\n","Epoch: [15][390/391]\tTime  0.157 ( 0.176)\tLoss 1.2277e+00 (1.2567e+00)\tAcc@1  63.75 ( 63.59)\tAcc@5  92.50 ( 90.19)\n","==> Train Accuracy: Acc@1 63.594 || Acc@5 90.186\n","==> Test Accuracy:  Acc@1 52.680 || Acc@5 81.950\n","==> 73.10 seconds to train this epoch\n","\n","\n","----- epoch: 16, lr: 0.1 -----\n","Epoch: [16][  0/391]\tTime  0.243 ( 0.243)\tLoss 1.3376e+00 (1.3376e+00)\tAcc@1  58.59 ( 58.59)\tAcc@5  89.84 ( 89.84)\n","Epoch: [16][ 30/391]\tTime  0.174 ( 0.177)\tLoss 1.2518e+00 (1.1796e+00)\tAcc@1  64.84 ( 65.10)\tAcc@5  91.41 ( 91.66)\n","Epoch: [16][ 60/391]\tTime  0.176 ( 0.177)\tLoss 1.1727e+00 (1.1742e+00)\tAcc@1  65.62 ( 65.30)\tAcc@5  87.50 ( 91.65)\n","Epoch: [16][ 90/391]\tTime  0.174 ( 0.176)\tLoss 1.2971e+00 (1.1868e+00)\tAcc@1  67.19 ( 65.14)\tAcc@5  89.06 ( 91.35)\n","Epoch: [16][120/391]\tTime  0.176 ( 0.176)\tLoss 1.1208e+00 (1.1925e+00)\tAcc@1  64.06 ( 65.05)\tAcc@5  92.97 ( 91.23)\n","Epoch: [16][150/391]\tTime  0.177 ( 0.176)\tLoss 1.2851e+00 (1.1983e+00)\tAcc@1  64.06 ( 65.10)\tAcc@5  89.84 ( 91.19)\n","Epoch: [16][180/391]\tTime  0.175 ( 0.176)\tLoss 1.1245e+00 (1.1977e+00)\tAcc@1  65.62 ( 65.06)\tAcc@5  92.19 ( 91.17)\n","Epoch: [16][210/391]\tTime  0.176 ( 0.176)\tLoss 1.1778e+00 (1.2036e+00)\tAcc@1  69.53 ( 65.06)\tAcc@5  93.75 ( 91.07)\n","Epoch: [16][240/391]\tTime  0.177 ( 0.176)\tLoss 1.2524e+00 (1.2074e+00)\tAcc@1  66.41 ( 65.03)\tAcc@5  85.94 ( 90.93)\n","Epoch: [16][270/391]\tTime  0.175 ( 0.176)\tLoss 1.2703e+00 (1.2106e+00)\tAcc@1  57.81 ( 64.91)\tAcc@5  90.62 ( 90.88)\n","Epoch: [16][300/391]\tTime  0.176 ( 0.176)\tLoss 1.3710e+00 (1.2224e+00)\tAcc@1  64.06 ( 64.66)\tAcc@5  86.72 ( 90.71)\n","Epoch: [16][330/391]\tTime  0.176 ( 0.176)\tLoss 1.2864e+00 (1.2272e+00)\tAcc@1  59.38 ( 64.44)\tAcc@5  88.28 ( 90.67)\n","Epoch: [16][360/391]\tTime  0.179 ( 0.176)\tLoss 1.2124e+00 (1.2296e+00)\tAcc@1  62.50 ( 64.32)\tAcc@5  90.62 ( 90.69)\n","Epoch: [16][390/391]\tTime  0.159 ( 0.176)\tLoss 1.1489e+00 (1.2341e+00)\tAcc@1  70.00 ( 64.21)\tAcc@5  91.25 ( 90.60)\n","==> Train Accuracy: Acc@1 64.208 || Acc@5 90.598\n","==> Test Accuracy:  Acc@1 54.540 || Acc@5 83.390\n","==> 73.12 seconds to train this epoch\n","\n","\n","----- epoch: 17, lr: 0.1 -----\n","Epoch: [17][  0/391]\tTime  0.228 ( 0.228)\tLoss 9.0514e-01 (9.0514e-01)\tAcc@1  71.88 ( 71.88)\tAcc@5  92.19 ( 92.19)\n","Epoch: [17][ 30/391]\tTime  0.178 ( 0.177)\tLoss 1.2603e+00 (1.0990e+00)\tAcc@1  60.16 ( 67.57)\tAcc@5  89.84 ( 92.57)\n","Epoch: [17][ 60/391]\tTime  0.176 ( 0.176)\tLoss 1.4638e+00 (1.1448e+00)\tAcc@1  63.28 ( 66.62)\tAcc@5  88.28 ( 91.93)\n","Epoch: [17][ 90/391]\tTime  0.175 ( 0.176)\tLoss 1.1462e+00 (1.1627e+00)\tAcc@1  64.84 ( 65.89)\tAcc@5  92.19 ( 91.41)\n","Epoch: [17][120/391]\tTime  0.177 ( 0.176)\tLoss 1.1633e+00 (1.1757e+00)\tAcc@1  63.28 ( 65.68)\tAcc@5  90.62 ( 91.25)\n","Epoch: [17][150/391]\tTime  0.176 ( 0.176)\tLoss 1.1901e+00 (1.1804e+00)\tAcc@1  60.16 ( 65.60)\tAcc@5  92.97 ( 91.24)\n","Epoch: [17][180/391]\tTime  0.178 ( 0.176)\tLoss 1.4571e+00 (1.1966e+00)\tAcc@1  60.16 ( 65.18)\tAcc@5  89.06 ( 91.15)\n","Epoch: [17][210/391]\tTime  0.175 ( 0.176)\tLoss 1.1346e+00 (1.1950e+00)\tAcc@1  60.94 ( 65.19)\tAcc@5  94.53 ( 91.15)\n","Epoch: [17][240/391]\tTime  0.177 ( 0.176)\tLoss 1.1909e+00 (1.1953e+00)\tAcc@1  65.62 ( 65.22)\tAcc@5  92.97 ( 91.09)\n","Epoch: [17][270/391]\tTime  0.176 ( 0.176)\tLoss 1.1753e+00 (1.1969e+00)\tAcc@1  67.97 ( 65.21)\tAcc@5  88.28 ( 91.02)\n","Epoch: [17][300/391]\tTime  0.175 ( 0.176)\tLoss 1.4555e+00 (1.1976e+00)\tAcc@1  61.72 ( 65.27)\tAcc@5  89.06 ( 90.98)\n","Epoch: [17][330/391]\tTime  0.176 ( 0.176)\tLoss 1.3888e+00 (1.2025e+00)\tAcc@1  59.38 ( 65.19)\tAcc@5  89.84 ( 90.93)\n","Epoch: [17][360/391]\tTime  0.175 ( 0.176)\tLoss 1.2425e+00 (1.2041e+00)\tAcc@1  64.84 ( 65.16)\tAcc@5  89.84 ( 90.85)\n","Epoch: [17][390/391]\tTime  0.159 ( 0.176)\tLoss 1.1234e+00 (1.2092e+00)\tAcc@1  67.50 ( 65.05)\tAcc@5  95.00 ( 90.82)\n","==> Train Accuracy: Acc@1 65.052 || Acc@5 90.824\n","==> Test Accuracy:  Acc@1 56.400 || Acc@5 85.990\n","==> 73.10 seconds to train this epoch\n","\n","\n","----- epoch: 18, lr: 0.1 -----\n","Epoch: [18][  0/391]\tTime  0.229 ( 0.229)\tLoss 1.0619e+00 (1.0619e+00)\tAcc@1  71.09 ( 71.09)\tAcc@5  91.41 ( 91.41)\n","Epoch: [18][ 30/391]\tTime  0.183 ( 0.177)\tLoss 1.1009e+00 (1.1313e+00)\tAcc@1  68.75 ( 67.57)\tAcc@5  90.62 ( 92.19)\n","Epoch: [18][ 60/391]\tTime  0.174 ( 0.176)\tLoss 1.3354e+00 (1.1108e+00)\tAcc@1  60.16 ( 67.73)\tAcc@5  92.19 ( 92.32)\n","Epoch: [18][ 90/391]\tTime  0.176 ( 0.176)\tLoss 1.1038e+00 (1.1140e+00)\tAcc@1  67.19 ( 67.32)\tAcc@5  94.53 ( 92.43)\n","Epoch: [18][120/391]\tTime  0.175 ( 0.176)\tLoss 9.5977e-01 (1.1282e+00)\tAcc@1  74.22 ( 66.97)\tAcc@5  92.19 ( 92.16)\n","Epoch: [18][150/391]\tTime  0.176 ( 0.176)\tLoss 1.0179e+00 (1.1436e+00)\tAcc@1  70.31 ( 66.80)\tAcc@5  94.53 ( 91.95)\n","Epoch: [18][180/391]\tTime  0.176 ( 0.176)\tLoss 1.1232e+00 (1.1510e+00)\tAcc@1  67.19 ( 66.71)\tAcc@5  93.75 ( 91.70)\n","Epoch: [18][210/391]\tTime  0.175 ( 0.176)\tLoss 1.3960e+00 (1.1587e+00)\tAcc@1  57.03 ( 66.44)\tAcc@5  89.06 ( 91.56)\n","Epoch: [18][240/391]\tTime  0.176 ( 0.176)\tLoss 1.2624e+00 (1.1680e+00)\tAcc@1  65.62 ( 66.13)\tAcc@5  89.06 ( 91.55)\n","Epoch: [18][270/391]\tTime  0.176 ( 0.176)\tLoss 9.3217e-01 (1.1708e+00)\tAcc@1  72.66 ( 65.96)\tAcc@5  96.88 ( 91.44)\n","Epoch: [18][300/391]\tTime  0.176 ( 0.176)\tLoss 1.4431e+00 (1.1745e+00)\tAcc@1  63.28 ( 65.94)\tAcc@5  87.50 ( 91.35)\n","Epoch: [18][330/391]\tTime  0.176 ( 0.176)\tLoss 1.3601e+00 (1.1783e+00)\tAcc@1  63.28 ( 65.79)\tAcc@5  90.62 ( 91.31)\n","Epoch: [18][360/391]\tTime  0.177 ( 0.176)\tLoss 1.0787e+00 (1.1848e+00)\tAcc@1  71.88 ( 65.67)\tAcc@5  94.53 ( 91.16)\n","Epoch: [18][390/391]\tTime  0.159 ( 0.176)\tLoss 1.5019e+00 (1.1876e+00)\tAcc@1  60.00 ( 65.64)\tAcc@5  86.25 ( 91.13)\n","==> Train Accuracy: Acc@1 65.640 || Acc@5 91.126\n","==> Test Accuracy:  Acc@1 56.250 || Acc@5 83.860\n","==> 73.02 seconds to train this epoch\n","\n","\n","----- epoch: 19, lr: 0.1 -----\n","Epoch: [19][  0/391]\tTime  0.224 ( 0.224)\tLoss 1.0765e+00 (1.0765e+00)\tAcc@1  73.44 ( 73.44)\tAcc@5  92.19 ( 92.19)\n","Epoch: [19][ 30/391]\tTime  0.174 ( 0.177)\tLoss 1.0612e+00 (1.0788e+00)\tAcc@1  70.31 ( 68.20)\tAcc@5  90.62 ( 92.16)\n","Epoch: [19][ 60/391]\tTime  0.175 ( 0.176)\tLoss 1.0751e+00 (1.0926e+00)\tAcc@1  69.53 ( 67.52)\tAcc@5  91.41 ( 92.29)\n","Epoch: [19][ 90/391]\tTime  0.175 ( 0.176)\tLoss 1.0403e+00 (1.1026e+00)\tAcc@1  64.84 ( 67.37)\tAcc@5  92.19 ( 92.06)\n","Epoch: [19][120/391]\tTime  0.176 ( 0.176)\tLoss 1.2547e+00 (1.1086e+00)\tAcc@1  66.41 ( 67.25)\tAcc@5  92.19 ( 92.02)\n","Epoch: [19][150/391]\tTime  0.177 ( 0.176)\tLoss 1.0074e+00 (1.1143e+00)\tAcc@1  73.44 ( 67.20)\tAcc@5  93.75 ( 92.05)\n","Epoch: [19][180/391]\tTime  0.178 ( 0.176)\tLoss 1.3768e+00 (1.1302e+00)\tAcc@1  60.94 ( 66.84)\tAcc@5  88.28 ( 91.87)\n","Epoch: [19][210/391]\tTime  0.177 ( 0.176)\tLoss 1.0473e+00 (1.1380e+00)\tAcc@1  67.97 ( 66.57)\tAcc@5  91.41 ( 91.71)\n","Epoch: [19][240/391]\tTime  0.175 ( 0.176)\tLoss 1.3240e+00 (1.1453e+00)\tAcc@1  62.50 ( 66.43)\tAcc@5  90.62 ( 91.61)\n","Epoch: [19][270/391]\tTime  0.176 ( 0.176)\tLoss 1.2190e+00 (1.1504e+00)\tAcc@1  64.06 ( 66.25)\tAcc@5  89.06 ( 91.54)\n","Epoch: [19][300/391]\tTime  0.175 ( 0.176)\tLoss 1.3338e+00 (1.1552e+00)\tAcc@1  62.50 ( 66.18)\tAcc@5  86.72 ( 91.43)\n","Epoch: [19][330/391]\tTime  0.177 ( 0.176)\tLoss 1.3332e+00 (1.1614e+00)\tAcc@1  63.28 ( 65.96)\tAcc@5  91.41 ( 91.37)\n","Epoch: [19][360/391]\tTime  0.175 ( 0.176)\tLoss 1.1996e+00 (1.1650e+00)\tAcc@1  65.62 ( 65.91)\tAcc@5  93.75 ( 91.34)\n","Epoch: [19][390/391]\tTime  0.159 ( 0.176)\tLoss 1.3358e+00 (1.1682e+00)\tAcc@1  61.25 ( 65.94)\tAcc@5  87.50 ( 91.28)\n","==> Train Accuracy: Acc@1 65.942 || Acc@5 91.278\n","==> Test Accuracy:  Acc@1 49.750 || Acc@5 79.450\n","==> 73.07 seconds to train this epoch\n","\n","\n","----- epoch: 20, lr: 0.1 -----\n","Epoch: [20][  0/391]\tTime  0.225 ( 0.225)\tLoss 1.1688e+00 (1.1688e+00)\tAcc@1  60.16 ( 60.16)\tAcc@5  93.75 ( 93.75)\n","Epoch: [20][ 30/391]\tTime  0.176 ( 0.176)\tLoss 1.2409e+00 (1.0903e+00)\tAcc@1  63.28 ( 67.69)\tAcc@5  91.41 ( 92.41)\n","Epoch: [20][ 60/391]\tTime  0.176 ( 0.176)\tLoss 1.0737e+00 (1.0889e+00)\tAcc@1  67.19 ( 67.70)\tAcc@5  89.84 ( 92.23)\n","Epoch: [20][ 90/391]\tTime  0.174 ( 0.176)\tLoss 1.3210e+00 (1.0906e+00)\tAcc@1  61.72 ( 67.72)\tAcc@5  86.72 ( 92.01)\n","Epoch: [20][120/391]\tTime  0.176 ( 0.176)\tLoss 1.1012e+00 (1.1023e+00)\tAcc@1  71.09 ( 67.52)\tAcc@5  92.19 ( 91.92)\n","Epoch: [20][150/391]\tTime  0.175 ( 0.176)\tLoss 9.8240e-01 (1.1016e+00)\tAcc@1  69.53 ( 67.64)\tAcc@5  95.31 ( 91.98)\n","Epoch: [20][180/391]\tTime  0.177 ( 0.176)\tLoss 1.0250e+00 (1.1049e+00)\tAcc@1  67.19 ( 67.39)\tAcc@5  92.97 ( 91.94)\n","Epoch: [20][210/391]\tTime  0.176 ( 0.176)\tLoss 1.2688e+00 (1.1130e+00)\tAcc@1  62.50 ( 67.10)\tAcc@5  89.06 ( 91.89)\n","Epoch: [20][240/391]\tTime  0.175 ( 0.176)\tLoss 1.0598e+00 (1.1244e+00)\tAcc@1  66.41 ( 66.80)\tAcc@5  96.09 ( 91.70)\n","Epoch: [20][270/391]\tTime  0.176 ( 0.176)\tLoss 1.2056e+00 (1.1273e+00)\tAcc@1  61.72 ( 66.74)\tAcc@5  92.19 ( 91.70)\n","Epoch: [20][300/391]\tTime  0.173 ( 0.176)\tLoss 1.1139e+00 (1.1311e+00)\tAcc@1  66.41 ( 66.64)\tAcc@5  93.75 ( 91.70)\n","Epoch: [20][330/391]\tTime  0.175 ( 0.176)\tLoss 9.1772e-01 (1.1395e+00)\tAcc@1  74.22 ( 66.49)\tAcc@5  92.19 ( 91.59)\n","Epoch: [20][360/391]\tTime  0.175 ( 0.176)\tLoss 1.2519e+00 (1.1459e+00)\tAcc@1  64.84 ( 66.34)\tAcc@5  90.62 ( 91.51)\n","Epoch: [20][390/391]\tTime  0.161 ( 0.176)\tLoss 1.2776e+00 (1.1480e+00)\tAcc@1  66.25 ( 66.25)\tAcc@5  88.75 ( 91.48)\n","==> Train Accuracy: Acc@1 66.246 || Acc@5 91.478\n","==> Test Accuracy:  Acc@1 59.920 || Acc@5 86.370\n","==> 72.97 seconds to train this epoch\n","\n","\n","----- epoch: 21, lr: 0.1 -----\n","Epoch: [21][  0/391]\tTime  0.213 ( 0.213)\tLoss 1.2712e+00 (1.2712e+00)\tAcc@1  62.50 ( 62.50)\tAcc@5  88.28 ( 88.28)\n","Epoch: [21][ 30/391]\tTime  0.175 ( 0.176)\tLoss 1.0018e+00 (1.0661e+00)\tAcc@1  76.56 ( 69.15)\tAcc@5  92.19 ( 93.30)\n","Epoch: [21][ 60/391]\tTime  0.172 ( 0.176)\tLoss 1.3115e+00 (1.0499e+00)\tAcc@1  63.28 ( 69.48)\tAcc@5  86.72 ( 93.26)\n","Epoch: [21][ 90/391]\tTime  0.176 ( 0.176)\tLoss 1.2691e+00 (1.0643e+00)\tAcc@1  60.94 ( 68.65)\tAcc@5  89.84 ( 92.84)\n","Epoch: [21][120/391]\tTime  0.176 ( 0.176)\tLoss 1.2019e+00 (1.0742e+00)\tAcc@1  65.62 ( 68.40)\tAcc@5  92.19 ( 92.78)\n","Epoch: [21][150/391]\tTime  0.174 ( 0.176)\tLoss 9.5803e-01 (1.0833e+00)\tAcc@1  68.75 ( 67.97)\tAcc@5  95.31 ( 92.64)\n","Epoch: [21][180/391]\tTime  0.176 ( 0.176)\tLoss 9.9791e-01 (1.0991e+00)\tAcc@1  69.53 ( 67.51)\tAcc@5  92.97 ( 92.50)\n","Epoch: [21][210/391]\tTime  0.177 ( 0.176)\tLoss 1.1429e+00 (1.1141e+00)\tAcc@1  67.97 ( 67.26)\tAcc@5  93.75 ( 92.24)\n","Epoch: [21][240/391]\tTime  0.175 ( 0.176)\tLoss 1.2393e+00 (1.1180e+00)\tAcc@1  64.06 ( 67.13)\tAcc@5  90.62 ( 92.17)\n","Epoch: [21][270/391]\tTime  0.174 ( 0.176)\tLoss 1.0527e+00 (1.1181e+00)\tAcc@1  74.22 ( 67.07)\tAcc@5  92.97 ( 92.11)\n","Epoch: [21][300/391]\tTime  0.175 ( 0.176)\tLoss 1.1621e+00 (1.1206e+00)\tAcc@1  62.50 ( 67.04)\tAcc@5  91.41 ( 92.10)\n","Epoch: [21][330/391]\tTime  0.175 ( 0.176)\tLoss 1.1780e+00 (1.1275e+00)\tAcc@1  67.97 ( 66.94)\tAcc@5  92.97 ( 92.02)\n","Epoch: [21][360/391]\tTime  0.177 ( 0.176)\tLoss 1.3365e+00 (1.1340e+00)\tAcc@1  61.72 ( 66.76)\tAcc@5  89.06 ( 91.96)\n","Epoch: [21][390/391]\tTime  0.159 ( 0.176)\tLoss 1.3617e+00 (1.1378e+00)\tAcc@1  57.50 ( 66.65)\tAcc@5  91.25 ( 91.89)\n","==> Train Accuracy: Acc@1 66.654 || Acc@5 91.888\n","==> Test Accuracy:  Acc@1 58.510 || Acc@5 85.720\n","==> 72.98 seconds to train this epoch\n","\n","\n","----- epoch: 22, lr: 0.1 -----\n","Epoch: [22][  0/391]\tTime  0.221 ( 0.221)\tLoss 1.0786e+00 (1.0786e+00)\tAcc@1  70.31 ( 70.31)\tAcc@5  92.97 ( 92.97)\n","Epoch: [22][ 30/391]\tTime  0.175 ( 0.177)\tLoss 1.2086e+00 (1.0336e+00)\tAcc@1  61.72 ( 69.38)\tAcc@5  92.97 ( 93.27)\n","Epoch: [22][ 60/391]\tTime  0.175 ( 0.176)\tLoss 1.1524e+00 (1.0529e+00)\tAcc@1  64.84 ( 68.76)\tAcc@5  91.41 ( 92.85)\n","Epoch: [22][ 90/391]\tTime  0.175 ( 0.176)\tLoss 1.0859e+00 (1.0656e+00)\tAcc@1  67.97 ( 68.23)\tAcc@5  92.19 ( 92.93)\n","Epoch: [22][120/391]\tTime  0.176 ( 0.176)\tLoss 7.9545e-01 (1.0706e+00)\tAcc@1  78.91 ( 68.25)\tAcc@5  93.75 ( 92.81)\n","Epoch: [22][150/391]\tTime  0.174 ( 0.176)\tLoss 1.1020e+00 (1.0779e+00)\tAcc@1  62.50 ( 68.03)\tAcc@5  96.09 ( 92.69)\n","Epoch: [22][180/391]\tTime  0.175 ( 0.176)\tLoss 1.2372e+00 (1.0901e+00)\tAcc@1  66.41 ( 67.72)\tAcc@5  91.41 ( 92.47)\n","Epoch: [22][210/391]\tTime  0.176 ( 0.176)\tLoss 1.0493e+00 (1.0926e+00)\tAcc@1  71.09 ( 67.79)\tAcc@5  92.19 ( 92.34)\n","Epoch: [22][240/391]\tTime  0.175 ( 0.176)\tLoss 1.0741e+00 (1.1015e+00)\tAcc@1  67.19 ( 67.50)\tAcc@5  93.75 ( 92.21)\n","Epoch: [22][270/391]\tTime  0.175 ( 0.176)\tLoss 1.2834e+00 (1.1084e+00)\tAcc@1  64.06 ( 67.42)\tAcc@5  91.41 ( 92.14)\n","Epoch: [22][300/391]\tTime  0.175 ( 0.176)\tLoss 1.0794e+00 (1.1127e+00)\tAcc@1  67.97 ( 67.36)\tAcc@5  91.41 ( 92.10)\n","Epoch: [22][330/391]\tTime  0.175 ( 0.176)\tLoss 1.1335e+00 (1.1141e+00)\tAcc@1  64.06 ( 67.35)\tAcc@5  95.31 ( 92.08)\n","Epoch: [22][360/391]\tTime  0.176 ( 0.176)\tLoss 1.0537e+00 (1.1176e+00)\tAcc@1  73.44 ( 67.25)\tAcc@5  90.62 ( 92.01)\n","Epoch: [22][390/391]\tTime  0.158 ( 0.176)\tLoss 9.8908e-01 (1.1200e+00)\tAcc@1  70.00 ( 67.14)\tAcc@5  93.75 ( 92.02)\n","==> Train Accuracy: Acc@1 67.140 || Acc@5 92.024\n","==> Test Accuracy:  Acc@1 55.970 || Acc@5 84.830\n","==> 72.99 seconds to train this epoch\n","\n","\n","----- epoch: 23, lr: 0.1 -----\n","Epoch: [23][  0/391]\tTime  0.212 ( 0.212)\tLoss 1.1345e+00 (1.1345e+00)\tAcc@1  66.41 ( 66.41)\tAcc@5  91.41 ( 91.41)\n","Epoch: [23][ 30/391]\tTime  0.175 ( 0.177)\tLoss 1.0836e+00 (1.0151e+00)\tAcc@1  72.66 ( 69.96)\tAcc@5  90.62 ( 93.75)\n","Epoch: [23][ 60/391]\tTime  0.176 ( 0.176)\tLoss 1.0210e+00 (1.0301e+00)\tAcc@1  67.19 ( 69.66)\tAcc@5  95.31 ( 93.42)\n","Epoch: [23][ 90/391]\tTime  0.176 ( 0.176)\tLoss 9.5138e-01 (1.0524e+00)\tAcc@1  68.75 ( 68.78)\tAcc@5  95.31 ( 93.23)\n","Epoch: [23][120/391]\tTime  0.176 ( 0.176)\tLoss 1.1650e+00 (1.0633e+00)\tAcc@1  69.53 ( 68.52)\tAcc@5  91.41 ( 92.87)\n","Epoch: [23][150/391]\tTime  0.176 ( 0.176)\tLoss 8.5118e-01 (1.0673e+00)\tAcc@1  71.88 ( 68.31)\tAcc@5  93.75 ( 92.76)\n","Epoch: [23][180/391]\tTime  0.176 ( 0.176)\tLoss 8.9852e-01 (1.0758e+00)\tAcc@1  70.31 ( 68.20)\tAcc@5  95.31 ( 92.61)\n","Epoch: [23][210/391]\tTime  0.182 ( 0.176)\tLoss 1.1569e+00 (1.0814e+00)\tAcc@1  68.75 ( 68.18)\tAcc@5  92.19 ( 92.54)\n","Epoch: [23][240/391]\tTime  0.177 ( 0.176)\tLoss 9.2527e-01 (1.0864e+00)\tAcc@1  71.09 ( 68.00)\tAcc@5  95.31 ( 92.50)\n","Epoch: [23][270/391]\tTime  0.175 ( 0.176)\tLoss 1.2642e+00 (1.0970e+00)\tAcc@1  62.50 ( 67.76)\tAcc@5  90.62 ( 92.28)\n","Epoch: [23][300/391]\tTime  0.174 ( 0.176)\tLoss 1.0633e+00 (1.1015e+00)\tAcc@1  67.19 ( 67.62)\tAcc@5  92.97 ( 92.29)\n","Epoch: [23][330/391]\tTime  0.176 ( 0.176)\tLoss 1.1589e+00 (1.1065e+00)\tAcc@1  64.84 ( 67.56)\tAcc@5  90.62 ( 92.18)\n","Epoch: [23][360/391]\tTime  0.176 ( 0.176)\tLoss 1.3056e+00 (1.1130e+00)\tAcc@1  59.38 ( 67.48)\tAcc@5  91.41 ( 92.08)\n","Epoch: [23][390/391]\tTime  0.157 ( 0.176)\tLoss 1.1439e+00 (1.1147e+00)\tAcc@1  60.00 ( 67.46)\tAcc@5  91.25 ( 92.08)\n","==> Train Accuracy: Acc@1 67.462 || Acc@5 92.078\n","==> Test Accuracy:  Acc@1 58.500 || Acc@5 85.160\n","==> 72.98 seconds to train this epoch\n","\n","\n","----- epoch: 24, lr: 0.1 -----\n","Epoch: [24][  0/391]\tTime  0.209 ( 0.209)\tLoss 1.1748e+00 (1.1748e+00)\tAcc@1  67.19 ( 67.19)\tAcc@5  90.62 ( 90.62)\n","Epoch: [24][ 30/391]\tTime  0.176 ( 0.176)\tLoss 1.0831e+00 (9.7620e-01)\tAcc@1  69.53 ( 71.12)\tAcc@5  89.84 ( 93.70)\n","Epoch: [24][ 60/391]\tTime  0.173 ( 0.176)\tLoss 9.9816e-01 (9.9025e-01)\tAcc@1  73.44 ( 70.71)\tAcc@5  92.19 ( 93.40)\n","Epoch: [24][ 90/391]\tTime  0.170 ( 0.176)\tLoss 8.9796e-01 (1.0247e+00)\tAcc@1  71.09 ( 69.68)\tAcc@5  92.97 ( 93.05)\n","Epoch: [24][120/391]\tTime  0.176 ( 0.176)\tLoss 9.4926e-01 (1.0423e+00)\tAcc@1  70.31 ( 69.22)\tAcc@5  96.09 ( 92.83)\n","Epoch: [24][150/391]\tTime  0.176 ( 0.176)\tLoss 1.0776e+00 (1.0520e+00)\tAcc@1  70.31 ( 68.89)\tAcc@5  95.31 ( 92.81)\n","Epoch: [24][180/391]\tTime  0.176 ( 0.176)\tLoss 1.1323e+00 (1.0695e+00)\tAcc@1  68.75 ( 68.35)\tAcc@5  91.41 ( 92.63)\n","Epoch: [24][210/391]\tTime  0.176 ( 0.176)\tLoss 1.1814e+00 (1.0769e+00)\tAcc@1  63.28 ( 68.14)\tAcc@5  92.19 ( 92.53)\n","Epoch: [24][240/391]\tTime  0.175 ( 0.176)\tLoss 1.3057e+00 (1.0840e+00)\tAcc@1  66.41 ( 67.92)\tAcc@5  89.06 ( 92.43)\n","Epoch: [24][270/391]\tTime  0.176 ( 0.176)\tLoss 1.2865e+00 (1.0847e+00)\tAcc@1  57.81 ( 67.90)\tAcc@5  92.19 ( 92.44)\n","Epoch: [24][300/391]\tTime  0.175 ( 0.176)\tLoss 9.0525e-01 (1.0911e+00)\tAcc@1  75.00 ( 67.76)\tAcc@5  96.09 ( 92.32)\n","Epoch: [24][330/391]\tTime  0.175 ( 0.176)\tLoss 1.0720e+00 (1.0952e+00)\tAcc@1  66.41 ( 67.69)\tAcc@5  93.75 ( 92.29)\n","Epoch: [24][360/391]\tTime  0.175 ( 0.176)\tLoss 1.3423e+00 (1.0976e+00)\tAcc@1  60.94 ( 67.65)\tAcc@5  87.50 ( 92.25)\n","Epoch: [24][390/391]\tTime  0.158 ( 0.176)\tLoss 1.2931e+00 (1.1003e+00)\tAcc@1  62.50 ( 67.63)\tAcc@5  91.25 ( 92.24)\n","==> Train Accuracy: Acc@1 67.630 || Acc@5 92.236\n","==> Test Accuracy:  Acc@1 59.450 || Acc@5 86.590\n","==> 72.96 seconds to train this epoch\n","\n","\n","----- epoch: 25, lr: 0.1 -----\n","Epoch: [25][  0/391]\tTime  0.227 ( 0.227)\tLoss 1.1201e+00 (1.1201e+00)\tAcc@1  68.75 ( 68.75)\tAcc@5  91.41 ( 91.41)\n","Epoch: [25][ 30/391]\tTime  0.173 ( 0.177)\tLoss 1.0830e+00 (1.0369e+00)\tAcc@1  67.97 ( 69.68)\tAcc@5  96.09 ( 93.25)\n","Epoch: [25][ 60/391]\tTime  0.175 ( 0.176)\tLoss 1.1202e+00 (1.0275e+00)\tAcc@1  68.75 ( 69.74)\tAcc@5  91.41 ( 93.28)\n","Epoch: [25][ 90/391]\tTime  0.175 ( 0.176)\tLoss 9.1848e-01 (1.0360e+00)\tAcc@1  69.53 ( 69.13)\tAcc@5  93.75 ( 93.17)\n","Epoch: [25][120/391]\tTime  0.175 ( 0.176)\tLoss 1.0683e+00 (1.0406e+00)\tAcc@1  68.75 ( 69.18)\tAcc@5  92.97 ( 93.13)\n","Epoch: [25][150/391]\tTime  0.176 ( 0.176)\tLoss 1.0726e+00 (1.0510e+00)\tAcc@1  69.53 ( 68.93)\tAcc@5  92.97 ( 93.04)\n","Epoch: [25][180/391]\tTime  0.175 ( 0.176)\tLoss 1.0113e+00 (1.0529e+00)\tAcc@1  75.78 ( 68.97)\tAcc@5  91.41 ( 92.99)\n","Epoch: [25][210/391]\tTime  0.174 ( 0.176)\tLoss 1.0343e+00 (1.0567e+00)\tAcc@1  73.44 ( 68.91)\tAcc@5  92.19 ( 92.89)\n","Epoch: [25][240/391]\tTime  0.175 ( 0.176)\tLoss 9.7321e-01 (1.0700e+00)\tAcc@1  69.53 ( 68.59)\tAcc@5  92.97 ( 92.74)\n","Epoch: [25][270/391]\tTime  0.176 ( 0.176)\tLoss 1.1627e+00 (1.0732e+00)\tAcc@1  64.06 ( 68.50)\tAcc@5  93.75 ( 92.75)\n","Epoch: [25][300/391]\tTime  0.175 ( 0.176)\tLoss 1.0052e+00 (1.0819e+00)\tAcc@1  68.75 ( 68.29)\tAcc@5  94.53 ( 92.58)\n","Epoch: [25][330/391]\tTime  0.177 ( 0.176)\tLoss 1.0249e+00 (1.0924e+00)\tAcc@1  74.22 ( 67.98)\tAcc@5  92.19 ( 92.44)\n","Epoch: [25][360/391]\tTime  0.170 ( 0.176)\tLoss 9.7398e-01 (1.0932e+00)\tAcc@1  69.53 ( 68.01)\tAcc@5  95.31 ( 92.43)\n","Epoch: [25][390/391]\tTime  0.160 ( 0.176)\tLoss 9.9518e-01 (1.0942e+00)\tAcc@1  71.25 ( 67.95)\tAcc@5  92.50 ( 92.39)\n","==> Train Accuracy: Acc@1 67.948 || Acc@5 92.394\n","==> Test Accuracy:  Acc@1 53.370 || Acc@5 82.150\n","==> 72.95 seconds to train this epoch\n","\n","\n","----- epoch: 26, lr: 0.1 -----\n","Epoch: [26][  0/391]\tTime  0.226 ( 0.226)\tLoss 9.8902e-01 (9.8902e-01)\tAcc@1  67.97 ( 67.97)\tAcc@5  92.97 ( 92.97)\n","Epoch: [26][ 30/391]\tTime  0.174 ( 0.176)\tLoss 9.4923e-01 (9.8625e-01)\tAcc@1  72.66 ( 70.69)\tAcc@5  92.97 ( 93.55)\n","Epoch: [26][ 60/391]\tTime  0.175 ( 0.176)\tLoss 1.0534e+00 (9.8038e-01)\tAcc@1  69.53 ( 70.63)\tAcc@5  91.41 ( 93.90)\n","Epoch: [26][ 90/391]\tTime  0.176 ( 0.176)\tLoss 9.5612e-01 (1.0003e+00)\tAcc@1  72.66 ( 70.42)\tAcc@5  92.97 ( 93.63)\n","Epoch: [26][120/391]\tTime  0.175 ( 0.176)\tLoss 1.1475e+00 (1.0150e+00)\tAcc@1  70.31 ( 70.22)\tAcc@5  89.84 ( 93.33)\n","Epoch: [26][150/391]\tTime  0.175 ( 0.175)\tLoss 1.0763e+00 (1.0273e+00)\tAcc@1  68.75 ( 69.98)\tAcc@5  91.41 ( 93.18)\n","Epoch: [26][180/391]\tTime  0.174 ( 0.175)\tLoss 1.1682e+00 (1.0421e+00)\tAcc@1  64.84 ( 69.58)\tAcc@5  89.06 ( 92.96)\n","Epoch: [26][210/391]\tTime  0.174 ( 0.175)\tLoss 1.1703e+00 (1.0439e+00)\tAcc@1  67.97 ( 69.57)\tAcc@5  89.06 ( 92.89)\n","Epoch: [26][240/391]\tTime  0.180 ( 0.175)\tLoss 1.0725e+00 (1.0570e+00)\tAcc@1  66.41 ( 69.20)\tAcc@5  92.19 ( 92.74)\n","Epoch: [26][270/391]\tTime  0.175 ( 0.175)\tLoss 1.1552e+00 (1.0620e+00)\tAcc@1  68.75 ( 69.05)\tAcc@5  90.62 ( 92.65)\n","Epoch: [26][300/391]\tTime  0.174 ( 0.175)\tLoss 1.4442e+00 (1.0671e+00)\tAcc@1  63.28 ( 68.96)\tAcc@5  85.16 ( 92.55)\n","Epoch: [26][330/391]\tTime  0.177 ( 0.175)\tLoss 1.1788e+00 (1.0713e+00)\tAcc@1  69.53 ( 68.85)\tAcc@5  89.06 ( 92.50)\n","Epoch: [26][360/391]\tTime  0.174 ( 0.175)\tLoss 9.0838e-01 (1.0767e+00)\tAcc@1  71.88 ( 68.65)\tAcc@5  94.53 ( 92.45)\n","Epoch: [26][390/391]\tTime  0.157 ( 0.175)\tLoss 1.1259e+00 (1.0775e+00)\tAcc@1  68.75 ( 68.64)\tAcc@5  92.50 ( 92.46)\n","==> Train Accuracy: Acc@1 68.638 || Acc@5 92.456\n","==> Test Accuracy:  Acc@1 58.590 || Acc@5 85.560\n","==> 72.87 seconds to train this epoch\n","\n","\n","----- epoch: 27, lr: 0.1 -----\n","Epoch: [27][  0/391]\tTime  0.226 ( 0.226)\tLoss 9.3467e-01 (9.3467e-01)\tAcc@1  71.09 ( 71.09)\tAcc@5  96.09 ( 96.09)\n","Epoch: [27][ 30/391]\tTime  0.175 ( 0.177)\tLoss 1.1627e+00 (1.0341e+00)\tAcc@1  71.88 ( 69.73)\tAcc@5  93.75 ( 92.92)\n","Epoch: [27][ 60/391]\tTime  0.175 ( 0.176)\tLoss 9.4245e-01 (1.0323e+00)\tAcc@1  71.09 ( 69.79)\tAcc@5  92.97 ( 92.99)\n","Epoch: [27][ 90/391]\tTime  0.174 ( 0.176)\tLoss 9.1184e-01 (1.0381e+00)\tAcc@1  74.22 ( 69.39)\tAcc@5  94.53 ( 92.78)\n","Epoch: [27][120/391]\tTime  0.175 ( 0.176)\tLoss 1.0449e+00 (1.0509e+00)\tAcc@1  71.88 ( 69.23)\tAcc@5  92.97 ( 92.69)\n","Epoch: [27][150/391]\tTime  0.176 ( 0.175)\tLoss 1.1039e+00 (1.0577e+00)\tAcc@1  66.41 ( 68.92)\tAcc@5  92.97 ( 92.77)\n","Epoch: [27][180/391]\tTime  0.175 ( 0.175)\tLoss 1.0093e+00 (1.0540e+00)\tAcc@1  67.19 ( 68.95)\tAcc@5  92.97 ( 92.85)\n","Epoch: [27][210/391]\tTime  0.175 ( 0.175)\tLoss 9.8808e-01 (1.0567e+00)\tAcc@1  69.53 ( 68.81)\tAcc@5  93.75 ( 92.87)\n","Epoch: [27][240/391]\tTime  0.176 ( 0.175)\tLoss 1.0903e+00 (1.0668e+00)\tAcc@1  71.09 ( 68.64)\tAcc@5  91.41 ( 92.72)\n","Epoch: [27][270/391]\tTime  0.178 ( 0.175)\tLoss 1.0378e+00 (1.0714e+00)\tAcc@1  64.06 ( 68.61)\tAcc@5  94.53 ( 92.63)\n","Epoch: [27][300/391]\tTime  0.174 ( 0.175)\tLoss 1.0628e+00 (1.0722e+00)\tAcc@1  65.62 ( 68.60)\tAcc@5  92.97 ( 92.66)\n","Epoch: [27][330/391]\tTime  0.176 ( 0.175)\tLoss 1.1804e+00 (1.0690e+00)\tAcc@1  65.62 ( 68.70)\tAcc@5  89.06 ( 92.65)\n","Epoch: [27][360/391]\tTime  0.175 ( 0.175)\tLoss 1.1519e+00 (1.0728e+00)\tAcc@1  67.19 ( 68.64)\tAcc@5  91.41 ( 92.56)\n","Epoch: [27][390/391]\tTime  0.157 ( 0.175)\tLoss 9.5520e-01 (1.0757e+00)\tAcc@1  68.75 ( 68.52)\tAcc@5  92.50 ( 92.52)\n","==> Train Accuracy: Acc@1 68.522 || Acc@5 92.518\n","==> Test Accuracy:  Acc@1 58.880 || Acc@5 85.690\n","==> 72.87 seconds to train this epoch\n","\n","\n","----- epoch: 28, lr: 0.1 -----\n","Epoch: [28][  0/391]\tTime  0.223 ( 0.223)\tLoss 1.0529e+00 (1.0529e+00)\tAcc@1  67.97 ( 67.97)\tAcc@5  94.53 ( 94.53)\n","Epoch: [28][ 30/391]\tTime  0.175 ( 0.176)\tLoss 9.6003e-01 (9.5224e-01)\tAcc@1  73.44 ( 71.65)\tAcc@5  90.62 ( 94.35)\n","Epoch: [28][ 60/391]\tTime  0.175 ( 0.176)\tLoss 1.1033e+00 (9.7389e-01)\tAcc@1  67.97 ( 71.26)\tAcc@5  92.97 ( 94.02)\n","Epoch: [28][ 90/391]\tTime  0.175 ( 0.176)\tLoss 8.6331e-01 (9.8945e-01)\tAcc@1  71.88 ( 70.87)\tAcc@5  95.31 ( 93.75)\n","Epoch: [28][120/391]\tTime  0.175 ( 0.175)\tLoss 1.1259e+00 (1.0051e+00)\tAcc@1  64.84 ( 70.29)\tAcc@5  93.75 ( 93.52)\n","Epoch: [28][150/391]\tTime  0.176 ( 0.175)\tLoss 1.0687e+00 (1.0226e+00)\tAcc@1  69.53 ( 69.84)\tAcc@5  89.84 ( 93.29)\n","Epoch: [28][180/391]\tTime  0.174 ( 0.175)\tLoss 1.2386e+00 (1.0321e+00)\tAcc@1  69.53 ( 69.58)\tAcc@5  86.72 ( 93.08)\n","Epoch: [28][210/391]\tTime  0.175 ( 0.175)\tLoss 8.3819e-01 (1.0387e+00)\tAcc@1  71.88 ( 69.45)\tAcc@5  95.31 ( 92.97)\n","Epoch: [28][240/391]\tTime  0.174 ( 0.175)\tLoss 1.0673e+00 (1.0438e+00)\tAcc@1  69.53 ( 69.31)\tAcc@5  92.97 ( 92.97)\n","Epoch: [28][270/391]\tTime  0.175 ( 0.175)\tLoss 1.0261e+00 (1.0437e+00)\tAcc@1  70.31 ( 69.33)\tAcc@5  92.19 ( 92.95)\n","Epoch: [28][300/391]\tTime  0.181 ( 0.175)\tLoss 8.8571e-01 (1.0457e+00)\tAcc@1  75.00 ( 69.33)\tAcc@5  92.19 ( 92.96)\n","Epoch: [28][330/391]\tTime  0.175 ( 0.175)\tLoss 1.3038e+00 (1.0477e+00)\tAcc@1  60.94 ( 69.26)\tAcc@5  92.19 ( 92.94)\n","Epoch: [28][360/391]\tTime  0.174 ( 0.175)\tLoss 1.1101e+00 (1.0539e+00)\tAcc@1  72.66 ( 69.07)\tAcc@5  88.28 ( 92.83)\n","Epoch: [28][390/391]\tTime  0.158 ( 0.175)\tLoss 1.1804e+00 (1.0593e+00)\tAcc@1  62.50 ( 68.90)\tAcc@5  90.00 ( 92.77)\n","==> Train Accuracy: Acc@1 68.898 || Acc@5 92.768\n","==> Test Accuracy:  Acc@1 52.690 || Acc@5 82.210\n","==> 72.84 seconds to train this epoch\n","\n","\n","----- epoch: 29, lr: 0.1 -----\n","Epoch: [29][  0/391]\tTime  0.241 ( 0.241)\tLoss 9.3484e-01 (9.3484e-01)\tAcc@1  78.12 ( 78.12)\tAcc@5  92.19 ( 92.19)\n","Epoch: [29][ 30/391]\tTime  0.174 ( 0.177)\tLoss 1.0895e+00 (9.7387e-01)\tAcc@1  67.19 ( 71.17)\tAcc@5  90.62 ( 93.70)\n","Epoch: [29][ 60/391]\tTime  0.174 ( 0.176)\tLoss 1.2665e+00 (9.7881e-01)\tAcc@1  64.06 ( 71.22)\tAcc@5  86.72 ( 93.69)\n","Epoch: [29][ 90/391]\tTime  0.176 ( 0.176)\tLoss 1.1181e+00 (9.9595e-01)\tAcc@1  69.53 ( 71.05)\tAcc@5  94.53 ( 93.49)\n","Epoch: [29][120/391]\tTime  0.177 ( 0.176)\tLoss 1.0115e+00 (1.0122e+00)\tAcc@1  71.09 ( 70.56)\tAcc@5  92.97 ( 93.31)\n","Epoch: [29][150/391]\tTime  0.175 ( 0.176)\tLoss 8.5429e-01 (1.0138e+00)\tAcc@1  73.44 ( 70.63)\tAcc@5  96.88 ( 93.25)\n","Epoch: [29][180/391]\tTime  0.175 ( 0.175)\tLoss 1.0716e+00 (1.0151e+00)\tAcc@1  66.41 ( 70.46)\tAcc@5  94.53 ( 93.29)\n","Epoch: [29][210/391]\tTime  0.175 ( 0.175)\tLoss 9.9685e-01 (1.0247e+00)\tAcc@1  67.97 ( 70.11)\tAcc@5  93.75 ( 93.21)\n","Epoch: [29][240/391]\tTime  0.174 ( 0.175)\tLoss 1.1343e+00 (1.0323e+00)\tAcc@1  64.06 ( 69.90)\tAcc@5  90.62 ( 93.10)\n","Epoch: [29][270/391]\tTime  0.176 ( 0.175)\tLoss 1.3167e+00 (1.0386e+00)\tAcc@1  59.38 ( 69.68)\tAcc@5  89.06 ( 93.04)\n","Epoch: [29][300/391]\tTime  0.176 ( 0.175)\tLoss 9.6816e-01 (1.0420e+00)\tAcc@1  73.44 ( 69.57)\tAcc@5  95.31 ( 92.99)\n","Epoch: [29][330/391]\tTime  0.175 ( 0.175)\tLoss 9.2283e-01 (1.0507e+00)\tAcc@1  70.31 ( 69.37)\tAcc@5  94.53 ( 92.86)\n","Epoch: [29][360/391]\tTime  0.175 ( 0.175)\tLoss 1.0805e+00 (1.0523e+00)\tAcc@1  62.50 ( 69.30)\tAcc@5  94.53 ( 92.89)\n","Epoch: [29][390/391]\tTime  0.158 ( 0.175)\tLoss 1.0350e+00 (1.0574e+00)\tAcc@1  67.50 ( 69.15)\tAcc@5  95.00 ( 92.79)\n","==> Train Accuracy: Acc@1 69.146 || Acc@5 92.788\n","==> Test Accuracy:  Acc@1 59.180 || Acc@5 87.370\n","==> 72.84 seconds to train this epoch\n","\n","\n","----- epoch: 30, lr: 0.1 -----\n","Epoch: [30][  0/391]\tTime  0.220 ( 0.220)\tLoss 1.0685e+00 (1.0685e+00)\tAcc@1  69.53 ( 69.53)\tAcc@5  89.84 ( 89.84)\n","Epoch: [30][ 30/391]\tTime  0.174 ( 0.176)\tLoss 1.0093e+00 (9.5116e-01)\tAcc@1  68.75 ( 71.85)\tAcc@5  94.53 ( 94.08)\n","Epoch: [30][ 60/391]\tTime  0.174 ( 0.176)\tLoss 1.1980e+00 (9.5968e-01)\tAcc@1  62.50 ( 71.30)\tAcc@5  91.41 ( 93.90)\n","Epoch: [30][ 90/391]\tTime  0.176 ( 0.175)\tLoss 9.8366e-01 (9.8808e-01)\tAcc@1  66.41 ( 70.62)\tAcc@5  96.09 ( 93.56)\n","Epoch: [30][120/391]\tTime  0.178 ( 0.175)\tLoss 8.9929e-01 (1.0012e+00)\tAcc@1  74.22 ( 70.39)\tAcc@5  93.75 ( 93.45)\n","Epoch: [30][150/391]\tTime  0.175 ( 0.175)\tLoss 7.9625e-01 (1.0041e+00)\tAcc@1  77.34 ( 70.32)\tAcc@5  96.88 ( 93.46)\n","Epoch: [30][180/391]\tTime  0.174 ( 0.175)\tLoss 1.0454e+00 (1.0109e+00)\tAcc@1  66.41 ( 69.91)\tAcc@5  91.41 ( 93.42)\n","Epoch: [30][210/391]\tTime  0.176 ( 0.175)\tLoss 1.2466e+00 (1.0174e+00)\tAcc@1  64.84 ( 69.63)\tAcc@5  86.72 ( 93.38)\n","Epoch: [30][240/391]\tTime  0.174 ( 0.175)\tLoss 1.1968e+00 (1.0283e+00)\tAcc@1  61.72 ( 69.39)\tAcc@5  89.06 ( 93.28)\n","Epoch: [30][270/391]\tTime  0.175 ( 0.175)\tLoss 1.1023e+00 (1.0345e+00)\tAcc@1  72.66 ( 69.27)\tAcc@5  91.41 ( 93.13)\n","Epoch: [30][300/391]\tTime  0.175 ( 0.175)\tLoss 1.0097e+00 (1.0357e+00)\tAcc@1  74.22 ( 69.27)\tAcc@5  91.41 ( 93.10)\n","Epoch: [30][330/391]\tTime  0.176 ( 0.175)\tLoss 1.2005e+00 (1.0430e+00)\tAcc@1  69.53 ( 69.11)\tAcc@5  88.28 ( 92.97)\n","Epoch: [30][360/391]\tTime  0.174 ( 0.175)\tLoss 9.7766e-01 (1.0424e+00)\tAcc@1  67.97 ( 69.20)\tAcc@5  92.97 ( 92.93)\n","Epoch: [30][390/391]\tTime  0.158 ( 0.175)\tLoss 1.2361e+00 (1.0456e+00)\tAcc@1  63.75 ( 69.17)\tAcc@5  88.75 ( 92.87)\n","==> Train Accuracy: Acc@1 69.170 || Acc@5 92.868\n","==> Test Accuracy:  Acc@1 56.820 || Acc@5 84.480\n","==> 72.83 seconds to train this epoch\n","\n","\n","----- epoch: 31, lr: 0.1 -----\n","Epoch: [31][  0/391]\tTime  0.213 ( 0.213)\tLoss 1.0998e+00 (1.0998e+00)\tAcc@1  65.62 ( 65.62)\tAcc@5  92.97 ( 92.97)\n","Epoch: [31][ 30/391]\tTime  0.175 ( 0.176)\tLoss 8.7951e-01 (9.3685e-01)\tAcc@1  76.56 ( 72.18)\tAcc@5  96.09 ( 94.58)\n","Epoch: [31][ 60/391]\tTime  0.174 ( 0.176)\tLoss 9.6662e-01 (9.6037e-01)\tAcc@1  72.66 ( 71.50)\tAcc@5  95.31 ( 94.24)\n","Epoch: [31][ 90/391]\tTime  0.174 ( 0.175)\tLoss 9.4851e-01 (9.6884e-01)\tAcc@1  67.19 ( 71.12)\tAcc@5  92.97 ( 93.99)\n","Epoch: [31][120/391]\tTime  0.175 ( 0.175)\tLoss 1.0244e+00 (9.7410e-01)\tAcc@1  61.72 ( 70.92)\tAcc@5  93.75 ( 93.90)\n","Epoch: [31][150/391]\tTime  0.173 ( 0.175)\tLoss 9.1277e-01 (9.8941e-01)\tAcc@1  73.44 ( 70.55)\tAcc@5  93.75 ( 93.72)\n","Epoch: [31][180/391]\tTime  0.175 ( 0.175)\tLoss 1.0729e+00 (1.0014e+00)\tAcc@1  69.53 ( 70.23)\tAcc@5  93.75 ( 93.56)\n","Epoch: [31][210/391]\tTime  0.174 ( 0.175)\tLoss 1.0680e+00 (1.0145e+00)\tAcc@1  70.31 ( 69.89)\tAcc@5  85.94 ( 93.35)\n","Epoch: [31][240/391]\tTime  0.175 ( 0.175)\tLoss 1.0665e+00 (1.0170e+00)\tAcc@1  69.53 ( 69.85)\tAcc@5  94.53 ( 93.34)\n","Epoch: [31][270/391]\tTime  0.176 ( 0.175)\tLoss 9.6140e-01 (1.0229e+00)\tAcc@1  70.31 ( 69.66)\tAcc@5  96.09 ( 93.30)\n","Epoch: [31][300/391]\tTime  0.175 ( 0.175)\tLoss 1.0765e+00 (1.0315e+00)\tAcc@1  66.41 ( 69.49)\tAcc@5  93.75 ( 93.20)\n","Epoch: [31][330/391]\tTime  0.175 ( 0.175)\tLoss 1.2373e+00 (1.0335e+00)\tAcc@1  65.62 ( 69.45)\tAcc@5  90.62 ( 93.13)\n","Epoch: [31][360/391]\tTime  0.176 ( 0.175)\tLoss 1.3642e+00 (1.0398e+00)\tAcc@1  62.50 ( 69.34)\tAcc@5  85.94 ( 93.01)\n","Epoch: [31][390/391]\tTime  0.159 ( 0.175)\tLoss 1.0580e+00 (1.0463e+00)\tAcc@1  72.50 ( 69.18)\tAcc@5  97.50 ( 92.91)\n","==> Train Accuracy: Acc@1 69.182 || Acc@5 92.906\n","==> Test Accuracy:  Acc@1 60.360 || Acc@5 87.040\n","==> 72.82 seconds to train this epoch\n","\n","\n","----- epoch: 32, lr: 0.1 -----\n","Epoch: [32][  0/391]\tTime  0.211 ( 0.211)\tLoss 8.1218e-01 (8.1218e-01)\tAcc@1  76.56 ( 76.56)\tAcc@5  96.09 ( 96.09)\n","Epoch: [32][ 30/391]\tTime  0.174 ( 0.176)\tLoss 8.8712e-01 (9.0636e-01)\tAcc@1  72.66 ( 72.61)\tAcc@5  94.53 ( 95.21)\n","Epoch: [32][ 60/391]\tTime  0.173 ( 0.176)\tLoss 1.0231e+00 (9.2601e-01)\tAcc@1  71.88 ( 72.32)\tAcc@5  92.97 ( 94.56)\n","Epoch: [32][ 90/391]\tTime  0.172 ( 0.176)\tLoss 9.7228e-01 (9.3783e-01)\tAcc@1  71.88 ( 72.00)\tAcc@5  92.97 ( 94.29)\n","Epoch: [32][120/391]\tTime  0.175 ( 0.175)\tLoss 1.1005e+00 (9.4879e-01)\tAcc@1  70.31 ( 71.77)\tAcc@5  92.19 ( 94.21)\n","Epoch: [32][150/391]\tTime  0.175 ( 0.175)\tLoss 1.0386e+00 (9.7988e-01)\tAcc@1  67.19 ( 70.86)\tAcc@5  92.19 ( 93.80)\n","Epoch: [32][180/391]\tTime  0.175 ( 0.175)\tLoss 9.5883e-01 (9.8846e-01)\tAcc@1  72.66 ( 70.64)\tAcc@5  92.19 ( 93.72)\n","Epoch: [32][210/391]\tTime  0.176 ( 0.175)\tLoss 1.1053e+00 (1.0005e+00)\tAcc@1  65.62 ( 70.26)\tAcc@5  92.19 ( 93.62)\n","Epoch: [32][240/391]\tTime  0.175 ( 0.175)\tLoss 1.0988e+00 (1.0102e+00)\tAcc@1  71.09 ( 70.20)\tAcc@5  91.41 ( 93.43)\n","Epoch: [32][270/391]\tTime  0.176 ( 0.175)\tLoss 8.9851e-01 (1.0208e+00)\tAcc@1  73.44 ( 69.90)\tAcc@5  94.53 ( 93.29)\n","Epoch: [32][300/391]\tTime  0.175 ( 0.175)\tLoss 1.0144e+00 (1.0239e+00)\tAcc@1  71.88 ( 69.84)\tAcc@5  89.06 ( 93.24)\n","Epoch: [32][330/391]\tTime  0.176 ( 0.175)\tLoss 1.0591e+00 (1.0259e+00)\tAcc@1  71.88 ( 69.85)\tAcc@5  91.41 ( 93.19)\n","Epoch: [32][360/391]\tTime  0.175 ( 0.175)\tLoss 1.1507e+00 (1.0335e+00)\tAcc@1  64.84 ( 69.67)\tAcc@5  94.53 ( 93.06)\n","Epoch: [32][390/391]\tTime  0.158 ( 0.175)\tLoss 1.1819e+00 (1.0373e+00)\tAcc@1  66.25 ( 69.63)\tAcc@5  92.50 ( 93.02)\n","==> Train Accuracy: Acc@1 69.632 || Acc@5 93.016\n","==> Test Accuracy:  Acc@1 59.250 || Acc@5 84.850\n","==> 72.84 seconds to train this epoch\n","\n","\n","----- epoch: 33, lr: 0.1 -----\n","Epoch: [33][  0/391]\tTime  0.212 ( 0.212)\tLoss 6.6362e-01 (6.6362e-01)\tAcc@1  81.25 ( 81.25)\tAcc@5  97.66 ( 97.66)\n","Epoch: [33][ 30/391]\tTime  0.174 ( 0.176)\tLoss 8.7763e-01 (9.4365e-01)\tAcc@1  75.00 ( 72.30)\tAcc@5  94.53 ( 93.67)\n","Epoch: [33][ 60/391]\tTime  0.175 ( 0.176)\tLoss 1.0162e+00 (9.4716e-01)\tAcc@1  71.88 ( 72.44)\tAcc@5  92.97 ( 93.87)\n","Epoch: [33][ 90/391]\tTime  0.177 ( 0.176)\tLoss 8.8647e-01 (9.5200e-01)\tAcc@1  73.44 ( 72.47)\tAcc@5  95.31 ( 94.00)\n","Epoch: [33][120/391]\tTime  0.175 ( 0.176)\tLoss 7.4552e-01 (9.6338e-01)\tAcc@1  81.25 ( 72.14)\tAcc@5  95.31 ( 93.86)\n","Epoch: [33][150/391]\tTime  0.176 ( 0.175)\tLoss 9.3561e-01 (9.7604e-01)\tAcc@1  71.09 ( 71.50)\tAcc@5  92.97 ( 93.80)\n","Epoch: [33][180/391]\tTime  0.176 ( 0.175)\tLoss 8.6410e-01 (9.8418e-01)\tAcc@1  77.34 ( 71.24)\tAcc@5  95.31 ( 93.73)\n","Epoch: [33][210/391]\tTime  0.173 ( 0.175)\tLoss 1.0143e+00 (9.9504e-01)\tAcc@1  68.75 ( 70.93)\tAcc@5  94.53 ( 93.59)\n","Epoch: [33][240/391]\tTime  0.175 ( 0.175)\tLoss 9.9207e-01 (1.0012e+00)\tAcc@1  67.97 ( 70.76)\tAcc@5  94.53 ( 93.49)\n","Epoch: [33][270/391]\tTime  0.175 ( 0.175)\tLoss 9.7136e-01 (1.0076e+00)\tAcc@1  75.00 ( 70.54)\tAcc@5  95.31 ( 93.43)\n","Epoch: [33][300/391]\tTime  0.175 ( 0.175)\tLoss 1.1096e+00 (1.0127e+00)\tAcc@1  64.84 ( 70.38)\tAcc@5  90.62 ( 93.32)\n","Epoch: [33][330/391]\tTime  0.175 ( 0.175)\tLoss 1.1601e+00 (1.0200e+00)\tAcc@1  64.84 ( 70.20)\tAcc@5  91.41 ( 93.26)\n","Epoch: [33][360/391]\tTime  0.174 ( 0.175)\tLoss 1.0558e+00 (1.0238e+00)\tAcc@1  74.22 ( 70.13)\tAcc@5  89.84 ( 93.20)\n","Epoch: [33][390/391]\tTime  0.159 ( 0.175)\tLoss 1.0871e+00 (1.0305e+00)\tAcc@1  63.75 ( 69.94)\tAcc@5  92.50 ( 93.10)\n","==> Train Accuracy: Acc@1 69.936 || Acc@5 93.102\n","==> Test Accuracy:  Acc@1 59.610 || Acc@5 86.860\n","==> 72.89 seconds to train this epoch\n","\n","\n","----- epoch: 34, lr: 0.1 -----\n","Epoch: [34][  0/391]\tTime  0.209 ( 0.209)\tLoss 9.5865e-01 (9.5865e-01)\tAcc@1  72.66 ( 72.66)\tAcc@5  96.09 ( 96.09)\n","Epoch: [34][ 30/391]\tTime  0.174 ( 0.176)\tLoss 8.5452e-01 (8.7723e-01)\tAcc@1  71.09 ( 73.66)\tAcc@5  96.09 ( 94.88)\n","Epoch: [34][ 60/391]\tTime  0.174 ( 0.176)\tLoss 1.0345e+00 (8.9566e-01)\tAcc@1  71.88 ( 73.46)\tAcc@5  94.53 ( 94.75)\n","Epoch: [34][ 90/391]\tTime  0.175 ( 0.176)\tLoss 9.2229e-01 (9.1774e-01)\tAcc@1  73.44 ( 72.86)\tAcc@5  92.97 ( 94.42)\n","Epoch: [34][120/391]\tTime  0.178 ( 0.175)\tLoss 1.0719e+00 (9.4647e-01)\tAcc@1  66.41 ( 72.13)\tAcc@5  92.97 ( 94.08)\n","Epoch: [34][150/391]\tTime  0.171 ( 0.175)\tLoss 9.4749e-01 (9.6316e-01)\tAcc@1  75.78 ( 71.63)\tAcc@5  92.97 ( 93.83)\n","Epoch: [34][180/391]\tTime  0.175 ( 0.175)\tLoss 1.1701e+00 (9.7684e-01)\tAcc@1  67.97 ( 71.22)\tAcc@5  90.62 ( 93.75)\n","Epoch: [34][210/391]\tTime  0.175 ( 0.175)\tLoss 8.6950e-01 (9.8906e-01)\tAcc@1  75.78 ( 70.95)\tAcc@5  93.75 ( 93.62)\n","Epoch: [34][240/391]\tTime  0.174 ( 0.175)\tLoss 8.9769e-01 (9.9714e-01)\tAcc@1  74.22 ( 70.83)\tAcc@5  94.53 ( 93.54)\n","Epoch: [34][270/391]\tTime  0.175 ( 0.175)\tLoss 1.1171e+00 (1.0015e+00)\tAcc@1  67.97 ( 70.72)\tAcc@5  92.97 ( 93.47)\n","Epoch: [34][300/391]\tTime  0.175 ( 0.175)\tLoss 1.2225e+00 (1.0100e+00)\tAcc@1  64.84 ( 70.36)\tAcc@5  89.84 ( 93.39)\n","Epoch: [34][330/391]\tTime  0.176 ( 0.175)\tLoss 1.0699e+00 (1.0203e+00)\tAcc@1  67.97 ( 70.11)\tAcc@5  92.97 ( 93.27)\n","Epoch: [34][360/391]\tTime  0.176 ( 0.175)\tLoss 1.0515e+00 (1.0226e+00)\tAcc@1  69.53 ( 70.02)\tAcc@5  92.97 ( 93.23)\n","Epoch: [34][390/391]\tTime  0.159 ( 0.175)\tLoss 1.1885e+00 (1.0259e+00)\tAcc@1  71.25 ( 69.96)\tAcc@5  88.75 ( 93.16)\n","==> Train Accuracy: Acc@1 69.958 || Acc@5 93.164\n","==> Test Accuracy:  Acc@1 58.380 || Acc@5 85.960\n","==> 72.86 seconds to train this epoch\n","\n","\n","----- epoch: 35, lr: 0.1 -----\n","Epoch: [35][  0/391]\tTime  0.229 ( 0.229)\tLoss 8.2873e-01 (8.2873e-01)\tAcc@1  75.78 ( 75.78)\tAcc@5  96.88 ( 96.88)\n","Epoch: [35][ 30/391]\tTime  0.175 ( 0.177)\tLoss 9.0109e-01 (9.4088e-01)\tAcc@1  70.31 ( 72.33)\tAcc@5  94.53 ( 94.63)\n","Epoch: [35][ 60/391]\tTime  0.175 ( 0.176)\tLoss 8.8412e-01 (9.5207e-01)\tAcc@1  71.09 ( 71.86)\tAcc@5  94.53 ( 94.34)\n","Epoch: [35][ 90/391]\tTime  0.176 ( 0.176)\tLoss 1.0215e+00 (9.5820e-01)\tAcc@1  71.09 ( 71.64)\tAcc@5  93.75 ( 94.35)\n","Epoch: [35][120/391]\tTime  0.176 ( 0.176)\tLoss 1.0408e+00 (9.7062e-01)\tAcc@1  73.44 ( 71.11)\tAcc@5  92.97 ( 94.19)\n","Epoch: [35][150/391]\tTime  0.181 ( 0.176)\tLoss 8.8285e-01 (9.8148e-01)\tAcc@1  73.44 ( 70.89)\tAcc@5  96.88 ( 94.02)\n","Epoch: [35][180/391]\tTime  0.173 ( 0.176)\tLoss 8.1663e-01 (9.8145e-01)\tAcc@1  74.22 ( 70.74)\tAcc@5  96.88 ( 93.99)\n","Epoch: [35][210/391]\tTime  0.175 ( 0.175)\tLoss 1.2817e+00 (9.9205e-01)\tAcc@1  64.84 ( 70.45)\tAcc@5  86.72 ( 93.84)\n","Epoch: [35][240/391]\tTime  0.176 ( 0.175)\tLoss 9.1115e-01 (9.9779e-01)\tAcc@1  75.00 ( 70.28)\tAcc@5  92.19 ( 93.71)\n","Epoch: [35][270/391]\tTime  0.176 ( 0.175)\tLoss 1.0900e+00 (1.0086e+00)\tAcc@1  71.88 ( 70.00)\tAcc@5  92.19 ( 93.54)\n","Epoch: [35][300/391]\tTime  0.176 ( 0.175)\tLoss 9.9670e-01 (1.0112e+00)\tAcc@1  72.66 ( 69.97)\tAcc@5  89.84 ( 93.46)\n","Epoch: [35][330/391]\tTime  0.174 ( 0.175)\tLoss 1.0479e+00 (1.0162e+00)\tAcc@1  67.97 ( 69.87)\tAcc@5  92.97 ( 93.39)\n","Epoch: [35][360/391]\tTime  0.175 ( 0.175)\tLoss 9.6510e-01 (1.0184e+00)\tAcc@1  71.88 ( 69.86)\tAcc@5  94.53 ( 93.33)\n","Epoch: [35][390/391]\tTime  0.159 ( 0.175)\tLoss 1.1972e+00 (1.0206e+00)\tAcc@1  65.00 ( 69.79)\tAcc@5  90.00 ( 93.27)\n","==> Train Accuracy: Acc@1 69.790 || Acc@5 93.268\n","==> Test Accuracy:  Acc@1 57.000 || Acc@5 85.650\n","==> 72.89 seconds to train this epoch\n","\n","\n","----- epoch: 36, lr: 0.1 -----\n","Epoch: [36][  0/391]\tTime  0.222 ( 0.222)\tLoss 8.6417e-01 (8.6417e-01)\tAcc@1  74.22 ( 74.22)\tAcc@5  92.97 ( 92.97)\n","Epoch: [36][ 30/391]\tTime  0.175 ( 0.176)\tLoss 9.0382e-01 (9.3239e-01)\tAcc@1  73.44 ( 71.82)\tAcc@5  97.66 ( 93.90)\n","Epoch: [36][ 60/391]\tTime  0.176 ( 0.176)\tLoss 1.0481e+00 (9.5691e-01)\tAcc@1  70.31 ( 71.47)\tAcc@5  89.84 ( 93.87)\n","Epoch: [36][ 90/391]\tTime  0.175 ( 0.176)\tLoss 1.0897e+00 (9.5442e-01)\tAcc@1  71.88 ( 71.76)\tAcc@5  93.75 ( 94.04)\n","Epoch: [36][120/391]\tTime  0.175 ( 0.176)\tLoss 6.8722e-01 (9.6247e-01)\tAcc@1  81.25 ( 71.51)\tAcc@5  96.88 ( 94.01)\n","Epoch: [36][150/391]\tTime  0.176 ( 0.176)\tLoss 1.1929e+00 (9.6838e-01)\tAcc@1  57.81 ( 71.34)\tAcc@5  93.75 ( 93.94)\n","Epoch: [36][180/391]\tTime  0.175 ( 0.176)\tLoss 1.1609e+00 (9.8471e-01)\tAcc@1  67.19 ( 70.92)\tAcc@5  90.62 ( 93.77)\n","Epoch: [36][210/391]\tTime  0.176 ( 0.175)\tLoss 1.1454e+00 (9.9410e-01)\tAcc@1  67.19 ( 70.63)\tAcc@5  91.41 ( 93.67)\n","Epoch: [36][240/391]\tTime  0.175 ( 0.175)\tLoss 9.6510e-01 (9.9937e-01)\tAcc@1  74.22 ( 70.52)\tAcc@5  93.75 ( 93.66)\n","Epoch: [36][270/391]\tTime  0.176 ( 0.175)\tLoss 1.1277e+00 (1.0087e+00)\tAcc@1  70.31 ( 70.29)\tAcc@5  93.75 ( 93.52)\n","Epoch: [36][300/391]\tTime  0.176 ( 0.175)\tLoss 1.0566e+00 (1.0157e+00)\tAcc@1  69.53 ( 70.05)\tAcc@5  94.53 ( 93.45)\n","Epoch: [36][330/391]\tTime  0.177 ( 0.175)\tLoss 1.0269e+00 (1.0195e+00)\tAcc@1  70.31 ( 69.91)\tAcc@5  92.19 ( 93.41)\n","Epoch: [36][360/391]\tTime  0.175 ( 0.175)\tLoss 9.4915e-01 (1.0185e+00)\tAcc@1  67.97 ( 69.94)\tAcc@5  96.88 ( 93.41)\n","Epoch: [36][390/391]\tTime  0.158 ( 0.175)\tLoss 1.0554e+00 (1.0231e+00)\tAcc@1  65.00 ( 69.85)\tAcc@5  96.25 ( 93.37)\n","==> Train Accuracy: Acc@1 69.846 || Acc@5 93.366\n","==> Test Accuracy:  Acc@1 58.280 || Acc@5 85.180\n","==> 72.88 seconds to train this epoch\n","\n","\n","----- epoch: 37, lr: 0.1 -----\n","Epoch: [37][  0/391]\tTime  0.249 ( 0.249)\tLoss 1.0030e+00 (1.0030e+00)\tAcc@1  72.66 ( 72.66)\tAcc@5  93.75 ( 93.75)\n","Epoch: [37][ 30/391]\tTime  0.175 ( 0.177)\tLoss 7.8341e-01 (8.6277e-01)\tAcc@1  71.09 ( 73.89)\tAcc@5  96.88 ( 94.98)\n","Epoch: [37][ 60/391]\tTime  0.175 ( 0.176)\tLoss 9.4306e-01 (9.0808e-01)\tAcc@1  73.44 ( 73.07)\tAcc@5  93.75 ( 94.30)\n","Epoch: [37][ 90/391]\tTime  0.175 ( 0.176)\tLoss 8.2149e-01 (9.3235e-01)\tAcc@1  80.47 ( 72.46)\tAcc@5  92.97 ( 93.99)\n","Epoch: [37][120/391]\tTime  0.175 ( 0.176)\tLoss 9.4242e-01 (9.5272e-01)\tAcc@1  73.44 ( 71.90)\tAcc@5  95.31 ( 93.83)\n","Epoch: [37][150/391]\tTime  0.175 ( 0.176)\tLoss 1.1149e+00 (9.6396e-01)\tAcc@1  70.31 ( 71.61)\tAcc@5  89.06 ( 93.79)\n","Epoch: [37][180/391]\tTime  0.175 ( 0.175)\tLoss 1.0335e+00 (9.7214e-01)\tAcc@1  70.31 ( 71.27)\tAcc@5  92.97 ( 93.70)\n","Epoch: [37][210/391]\tTime  0.176 ( 0.175)\tLoss 1.0520e+00 (9.8343e-01)\tAcc@1  67.19 ( 70.86)\tAcc@5  94.53 ( 93.56)\n","Epoch: [37][240/391]\tTime  0.175 ( 0.175)\tLoss 9.6719e-01 (9.9072e-01)\tAcc@1  70.31 ( 70.61)\tAcc@5  93.75 ( 93.54)\n","Epoch: [37][270/391]\tTime  0.175 ( 0.175)\tLoss 1.0540e+00 (9.9424e-01)\tAcc@1  71.09 ( 70.54)\tAcc@5  91.41 ( 93.44)\n","Epoch: [37][300/391]\tTime  0.174 ( 0.175)\tLoss 8.6410e-01 (9.9320e-01)\tAcc@1  75.00 ( 70.73)\tAcc@5  94.53 ( 93.44)\n","Epoch: [37][330/391]\tTime  0.175 ( 0.175)\tLoss 9.1996e-01 (9.9871e-01)\tAcc@1  74.22 ( 70.55)\tAcc@5  94.53 ( 93.32)\n","Epoch: [37][360/391]\tTime  0.175 ( 0.175)\tLoss 1.2310e+00 (1.0041e+00)\tAcc@1  65.62 ( 70.49)\tAcc@5  91.41 ( 93.26)\n","Epoch: [37][390/391]\tTime  0.160 ( 0.175)\tLoss 1.3038e+00 (1.0068e+00)\tAcc@1  66.25 ( 70.43)\tAcc@5  88.75 ( 93.22)\n","==> Train Accuracy: Acc@1 70.434 || Acc@5 93.218\n","==> Test Accuracy:  Acc@1 57.200 || Acc@5 85.560\n","==> 72.86 seconds to train this epoch\n","\n","\n","----- epoch: 38, lr: 0.1 -----\n","Epoch: [38][  0/391]\tTime  0.223 ( 0.223)\tLoss 8.8774e-01 (8.8774e-01)\tAcc@1  77.34 ( 77.34)\tAcc@5  92.97 ( 92.97)\n","Epoch: [38][ 30/391]\tTime  0.178 ( 0.176)\tLoss 9.2466e-01 (9.0553e-01)\tAcc@1  72.66 ( 72.73)\tAcc@5  96.88 ( 94.25)\n","Epoch: [38][ 60/391]\tTime  0.175 ( 0.176)\tLoss 9.2044e-01 (9.1203e-01)\tAcc@1  77.34 ( 72.84)\tAcc@5  91.41 ( 94.53)\n","Epoch: [38][ 90/391]\tTime  0.175 ( 0.176)\tLoss 1.1456e+00 (9.3265e-01)\tAcc@1  67.97 ( 72.14)\tAcc@5  91.41 ( 94.33)\n","Epoch: [38][120/391]\tTime  0.175 ( 0.175)\tLoss 1.0675e+00 (9.5165e-01)\tAcc@1  67.19 ( 71.64)\tAcc@5  94.53 ( 93.96)\n","Epoch: [38][150/391]\tTime  0.175 ( 0.175)\tLoss 1.0101e+00 (9.6717e-01)\tAcc@1  70.31 ( 71.11)\tAcc@5  93.75 ( 93.90)\n","Epoch: [38][180/391]\tTime  0.175 ( 0.175)\tLoss 1.0190e+00 (9.7895e-01)\tAcc@1  70.31 ( 70.79)\tAcc@5  91.41 ( 93.72)\n","Epoch: [38][210/391]\tTime  0.175 ( 0.175)\tLoss 1.2249e+00 (9.8838e-01)\tAcc@1  64.84 ( 70.50)\tAcc@5  91.41 ( 93.59)\n","Epoch: [38][240/391]\tTime  0.176 ( 0.175)\tLoss 9.4289e-01 (9.9090e-01)\tAcc@1  71.09 ( 70.50)\tAcc@5  96.88 ( 93.56)\n","Epoch: [38][270/391]\tTime  0.176 ( 0.175)\tLoss 1.0610e+00 (9.9508e-01)\tAcc@1  73.44 ( 70.37)\tAcc@5  91.41 ( 93.50)\n","Epoch: [38][300/391]\tTime  0.176 ( 0.175)\tLoss 1.1228e+00 (1.0014e+00)\tAcc@1  66.41 ( 70.21)\tAcc@5  93.75 ( 93.46)\n","Epoch: [38][330/391]\tTime  0.175 ( 0.175)\tLoss 1.2050e+00 (1.0033e+00)\tAcc@1  61.72 ( 70.21)\tAcc@5  89.84 ( 93.41)\n","Epoch: [38][360/391]\tTime  0.176 ( 0.175)\tLoss 9.5741e-01 (1.0052e+00)\tAcc@1  69.53 ( 70.10)\tAcc@5  92.97 ( 93.43)\n","Epoch: [38][390/391]\tTime  0.157 ( 0.175)\tLoss 1.0124e+00 (1.0071e+00)\tAcc@1  68.75 ( 70.08)\tAcc@5  93.75 ( 93.42)\n","==> Train Accuracy: Acc@1 70.084 || Acc@5 93.420\n","==> Test Accuracy:  Acc@1 58.580 || Acc@5 85.890\n","==> 72.82 seconds to train this epoch\n","\n","\n","----- epoch: 39, lr: 0.1 -----\n","Epoch: [39][  0/391]\tTime  0.210 ( 0.210)\tLoss 9.4805e-01 (9.4805e-01)\tAcc@1  71.09 ( 71.09)\tAcc@5  96.09 ( 96.09)\n","Epoch: [39][ 30/391]\tTime  0.173 ( 0.176)\tLoss 1.0375e+00 (9.1326e-01)\tAcc@1  67.97 ( 73.19)\tAcc@5  93.75 ( 94.56)\n","Epoch: [39][ 60/391]\tTime  0.176 ( 0.176)\tLoss 1.0554e+00 (9.1843e-01)\tAcc@1  74.22 ( 72.37)\tAcc@5  94.53 ( 94.68)\n","Epoch: [39][ 90/391]\tTime  0.176 ( 0.175)\tLoss 1.0995e+00 (9.3687e-01)\tAcc@1  67.19 ( 72.14)\tAcc@5  92.19 ( 94.26)\n","Epoch: [39][120/391]\tTime  0.175 ( 0.175)\tLoss 1.0132e+00 (9.5830e-01)\tAcc@1  70.31 ( 71.54)\tAcc@5  95.31 ( 94.07)\n","Epoch: [39][150/391]\tTime  0.176 ( 0.175)\tLoss 1.0089e+00 (9.6799e-01)\tAcc@1  67.97 ( 71.07)\tAcc@5  92.97 ( 94.03)\n","Epoch: [39][180/391]\tTime  0.175 ( 0.175)\tLoss 9.4494e-01 (9.6558e-01)\tAcc@1  73.44 ( 71.16)\tAcc@5  94.53 ( 94.09)\n","Epoch: [39][210/391]\tTime  0.174 ( 0.175)\tLoss 8.0041e-01 (9.7568e-01)\tAcc@1  75.00 ( 70.90)\tAcc@5  94.53 ( 93.95)\n","Epoch: [39][240/391]\tTime  0.175 ( 0.175)\tLoss 1.0332e+00 (9.8336e-01)\tAcc@1  67.19 ( 70.68)\tAcc@5  93.75 ( 93.85)\n","Epoch: [39][270/391]\tTime  0.176 ( 0.175)\tLoss 9.7237e-01 (9.8773e-01)\tAcc@1  75.00 ( 70.65)\tAcc@5  92.19 ( 93.78)\n","Epoch: [39][300/391]\tTime  0.177 ( 0.175)\tLoss 1.0212e+00 (9.9365e-01)\tAcc@1  67.19 ( 70.48)\tAcc@5  95.31 ( 93.71)\n","Epoch: [39][330/391]\tTime  0.175 ( 0.175)\tLoss 9.7743e-01 (9.9823e-01)\tAcc@1  75.78 ( 70.42)\tAcc@5  92.19 ( 93.67)\n","Epoch: [39][360/391]\tTime  0.175 ( 0.175)\tLoss 1.0321e+00 (1.0006e+00)\tAcc@1  66.41 ( 70.31)\tAcc@5  92.97 ( 93.69)\n","Epoch: [39][390/391]\tTime  0.159 ( 0.175)\tLoss 1.2291e+00 (1.0044e+00)\tAcc@1  62.50 ( 70.19)\tAcc@5  93.75 ( 93.63)\n","==> Train Accuracy: Acc@1 70.186 || Acc@5 93.626\n","==> Test Accuracy:  Acc@1 58.730 || Acc@5 86.190\n","==> 72.85 seconds to train this epoch\n","\n","\n","----- epoch: 40, lr: 0.1 -----\n","Epoch: [40][  0/391]\tTime  0.215 ( 0.215)\tLoss 8.9191e-01 (8.9191e-01)\tAcc@1  72.66 ( 72.66)\tAcc@5  92.97 ( 92.97)\n","Epoch: [40][ 30/391]\tTime  0.176 ( 0.176)\tLoss 1.1374e+00 (9.0495e-01)\tAcc@1  67.19 ( 73.06)\tAcc@5  89.06 ( 94.73)\n","Epoch: [40][ 60/391]\tTime  0.174 ( 0.176)\tLoss 7.4024e-01 (9.3645e-01)\tAcc@1  82.03 ( 72.20)\tAcc@5  95.31 ( 94.16)\n","Epoch: [40][ 90/391]\tTime  0.175 ( 0.176)\tLoss 9.9184e-01 (9.5186e-01)\tAcc@1  71.88 ( 71.82)\tAcc@5  93.75 ( 93.91)\n","Epoch: [40][120/391]\tTime  0.174 ( 0.175)\tLoss 7.6341e-01 (9.4424e-01)\tAcc@1  78.12 ( 72.07)\tAcc@5  92.19 ( 94.07)\n","Epoch: [40][150/391]\tTime  0.175 ( 0.175)\tLoss 8.8290e-01 (9.4719e-01)\tAcc@1  78.12 ( 72.09)\tAcc@5  92.97 ( 94.02)\n","Epoch: [40][180/391]\tTime  0.170 ( 0.175)\tLoss 1.0590e+00 (9.4949e-01)\tAcc@1  65.62 ( 71.86)\tAcc@5  89.84 ( 94.05)\n","Epoch: [40][210/391]\tTime  0.177 ( 0.175)\tLoss 1.0971e+00 (9.5715e-01)\tAcc@1  69.53 ( 71.69)\tAcc@5  93.75 ( 93.95)\n","Epoch: [40][240/391]\tTime  0.174 ( 0.175)\tLoss 1.2350e+00 (9.6151e-01)\tAcc@1  62.50 ( 71.50)\tAcc@5  92.97 ( 93.92)\n","Epoch: [40][270/391]\tTime  0.175 ( 0.175)\tLoss 1.0727e+00 (9.7267e-01)\tAcc@1  66.41 ( 71.21)\tAcc@5  93.75 ( 93.81)\n","Epoch: [40][300/391]\tTime  0.177 ( 0.175)\tLoss 1.1895e+00 (9.8175e-01)\tAcc@1  66.41 ( 71.05)\tAcc@5  87.50 ( 93.65)\n","Epoch: [40][330/391]\tTime  0.177 ( 0.175)\tLoss 1.0541e+00 (9.8614e-01)\tAcc@1  65.62 ( 70.99)\tAcc@5  96.09 ( 93.58)\n","Epoch: [40][360/391]\tTime  0.176 ( 0.175)\tLoss 8.7812e-01 (9.9034e-01)\tAcc@1  78.12 ( 70.91)\tAcc@5  95.31 ( 93.55)\n","Epoch: [40][390/391]\tTime  0.157 ( 0.175)\tLoss 8.8819e-01 (9.9257e-01)\tAcc@1  70.00 ( 70.80)\tAcc@5  96.25 ( 93.55)\n","==> Train Accuracy: Acc@1 70.796 || Acc@5 93.554\n","==> Test Accuracy:  Acc@1 57.690 || Acc@5 85.230\n","==> 72.85 seconds to train this epoch\n","\n","\n","----- epoch: 41, lr: 0.1 -----\n","Epoch: [41][  0/391]\tTime  0.216 ( 0.216)\tLoss 7.7902e-01 (7.7902e-01)\tAcc@1  74.22 ( 74.22)\tAcc@5  96.09 ( 96.09)\n","Epoch: [41][ 30/391]\tTime  0.174 ( 0.176)\tLoss 1.1459e+00 (8.9927e-01)\tAcc@1  67.97 ( 72.63)\tAcc@5  92.19 ( 94.98)\n","Epoch: [41][ 60/391]\tTime  0.176 ( 0.176)\tLoss 8.7127e-01 (9.0589e-01)\tAcc@1  75.78 ( 72.98)\tAcc@5  93.75 ( 94.94)\n","Epoch: [41][ 90/391]\tTime  0.176 ( 0.176)\tLoss 8.7057e-01 (9.1815e-01)\tAcc@1  73.44 ( 72.60)\tAcc@5  93.75 ( 94.84)\n","Epoch: [41][120/391]\tTime  0.175 ( 0.175)\tLoss 8.9276e-01 (9.2967e-01)\tAcc@1  73.44 ( 72.32)\tAcc@5  94.53 ( 94.65)\n","Epoch: [41][150/391]\tTime  0.176 ( 0.175)\tLoss 1.0282e+00 (9.4890e-01)\tAcc@1  68.75 ( 71.91)\tAcc@5  92.19 ( 94.30)\n","Epoch: [41][180/391]\tTime  0.179 ( 0.175)\tLoss 9.4851e-01 (9.6219e-01)\tAcc@1  75.00 ( 71.56)\tAcc@5  92.97 ( 94.11)\n","Epoch: [41][210/391]\tTime  0.176 ( 0.175)\tLoss 1.0556e+00 (9.7047e-01)\tAcc@1  69.53 ( 71.26)\tAcc@5  91.41 ( 93.99)\n","Epoch: [41][240/391]\tTime  0.175 ( 0.175)\tLoss 1.0261e+00 (9.7392e-01)\tAcc@1  68.75 ( 71.21)\tAcc@5  92.97 ( 93.92)\n","Epoch: [41][270/391]\tTime  0.174 ( 0.175)\tLoss 9.1002e-01 (9.7641e-01)\tAcc@1  69.53 ( 71.09)\tAcc@5  93.75 ( 93.93)\n","Epoch: [41][300/391]\tTime  0.175 ( 0.175)\tLoss 1.0037e+00 (9.8530e-01)\tAcc@1  69.53 ( 70.80)\tAcc@5  94.53 ( 93.76)\n","Epoch: [41][330/391]\tTime  0.174 ( 0.175)\tLoss 8.0541e-01 (9.8718e-01)\tAcc@1  81.25 ( 70.75)\tAcc@5  93.75 ( 93.72)\n","Epoch: [41][360/391]\tTime  0.171 ( 0.175)\tLoss 1.0938e+00 (9.9274e-01)\tAcc@1  67.97 ( 70.62)\tAcc@5  89.84 ( 93.62)\n","Epoch: [41][390/391]\tTime  0.157 ( 0.175)\tLoss 1.4137e+00 (9.9878e-01)\tAcc@1  61.25 ( 70.51)\tAcc@5  88.75 ( 93.51)\n","==> Train Accuracy: Acc@1 70.506 || Acc@5 93.506\n","==> Test Accuracy:  Acc@1 61.960 || Acc@5 87.500\n","==> 72.88 seconds to train this epoch\n","\n","\n","----- epoch: 42, lr: 0.1 -----\n","Epoch: [42][  0/391]\tTime  0.231 ( 0.231)\tLoss 9.1651e-01 (9.1651e-01)\tAcc@1  76.56 ( 76.56)\tAcc@5  91.41 ( 91.41)\n","Epoch: [42][ 30/391]\tTime  0.176 ( 0.176)\tLoss 1.0202e+00 (9.0018e-01)\tAcc@1  66.41 ( 72.96)\tAcc@5  94.53 ( 94.73)\n","Epoch: [42][ 60/391]\tTime  0.173 ( 0.176)\tLoss 9.3113e-01 (8.9615e-01)\tAcc@1  66.41 ( 72.76)\tAcc@5  96.88 ( 94.83)\n","Epoch: [42][ 90/391]\tTime  0.176 ( 0.176)\tLoss 8.5400e-01 (9.1953e-01)\tAcc@1  75.78 ( 72.41)\tAcc@5  95.31 ( 94.51)\n","Epoch: [42][120/391]\tTime  0.174 ( 0.176)\tLoss 6.5864e-01 (9.2134e-01)\tAcc@1  81.25 ( 72.49)\tAcc@5  95.31 ( 94.47)\n","Epoch: [42][150/391]\tTime  0.180 ( 0.176)\tLoss 7.1926e-01 (9.3470e-01)\tAcc@1  80.47 ( 72.05)\tAcc@5  99.22 ( 94.34)\n","Epoch: [42][180/391]\tTime  0.171 ( 0.176)\tLoss 1.0813e+00 (9.4793e-01)\tAcc@1  71.09 ( 71.73)\tAcc@5  88.28 ( 94.14)\n","Epoch: [42][210/391]\tTime  0.176 ( 0.176)\tLoss 8.6914e-01 (9.5366e-01)\tAcc@1  75.78 ( 71.67)\tAcc@5  92.97 ( 94.03)\n","Epoch: [42][240/391]\tTime  0.173 ( 0.176)\tLoss 8.9506e-01 (9.5899e-01)\tAcc@1  71.88 ( 71.59)\tAcc@5  96.09 ( 93.95)\n","Epoch: [42][270/391]\tTime  0.175 ( 0.176)\tLoss 9.9162e-01 (9.6367e-01)\tAcc@1  71.09 ( 71.45)\tAcc@5  92.97 ( 93.91)\n","Epoch: [42][300/391]\tTime  0.176 ( 0.176)\tLoss 8.5921e-01 (9.6945e-01)\tAcc@1  75.00 ( 71.32)\tAcc@5  96.88 ( 93.81)\n","Epoch: [42][330/391]\tTime  0.175 ( 0.176)\tLoss 1.0609e+00 (9.7425e-01)\tAcc@1  64.84 ( 71.20)\tAcc@5  95.31 ( 93.73)\n","Epoch: [42][360/391]\tTime  0.175 ( 0.176)\tLoss 9.7397e-01 (9.8194e-01)\tAcc@1  71.88 ( 71.01)\tAcc@5  93.75 ( 93.69)\n","Epoch: [42][390/391]\tTime  0.157 ( 0.176)\tLoss 1.2713e+00 (9.8175e-01)\tAcc@1  62.50 ( 71.01)\tAcc@5  92.50 ( 93.69)\n","==> Train Accuracy: Acc@1 71.008 || Acc@5 93.694\n","==> Test Accuracy:  Acc@1 59.630 || Acc@5 86.860\n","==> 72.96 seconds to train this epoch\n","\n","\n","----- epoch: 43, lr: 0.1 -----\n","Epoch: [43][  0/391]\tTime  0.218 ( 0.218)\tLoss 9.7738e-01 (9.7738e-01)\tAcc@1  72.66 ( 72.66)\tAcc@5  96.88 ( 96.88)\n","Epoch: [43][ 30/391]\tTime  0.175 ( 0.176)\tLoss 8.7173e-01 (9.1677e-01)\tAcc@1  72.66 ( 72.10)\tAcc@5  92.19 ( 94.71)\n","Epoch: [43][ 60/391]\tTime  0.175 ( 0.176)\tLoss 7.6205e-01 (9.1536e-01)\tAcc@1  74.22 ( 72.71)\tAcc@5  95.31 ( 94.52)\n","Epoch: [43][ 90/391]\tTime  0.177 ( 0.176)\tLoss 9.5825e-01 (9.1893e-01)\tAcc@1  70.31 ( 72.76)\tAcc@5  94.53 ( 94.45)\n","Epoch: [43][120/391]\tTime  0.174 ( 0.176)\tLoss 9.7117e-01 (9.3642e-01)\tAcc@1  71.09 ( 72.24)\tAcc@5  95.31 ( 94.15)\n","Epoch: [43][150/391]\tTime  0.174 ( 0.176)\tLoss 9.3790e-01 (9.4094e-01)\tAcc@1  73.44 ( 72.16)\tAcc@5  95.31 ( 94.04)\n","Epoch: [43][180/391]\tTime  0.174 ( 0.176)\tLoss 9.6591e-01 (9.4834e-01)\tAcc@1  73.44 ( 71.92)\tAcc@5  92.19 ( 94.06)\n","Epoch: [43][210/391]\tTime  0.175 ( 0.175)\tLoss 9.8495e-01 (9.5784e-01)\tAcc@1  71.09 ( 71.69)\tAcc@5  91.41 ( 93.98)\n","Epoch: [43][240/391]\tTime  0.174 ( 0.175)\tLoss 1.1813e+00 (9.6926e-01)\tAcc@1  67.19 ( 71.39)\tAcc@5  90.62 ( 93.87)\n","Epoch: [43][270/391]\tTime  0.174 ( 0.175)\tLoss 9.4864e-01 (9.7236e-01)\tAcc@1  71.09 ( 71.27)\tAcc@5  92.97 ( 93.88)\n","Epoch: [43][300/391]\tTime  0.175 ( 0.175)\tLoss 1.0061e+00 (9.7915e-01)\tAcc@1  71.88 ( 71.06)\tAcc@5  93.75 ( 93.78)\n","Epoch: [43][330/391]\tTime  0.178 ( 0.175)\tLoss 1.0458e+00 (9.8208e-01)\tAcc@1  71.09 ( 71.00)\tAcc@5  92.19 ( 93.78)\n","Epoch: [43][360/391]\tTime  0.173 ( 0.175)\tLoss 1.3379e+00 (9.8635e-01)\tAcc@1  64.84 ( 70.91)\tAcc@5  89.84 ( 93.72)\n","Epoch: [43][390/391]\tTime  0.157 ( 0.175)\tLoss 1.0295e+00 (9.8967e-01)\tAcc@1  68.75 ( 70.83)\tAcc@5  93.75 ( 93.67)\n","==> Train Accuracy: Acc@1 70.834 || Acc@5 93.668\n","==> Test Accuracy:  Acc@1 60.980 || Acc@5 86.670\n","==> 72.87 seconds to train this epoch\n","\n","\n","----- epoch: 44, lr: 0.1 -----\n","Epoch: [44][  0/391]\tTime  0.234 ( 0.234)\tLoss 9.5692e-01 (9.5692e-01)\tAcc@1  72.66 ( 72.66)\tAcc@5  93.75 ( 93.75)\n","Epoch: [44][ 30/391]\tTime  0.179 ( 0.177)\tLoss 8.1062e-01 (9.3349e-01)\tAcc@1  75.78 ( 72.68)\tAcc@5  99.22 ( 94.08)\n","Epoch: [44][ 60/391]\tTime  0.170 ( 0.176)\tLoss 1.1021e+00 (9.2046e-01)\tAcc@1  65.62 ( 72.64)\tAcc@5  95.31 ( 94.57)\n","Epoch: [44][ 90/391]\tTime  0.176 ( 0.176)\tLoss 7.3651e-01 (9.2046e-01)\tAcc@1  80.47 ( 72.60)\tAcc@5  95.31 ( 94.59)\n","Epoch: [44][120/391]\tTime  0.175 ( 0.176)\tLoss 1.1473e+00 (9.2507e-01)\tAcc@1  64.06 ( 72.34)\tAcc@5  91.41 ( 94.49)\n","Epoch: [44][150/391]\tTime  0.175 ( 0.176)\tLoss 1.0053e+00 (9.3200e-01)\tAcc@1  69.53 ( 72.20)\tAcc@5  97.66 ( 94.49)\n","Epoch: [44][180/391]\tTime  0.171 ( 0.176)\tLoss 9.3266e-01 (9.3643e-01)\tAcc@1  71.88 ( 72.16)\tAcc@5  94.53 ( 94.41)\n","Epoch: [44][210/391]\tTime  0.176 ( 0.176)\tLoss 1.0213e+00 (9.4116e-01)\tAcc@1  72.66 ( 71.99)\tAcc@5  92.97 ( 94.35)\n","Epoch: [44][240/391]\tTime  0.174 ( 0.176)\tLoss 1.1783e+00 (9.5867e-01)\tAcc@1  62.50 ( 71.50)\tAcc@5  89.06 ( 94.13)\n","Epoch: [44][270/391]\tTime  0.174 ( 0.175)\tLoss 1.4558e+00 (9.6463e-01)\tAcc@1  55.47 ( 71.38)\tAcc@5  88.28 ( 94.02)\n","Epoch: [44][300/391]\tTime  0.175 ( 0.175)\tLoss 8.8079e-01 (9.7184e-01)\tAcc@1  73.44 ( 71.17)\tAcc@5  95.31 ( 93.93)\n","Epoch: [44][330/391]\tTime  0.176 ( 0.175)\tLoss 8.9956e-01 (9.7459e-01)\tAcc@1  71.88 ( 71.14)\tAcc@5  97.66 ( 93.91)\n","Epoch: [44][360/391]\tTime  0.174 ( 0.175)\tLoss 9.3529e-01 (9.7925e-01)\tAcc@1  69.53 ( 71.00)\tAcc@5  97.66 ( 93.86)\n","Epoch: [44][390/391]\tTime  0.157 ( 0.175)\tLoss 1.0462e+00 (9.8372e-01)\tAcc@1  70.00 ( 70.93)\tAcc@5  88.75 ( 93.80)\n","==> Train Accuracy: Acc@1 70.928 || Acc@5 93.796\n","==> Test Accuracy:  Acc@1 58.060 || Acc@5 85.880\n","==> 72.86 seconds to train this epoch\n","\n","\n","----- epoch: 45, lr: 0.1 -----\n","Epoch: [45][  0/391]\tTime  0.230 ( 0.230)\tLoss 9.2315e-01 (9.2315e-01)\tAcc@1  74.22 ( 74.22)\tAcc@5  96.09 ( 96.09)\n","Epoch: [45][ 30/391]\tTime  0.175 ( 0.176)\tLoss 6.8444e-01 (9.1157e-01)\tAcc@1  77.34 ( 72.81)\tAcc@5  97.66 ( 94.76)\n","Epoch: [45][ 60/391]\tTime  0.175 ( 0.176)\tLoss 9.5871e-01 (9.0364e-01)\tAcc@1  74.22 ( 73.03)\tAcc@5  94.53 ( 94.88)\n","Epoch: [45][ 90/391]\tTime  0.175 ( 0.175)\tLoss 1.0124e+00 (9.1749e-01)\tAcc@1  64.06 ( 72.62)\tAcc@5  93.75 ( 94.53)\n","Epoch: [45][120/391]\tTime  0.173 ( 0.175)\tLoss 1.0557e+00 (9.2657e-01)\tAcc@1  69.53 ( 72.39)\tAcc@5  91.41 ( 94.35)\n","Epoch: [45][150/391]\tTime  0.175 ( 0.175)\tLoss 1.0123e+00 (9.3414e-01)\tAcc@1  71.09 ( 72.29)\tAcc@5  89.84 ( 94.06)\n","Epoch: [45][180/391]\tTime  0.175 ( 0.175)\tLoss 8.8277e-01 (9.3981e-01)\tAcc@1  73.44 ( 72.05)\tAcc@5  93.75 ( 93.94)\n","Epoch: [45][210/391]\tTime  0.176 ( 0.175)\tLoss 8.7701e-01 (9.4289e-01)\tAcc@1  71.88 ( 71.92)\tAcc@5  95.31 ( 94.05)\n","Epoch: [45][240/391]\tTime  0.176 ( 0.175)\tLoss 9.6579e-01 (9.5320e-01)\tAcc@1  67.97 ( 71.74)\tAcc@5  95.31 ( 93.93)\n","Epoch: [45][270/391]\tTime  0.174 ( 0.175)\tLoss 1.1267e+00 (9.5995e-01)\tAcc@1  67.19 ( 71.61)\tAcc@5  90.62 ( 93.87)\n","Epoch: [45][300/391]\tTime  0.176 ( 0.175)\tLoss 7.9305e-01 (9.6634e-01)\tAcc@1  74.22 ( 71.43)\tAcc@5  98.44 ( 93.81)\n","Epoch: [45][330/391]\tTime  0.174 ( 0.175)\tLoss 1.1171e+00 (9.6971e-01)\tAcc@1  67.19 ( 71.37)\tAcc@5  93.75 ( 93.79)\n","Epoch: [45][360/391]\tTime  0.173 ( 0.175)\tLoss 9.6137e-01 (9.7609e-01)\tAcc@1  71.09 ( 71.15)\tAcc@5  94.53 ( 93.75)\n","Epoch: [45][390/391]\tTime  0.159 ( 0.175)\tLoss 1.1788e+00 (9.8001e-01)\tAcc@1  68.75 ( 71.02)\tAcc@5  92.50 ( 93.71)\n","==> Train Accuracy: Acc@1 71.024 || Acc@5 93.712\n","==> Test Accuracy:  Acc@1 60.220 || Acc@5 87.690\n","==> 72.79 seconds to train this epoch\n","\n","\n","----- epoch: 46, lr: 0.1 -----\n","Epoch: [46][  0/391]\tTime  0.222 ( 0.222)\tLoss 8.6057e-01 (8.6057e-01)\tAcc@1  75.78 ( 75.78)\tAcc@5  95.31 ( 95.31)\n","Epoch: [46][ 30/391]\tTime  0.174 ( 0.176)\tLoss 1.0147e+00 (8.7536e-01)\tAcc@1  70.31 ( 74.40)\tAcc@5  95.31 ( 94.83)\n","Epoch: [46][ 60/391]\tTime  0.174 ( 0.176)\tLoss 8.3109e-01 (8.6978e-01)\tAcc@1  78.12 ( 74.49)\tAcc@5  96.09 ( 95.02)\n","Epoch: [46][ 90/391]\tTime  0.174 ( 0.175)\tLoss 7.7389e-01 (8.9237e-01)\tAcc@1  78.12 ( 73.83)\tAcc@5  95.31 ( 94.73)\n","Epoch: [46][120/391]\tTime  0.175 ( 0.175)\tLoss 1.0778e+00 (9.1331e-01)\tAcc@1  70.31 ( 73.20)\tAcc@5  93.75 ( 94.55)\n","Epoch: [46][150/391]\tTime  0.175 ( 0.175)\tLoss 9.8007e-01 (9.2554e-01)\tAcc@1  68.75 ( 72.87)\tAcc@5  97.66 ( 94.47)\n","Epoch: [46][180/391]\tTime  0.176 ( 0.175)\tLoss 8.9451e-01 (9.4056e-01)\tAcc@1  77.34 ( 72.34)\tAcc@5  96.09 ( 94.32)\n","Epoch: [46][210/391]\tTime  0.175 ( 0.175)\tLoss 1.0553e+00 (9.5439e-01)\tAcc@1  70.31 ( 71.92)\tAcc@5  93.75 ( 94.16)\n","Epoch: [46][240/391]\tTime  0.174 ( 0.175)\tLoss 8.5024e-01 (9.5739e-01)\tAcc@1  78.91 ( 71.87)\tAcc@5  96.09 ( 94.14)\n","Epoch: [46][270/391]\tTime  0.175 ( 0.175)\tLoss 1.0285e+00 (9.6130e-01)\tAcc@1  68.75 ( 71.73)\tAcc@5  93.75 ( 94.13)\n","Epoch: [46][300/391]\tTime  0.175 ( 0.175)\tLoss 8.1925e-01 (9.6714e-01)\tAcc@1  73.44 ( 71.57)\tAcc@5  95.31 ( 94.00)\n","Epoch: [46][330/391]\tTime  0.175 ( 0.175)\tLoss 9.5091e-01 (9.7043e-01)\tAcc@1  69.53 ( 71.47)\tAcc@5  93.75 ( 93.94)\n","Epoch: [46][360/391]\tTime  0.174 ( 0.175)\tLoss 9.7946e-01 (9.7823e-01)\tAcc@1  70.31 ( 71.25)\tAcc@5  94.53 ( 93.85)\n","Epoch: [46][390/391]\tTime  0.158 ( 0.175)\tLoss 1.1870e+00 (9.8325e-01)\tAcc@1  63.75 ( 71.14)\tAcc@5  91.25 ( 93.77)\n","==> Train Accuracy: Acc@1 71.140 || Acc@5 93.772\n","==> Test Accuracy:  Acc@1 57.610 || Acc@5 84.950\n","==> 72.72 seconds to train this epoch\n","\n","\n","----- epoch: 47, lr: 0.1 -----\n","Epoch: [47][  0/391]\tTime  0.217 ( 0.217)\tLoss 9.3786e-01 (9.3786e-01)\tAcc@1  73.44 ( 73.44)\tAcc@5  94.53 ( 94.53)\n","Epoch: [47][ 30/391]\tTime  0.175 ( 0.176)\tLoss 1.0791e+00 (9.2065e-01)\tAcc@1  66.41 ( 73.39)\tAcc@5  94.53 ( 95.06)\n","Epoch: [47][ 60/391]\tTime  0.174 ( 0.175)\tLoss 1.1105e+00 (9.0824e-01)\tAcc@1  67.97 ( 73.07)\tAcc@5  93.75 ( 95.07)\n","Epoch: [47][ 90/391]\tTime  0.175 ( 0.175)\tLoss 1.0118e+00 (8.9625e-01)\tAcc@1  66.41 ( 73.39)\tAcc@5  92.97 ( 94.85)\n","Epoch: [47][120/391]\tTime  0.174 ( 0.175)\tLoss 9.6816e-01 (9.0660e-01)\tAcc@1  68.75 ( 73.06)\tAcc@5  94.53 ( 94.75)\n","Epoch: [47][150/391]\tTime  0.174 ( 0.175)\tLoss 1.2089e+00 (9.1193e-01)\tAcc@1  62.50 ( 72.97)\tAcc@5  90.62 ( 94.62)\n","Epoch: [47][180/391]\tTime  0.175 ( 0.175)\tLoss 7.6814e-01 (9.2176e-01)\tAcc@1  76.56 ( 72.61)\tAcc@5  95.31 ( 94.45)\n","Epoch: [47][210/391]\tTime  0.173 ( 0.175)\tLoss 1.0155e+00 (9.2912e-01)\tAcc@1  75.00 ( 72.56)\tAcc@5  92.97 ( 94.37)\n","Epoch: [47][240/391]\tTime  0.173 ( 0.175)\tLoss 1.2856e+00 (9.3946e-01)\tAcc@1  63.28 ( 72.27)\tAcc@5  89.84 ( 94.25)\n","Epoch: [47][270/391]\tTime  0.174 ( 0.175)\tLoss 1.0155e+00 (9.5067e-01)\tAcc@1  70.31 ( 71.98)\tAcc@5  92.19 ( 94.11)\n","Epoch: [47][300/391]\tTime  0.175 ( 0.175)\tLoss 1.0043e+00 (9.6236e-01)\tAcc@1  64.84 ( 71.71)\tAcc@5  95.31 ( 93.97)\n","Epoch: [47][330/391]\tTime  0.174 ( 0.175)\tLoss 1.0642e+00 (9.6760e-01)\tAcc@1  71.88 ( 71.59)\tAcc@5  94.53 ( 93.91)\n","Epoch: [47][360/391]\tTime  0.175 ( 0.175)\tLoss 8.0148e-01 (9.6585e-01)\tAcc@1  75.78 ( 71.61)\tAcc@5  96.88 ( 93.95)\n","Epoch: [47][390/391]\tTime  0.157 ( 0.175)\tLoss 1.0395e+00 (9.7283e-01)\tAcc@1  72.50 ( 71.46)\tAcc@5  95.00 ( 93.88)\n","==> Train Accuracy: Acc@1 71.456 || Acc@5 93.882\n","==> Test Accuracy:  Acc@1 59.260 || Acc@5 86.600\n","==> 72.63 seconds to train this epoch\n","\n","\n","----- epoch: 48, lr: 0.1 -----\n","Epoch: [48][  0/391]\tTime  0.220 ( 0.220)\tLoss 8.2399e-01 (8.2399e-01)\tAcc@1  73.44 ( 73.44)\tAcc@5  95.31 ( 95.31)\n","Epoch: [48][ 30/391]\tTime  0.175 ( 0.176)\tLoss 6.1633e-01 (8.6247e-01)\tAcc@1  82.03 ( 74.92)\tAcc@5  99.22 ( 95.14)\n","Epoch: [48][ 60/391]\tTime  0.174 ( 0.175)\tLoss 1.1172e+00 (8.9423e-01)\tAcc@1  64.06 ( 73.55)\tAcc@5  92.19 ( 95.03)\n","Epoch: [48][ 90/391]\tTime  0.173 ( 0.175)\tLoss 8.8834e-01 (8.9693e-01)\tAcc@1  78.91 ( 73.52)\tAcc@5  93.75 ( 94.81)\n","Epoch: [48][120/391]\tTime  0.175 ( 0.175)\tLoss 1.0498e+00 (9.0362e-01)\tAcc@1  71.09 ( 73.24)\tAcc@5  90.62 ( 94.70)\n","Epoch: [48][150/391]\tTime  0.175 ( 0.175)\tLoss 9.1310e-01 (9.2729e-01)\tAcc@1  71.88 ( 72.65)\tAcc@5  92.19 ( 94.46)\n","Epoch: [48][180/391]\tTime  0.175 ( 0.175)\tLoss 9.0546e-01 (9.3324e-01)\tAcc@1  72.66 ( 72.44)\tAcc@5  96.09 ( 94.32)\n","Epoch: [48][210/391]\tTime  0.176 ( 0.175)\tLoss 9.9388e-01 (9.4446e-01)\tAcc@1  71.88 ( 72.15)\tAcc@5  92.97 ( 94.13)\n","Epoch: [48][240/391]\tTime  0.175 ( 0.175)\tLoss 1.1216e+00 (9.5018e-01)\tAcc@1  64.84 ( 72.06)\tAcc@5  94.53 ( 94.13)\n","Epoch: [48][270/391]\tTime  0.174 ( 0.175)\tLoss 8.6236e-01 (9.5530e-01)\tAcc@1  75.00 ( 71.87)\tAcc@5  95.31 ( 94.04)\n","Epoch: [48][300/391]\tTime  0.175 ( 0.175)\tLoss 9.9289e-01 (9.6078e-01)\tAcc@1  71.09 ( 71.67)\tAcc@5  94.53 ( 93.95)\n","Epoch: [48][330/391]\tTime  0.175 ( 0.175)\tLoss 9.8647e-01 (9.6269e-01)\tAcc@1  64.06 ( 71.59)\tAcc@5  96.09 ( 93.95)\n","Epoch: [48][360/391]\tTime  0.173 ( 0.175)\tLoss 9.1956e-01 (9.6669e-01)\tAcc@1  75.78 ( 71.53)\tAcc@5  92.97 ( 93.88)\n","Epoch: [48][390/391]\tTime  0.159 ( 0.175)\tLoss 1.2044e+00 (9.7379e-01)\tAcc@1  63.75 ( 71.36)\tAcc@5  93.75 ( 93.77)\n","==> Train Accuracy: Acc@1 71.364 || Acc@5 93.770\n","==> Test Accuracy:  Acc@1 52.780 || Acc@5 81.130\n","==> 72.71 seconds to train this epoch\n","\n","\n","----- epoch: 49, lr: 0.1 -----\n","Epoch: [49][  0/391]\tTime  0.241 ( 0.241)\tLoss 9.9751e-01 (9.9751e-01)\tAcc@1  68.75 ( 68.75)\tAcc@5  95.31 ( 95.31)\n","Epoch: [49][ 30/391]\tTime  0.175 ( 0.177)\tLoss 8.4187e-01 (8.6651e-01)\tAcc@1  71.09 ( 74.04)\tAcc@5  96.88 ( 94.81)\n","Epoch: [49][ 60/391]\tTime  0.175 ( 0.176)\tLoss 9.4950e-01 (8.8206e-01)\tAcc@1  71.88 ( 73.95)\tAcc@5  96.09 ( 94.70)\n","Epoch: [49][ 90/391]\tTime  0.175 ( 0.176)\tLoss 9.0466e-01 (8.8947e-01)\tAcc@1  72.66 ( 73.35)\tAcc@5  93.75 ( 94.67)\n","Epoch: [49][120/391]\tTime  0.174 ( 0.175)\tLoss 8.5304e-01 (9.0490e-01)\tAcc@1  74.22 ( 72.92)\tAcc@5  95.31 ( 94.60)\n","Epoch: [49][150/391]\tTime  0.174 ( 0.175)\tLoss 1.0006e+00 (9.0908e-01)\tAcc@1  75.00 ( 72.96)\tAcc@5  91.41 ( 94.55)\n","Epoch: [49][180/391]\tTime  0.176 ( 0.175)\tLoss 1.0071e+00 (9.1840e-01)\tAcc@1  70.31 ( 72.62)\tAcc@5  95.31 ( 94.44)\n","Epoch: [49][210/391]\tTime  0.175 ( 0.175)\tLoss 7.5931e-01 (9.3342e-01)\tAcc@1  79.69 ( 72.25)\tAcc@5  93.75 ( 94.29)\n","Epoch: [49][240/391]\tTime  0.175 ( 0.175)\tLoss 1.0194e+00 (9.3799e-01)\tAcc@1  71.88 ( 72.22)\tAcc@5  91.41 ( 94.18)\n","Epoch: [49][270/391]\tTime  0.175 ( 0.175)\tLoss 7.4885e-01 (9.4336e-01)\tAcc@1  78.91 ( 72.04)\tAcc@5  94.53 ( 94.10)\n","Epoch: [49][300/391]\tTime  0.176 ( 0.175)\tLoss 9.7457e-01 (9.4484e-01)\tAcc@1  67.19 ( 72.03)\tAcc@5  96.88 ( 94.15)\n","Epoch: [49][330/391]\tTime  0.174 ( 0.175)\tLoss 1.0314e+00 (9.5200e-01)\tAcc@1  74.22 ( 71.83)\tAcc@5  92.97 ( 94.08)\n","Epoch: [49][360/391]\tTime  0.175 ( 0.175)\tLoss 9.3626e-01 (9.5766e-01)\tAcc@1  71.09 ( 71.67)\tAcc@5  96.09 ( 94.04)\n","Epoch: [49][390/391]\tTime  0.157 ( 0.175)\tLoss 1.0557e+00 (9.6343e-01)\tAcc@1  75.00 ( 71.50)\tAcc@5  95.00 ( 93.96)\n","==> Train Accuracy: Acc@1 71.496 || Acc@5 93.960\n","==> Test Accuracy:  Acc@1 59.760 || Acc@5 85.910\n","==> 72.81 seconds to train this epoch\n","\n","\n","----- epoch: 50, lr: 0.1 -----\n","Epoch: [50][  0/391]\tTime  0.225 ( 0.225)\tLoss 9.1025e-01 (9.1025e-01)\tAcc@1  72.66 ( 72.66)\tAcc@5  93.75 ( 93.75)\n","Epoch: [50][ 30/391]\tTime  0.175 ( 0.176)\tLoss 1.0379e+00 (8.8120e-01)\tAcc@1  73.44 ( 74.29)\tAcc@5  92.97 ( 94.96)\n","Epoch: [50][ 60/391]\tTime  0.173 ( 0.176)\tLoss 8.0591e-01 (8.8265e-01)\tAcc@1  74.22 ( 73.92)\tAcc@5  97.66 ( 94.99)\n","Epoch: [50][ 90/391]\tTime  0.175 ( 0.176)\tLoss 8.1761e-01 (8.9372e-01)\tAcc@1  78.91 ( 73.76)\tAcc@5  93.75 ( 94.67)\n","Epoch: [50][120/391]\tTime  0.176 ( 0.176)\tLoss 9.0141e-01 (8.8711e-01)\tAcc@1  77.34 ( 73.91)\tAcc@5  95.31 ( 94.63)\n","Epoch: [50][150/391]\tTime  0.175 ( 0.176)\tLoss 7.9262e-01 (8.9661e-01)\tAcc@1  75.78 ( 73.56)\tAcc@5  95.31 ( 94.60)\n","Epoch: [50][180/391]\tTime  0.177 ( 0.176)\tLoss 1.1535e+00 (9.1550e-01)\tAcc@1  67.97 ( 72.89)\tAcc@5  90.62 ( 94.50)\n","Epoch: [50][210/391]\tTime  0.177 ( 0.176)\tLoss 9.2041e-01 (9.2837e-01)\tAcc@1  75.78 ( 72.62)\tAcc@5  94.53 ( 94.34)\n","Epoch: [50][240/391]\tTime  0.174 ( 0.176)\tLoss 1.0930e+00 (9.3457e-01)\tAcc@1  68.75 ( 72.43)\tAcc@5  94.53 ( 94.28)\n","Epoch: [50][270/391]\tTime  0.175 ( 0.176)\tLoss 9.1916e-01 (9.4113e-01)\tAcc@1  73.44 ( 72.20)\tAcc@5  94.53 ( 94.22)\n","Epoch: [50][300/391]\tTime  0.176 ( 0.176)\tLoss 1.0189e+00 (9.4653e-01)\tAcc@1  72.66 ( 72.09)\tAcc@5  92.97 ( 94.16)\n","Epoch: [50][330/391]\tTime  0.177 ( 0.175)\tLoss 1.2116e+00 (9.5315e-01)\tAcc@1  67.97 ( 71.88)\tAcc@5  90.62 ( 94.09)\n","Epoch: [50][360/391]\tTime  0.174 ( 0.175)\tLoss 9.9075e-01 (9.6055e-01)\tAcc@1  75.00 ( 71.67)\tAcc@5  93.75 ( 93.97)\n","Epoch: [50][390/391]\tTime  0.158 ( 0.175)\tLoss 1.1061e+00 (9.6492e-01)\tAcc@1  72.50 ( 71.52)\tAcc@5  88.75 ( 93.93)\n","==> Train Accuracy: Acc@1 71.522 || Acc@5 93.928\n","==> Test Accuracy:  Acc@1 60.290 || Acc@5 86.350\n","==> 72.91 seconds to train this epoch\n","\n","\n","----- epoch: 51, lr: 0.1 -----\n","Epoch: [51][  0/391]\tTime  0.213 ( 0.213)\tLoss 6.9830e-01 (6.9830e-01)\tAcc@1  78.12 ( 78.12)\tAcc@5  97.66 ( 97.66)\n","Epoch: [51][ 30/391]\tTime  0.176 ( 0.176)\tLoss 6.8026e-01 (8.0582e-01)\tAcc@1  80.47 ( 76.51)\tAcc@5  96.88 ( 95.49)\n","Epoch: [51][ 60/391]\tTime  0.176 ( 0.176)\tLoss 8.0626e-01 (8.4198e-01)\tAcc@1  77.34 ( 74.94)\tAcc@5  97.66 ( 95.16)\n","Epoch: [51][ 90/391]\tTime  0.175 ( 0.176)\tLoss 1.0970e+00 (8.7915e-01)\tAcc@1  64.84 ( 73.87)\tAcc@5  93.75 ( 94.71)\n","Epoch: [51][120/391]\tTime  0.174 ( 0.176)\tLoss 1.0538e+00 (8.9095e-01)\tAcc@1  67.19 ( 73.52)\tAcc@5  92.97 ( 94.62)\n","Epoch: [51][150/391]\tTime  0.176 ( 0.175)\tLoss 9.4779e-01 (9.0218e-01)\tAcc@1  69.53 ( 73.21)\tAcc@5  92.19 ( 94.50)\n","Epoch: [51][180/391]\tTime  0.175 ( 0.175)\tLoss 1.0571e+00 (9.0827e-01)\tAcc@1  71.88 ( 73.04)\tAcc@5  92.97 ( 94.48)\n","Epoch: [51][210/391]\tTime  0.176 ( 0.175)\tLoss 1.0639e+00 (9.3003e-01)\tAcc@1  65.62 ( 72.44)\tAcc@5  96.09 ( 94.26)\n","Epoch: [51][240/391]\tTime  0.174 ( 0.175)\tLoss 1.1135e+00 (9.3692e-01)\tAcc@1  66.41 ( 72.22)\tAcc@5  91.41 ( 94.17)\n","Epoch: [51][270/391]\tTime  0.175 ( 0.175)\tLoss 1.0794e+00 (9.4487e-01)\tAcc@1  68.75 ( 72.06)\tAcc@5  92.97 ( 94.09)\n","Epoch: [51][300/391]\tTime  0.175 ( 0.175)\tLoss 1.0779e+00 (9.5072e-01)\tAcc@1  62.50 ( 71.86)\tAcc@5  94.53 ( 94.03)\n","Epoch: [51][330/391]\tTime  0.175 ( 0.175)\tLoss 1.1894e+00 (9.5545e-01)\tAcc@1  67.19 ( 71.67)\tAcc@5  89.06 ( 94.00)\n","Epoch: [51][360/391]\tTime  0.175 ( 0.175)\tLoss 9.3106e-01 (9.5938e-01)\tAcc@1  75.00 ( 71.59)\tAcc@5  93.75 ( 93.92)\n","Epoch: [51][390/391]\tTime  0.157 ( 0.175)\tLoss 1.3581e+00 (9.5939e-01)\tAcc@1  61.25 ( 71.59)\tAcc@5  87.50 ( 93.94)\n","==> Train Accuracy: Acc@1 71.594 || Acc@5 93.936\n","==> Test Accuracy:  Acc@1 56.430 || Acc@5 84.570\n","==> 72.94 seconds to train this epoch\n","\n","\n","----- epoch: 52, lr: 0.1 -----\n","Epoch: [52][  0/391]\tTime  0.317 ( 0.317)\tLoss 6.6165e-01 (6.6165e-01)\tAcc@1  76.56 ( 76.56)\tAcc@5  95.31 ( 95.31)\n","Epoch: [52][ 30/391]\tTime  0.178 ( 0.179)\tLoss 8.5429e-01 (8.9165e-01)\tAcc@1  75.78 ( 73.84)\tAcc@5  95.31 ( 94.48)\n","Epoch: [52][ 60/391]\tTime  0.176 ( 0.177)\tLoss 1.0236e+00 (9.0560e-01)\tAcc@1  69.53 ( 72.78)\tAcc@5  92.97 ( 94.63)\n","Epoch: [52][ 90/391]\tTime  0.174 ( 0.177)\tLoss 1.0287e+00 (9.0265e-01)\tAcc@1  71.09 ( 72.91)\tAcc@5  91.41 ( 94.67)\n","Epoch: [52][120/391]\tTime  0.175 ( 0.176)\tLoss 9.0576e-01 (9.0623e-01)\tAcc@1  75.78 ( 72.71)\tAcc@5  94.53 ( 94.62)\n","Epoch: [52][150/391]\tTime  0.176 ( 0.176)\tLoss 1.0388e+00 (9.1660e-01)\tAcc@1  68.75 ( 72.57)\tAcc@5  92.97 ( 94.47)\n","Epoch: [52][180/391]\tTime  0.174 ( 0.176)\tLoss 1.0222e+00 (9.2353e-01)\tAcc@1  67.19 ( 72.35)\tAcc@5  92.97 ( 94.46)\n","Epoch: [52][210/391]\tTime  0.176 ( 0.176)\tLoss 9.3091e-01 (9.2799e-01)\tAcc@1  75.78 ( 72.24)\tAcc@5  94.53 ( 94.39)\n","Epoch: [52][240/391]\tTime  0.175 ( 0.176)\tLoss 1.0603e+00 (9.3846e-01)\tAcc@1  65.62 ( 71.91)\tAcc@5  92.19 ( 94.31)\n","Epoch: [52][270/391]\tTime  0.171 ( 0.176)\tLoss 1.1058e+00 (9.4543e-01)\tAcc@1  67.19 ( 71.67)\tAcc@5  94.53 ( 94.28)\n","Epoch: [52][300/391]\tTime  0.175 ( 0.176)\tLoss 1.0830e+00 (9.5118e-01)\tAcc@1  67.19 ( 71.58)\tAcc@5  93.75 ( 94.18)\n","Epoch: [52][330/391]\tTime  0.174 ( 0.175)\tLoss 1.0085e+00 (9.5746e-01)\tAcc@1  67.97 ( 71.46)\tAcc@5  96.09 ( 94.08)\n","Epoch: [52][360/391]\tTime  0.173 ( 0.175)\tLoss 9.7919e-01 (9.6115e-01)\tAcc@1  68.75 ( 71.43)\tAcc@5  94.53 ( 93.99)\n","Epoch: [52][390/391]\tTime  0.158 ( 0.175)\tLoss 9.7058e-01 (9.6745e-01)\tAcc@1  68.75 ( 71.27)\tAcc@5  96.25 ( 93.93)\n","==> Train Accuracy: Acc@1 71.270 || Acc@5 93.926\n","==> Test Accuracy:  Acc@1 59.330 || Acc@5 86.850\n","==> 72.93 seconds to train this epoch\n","\n","\n","----- epoch: 53, lr: 0.1 -----\n","Epoch: [53][  0/391]\tTime  0.230 ( 0.230)\tLoss 1.0249e+00 (1.0249e+00)\tAcc@1  68.75 ( 68.75)\tAcc@5  92.19 ( 92.19)\n","Epoch: [53][ 30/391]\tTime  0.176 ( 0.177)\tLoss 9.3986e-01 (9.0189e-01)\tAcc@1  76.56 ( 72.91)\tAcc@5  92.97 ( 94.66)\n","Epoch: [53][ 60/391]\tTime  0.174 ( 0.176)\tLoss 6.6651e-01 (8.6336e-01)\tAcc@1  82.03 ( 74.26)\tAcc@5  97.66 ( 95.08)\n","Epoch: [53][ 90/391]\tTime  0.176 ( 0.176)\tLoss 6.5290e-01 (8.8071e-01)\tAcc@1  80.47 ( 73.78)\tAcc@5  97.66 ( 94.86)\n","Epoch: [53][120/391]\tTime  0.174 ( 0.176)\tLoss 9.2678e-01 (8.9630e-01)\tAcc@1  73.44 ( 73.06)\tAcc@5  92.97 ( 94.67)\n","Epoch: [53][150/391]\tTime  0.175 ( 0.175)\tLoss 1.0136e+00 (9.0831e-01)\tAcc@1  69.53 ( 72.71)\tAcc@5  94.53 ( 94.62)\n","Epoch: [53][180/391]\tTime  0.178 ( 0.175)\tLoss 7.6228e-01 (9.1888e-01)\tAcc@1  81.25 ( 72.53)\tAcc@5  96.88 ( 94.48)\n","Epoch: [53][210/391]\tTime  0.175 ( 0.175)\tLoss 1.0166e+00 (9.2962e-01)\tAcc@1  68.75 ( 72.32)\tAcc@5  93.75 ( 94.36)\n","Epoch: [53][240/391]\tTime  0.176 ( 0.175)\tLoss 6.3157e-01 (9.3580e-01)\tAcc@1  80.47 ( 72.08)\tAcc@5  97.66 ( 94.34)\n","Epoch: [53][270/391]\tTime  0.174 ( 0.175)\tLoss 1.2384e+00 (9.4414e-01)\tAcc@1  67.97 ( 72.00)\tAcc@5  92.19 ( 94.20)\n","Epoch: [53][300/391]\tTime  0.174 ( 0.175)\tLoss 1.0822e+00 (9.5281e-01)\tAcc@1  70.31 ( 71.74)\tAcc@5  95.31 ( 94.12)\n","Epoch: [53][330/391]\tTime  0.177 ( 0.175)\tLoss 8.9087e-01 (9.5869e-01)\tAcc@1  72.66 ( 71.67)\tAcc@5  96.09 ( 94.00)\n","Epoch: [53][360/391]\tTime  0.177 ( 0.175)\tLoss 8.5215e-01 (9.6335e-01)\tAcc@1  74.22 ( 71.52)\tAcc@5  94.53 ( 93.97)\n","Epoch: [53][390/391]\tTime  0.158 ( 0.175)\tLoss 9.4553e-01 (9.6453e-01)\tAcc@1  68.75 ( 71.47)\tAcc@5  93.75 ( 93.95)\n","==> Train Accuracy: Acc@1 71.466 || Acc@5 93.948\n","==> Test Accuracy:  Acc@1 59.050 || Acc@5 85.790\n","==> 72.81 seconds to train this epoch\n","\n","\n","----- epoch: 54, lr: 0.1 -----\n","Epoch: [54][  0/391]\tTime  0.228 ( 0.228)\tLoss 8.9093e-01 (8.9093e-01)\tAcc@1  75.78 ( 75.78)\tAcc@5  97.66 ( 97.66)\n","Epoch: [54][ 30/391]\tTime  0.173 ( 0.176)\tLoss 7.7037e-01 (8.4054e-01)\tAcc@1  76.56 ( 75.15)\tAcc@5  96.88 ( 95.64)\n","Epoch: [54][ 60/391]\tTime  0.175 ( 0.176)\tLoss 8.4107e-01 (8.6524e-01)\tAcc@1  75.00 ( 74.46)\tAcc@5  96.09 ( 95.08)\n","Epoch: [54][ 90/391]\tTime  0.176 ( 0.175)\tLoss 1.0848e+00 (8.7921e-01)\tAcc@1  71.09 ( 73.62)\tAcc@5  92.19 ( 95.07)\n","Epoch: [54][120/391]\tTime  0.175 ( 0.175)\tLoss 7.8977e-01 (8.8963e-01)\tAcc@1  74.22 ( 73.41)\tAcc@5  95.31 ( 94.86)\n","Epoch: [54][150/391]\tTime  0.176 ( 0.175)\tLoss 1.1710e+00 (8.9731e-01)\tAcc@1  64.06 ( 73.14)\tAcc@5  91.41 ( 94.77)\n","Epoch: [54][180/391]\tTime  0.174 ( 0.175)\tLoss 9.7922e-01 (9.1390e-01)\tAcc@1  73.44 ( 72.76)\tAcc@5  92.97 ( 94.53)\n","Epoch: [54][210/391]\tTime  0.175 ( 0.175)\tLoss 9.2500e-01 (9.1733e-01)\tAcc@1  71.09 ( 72.67)\tAcc@5  93.75 ( 94.49)\n","Epoch: [54][240/391]\tTime  0.175 ( 0.175)\tLoss 1.0579e+00 (9.2783e-01)\tAcc@1  67.97 ( 72.42)\tAcc@5  91.41 ( 94.36)\n","Epoch: [54][270/391]\tTime  0.175 ( 0.175)\tLoss 8.1275e-01 (9.3913e-01)\tAcc@1  78.12 ( 72.14)\tAcc@5  93.75 ( 94.18)\n","Epoch: [54][300/391]\tTime  0.177 ( 0.175)\tLoss 1.1275e+00 (9.4880e-01)\tAcc@1  62.50 ( 71.84)\tAcc@5  92.97 ( 94.03)\n","Epoch: [54][330/391]\tTime  0.176 ( 0.175)\tLoss 1.0227e+00 (9.5000e-01)\tAcc@1  68.75 ( 71.83)\tAcc@5  92.97 ( 94.00)\n","Epoch: [54][360/391]\tTime  0.174 ( 0.175)\tLoss 9.6305e-01 (9.5136e-01)\tAcc@1  71.88 ( 71.82)\tAcc@5  95.31 ( 93.96)\n","Epoch: [54][390/391]\tTime  0.157 ( 0.175)\tLoss 1.1026e+00 (9.5748e-01)\tAcc@1  65.00 ( 71.68)\tAcc@5  92.50 ( 93.90)\n","==> Train Accuracy: Acc@1 71.680 || Acc@5 93.898\n","==> Test Accuracy:  Acc@1 58.380 || Acc@5 85.210\n","==> 72.69 seconds to train this epoch\n","\n","\n","----- epoch: 55, lr: 0.1 -----\n","Epoch: [55][  0/391]\tTime  0.238 ( 0.238)\tLoss 9.1329e-01 (9.1329e-01)\tAcc@1  69.53 ( 69.53)\tAcc@5  96.09 ( 96.09)\n","Epoch: [55][ 30/391]\tTime  0.172 ( 0.176)\tLoss 7.6686e-01 (8.7150e-01)\tAcc@1  82.03 ( 74.87)\tAcc@5  95.31 ( 95.61)\n","Epoch: [55][ 60/391]\tTime  0.174 ( 0.175)\tLoss 8.6471e-01 (8.6965e-01)\tAcc@1  78.12 ( 74.90)\tAcc@5  95.31 ( 95.26)\n","Epoch: [55][ 90/391]\tTime  0.173 ( 0.175)\tLoss 9.0416e-01 (8.8344e-01)\tAcc@1  69.53 ( 74.06)\tAcc@5  92.97 ( 94.99)\n","Epoch: [55][120/391]\tTime  0.176 ( 0.175)\tLoss 8.7382e-01 (9.0074e-01)\tAcc@1  75.00 ( 73.42)\tAcc@5  96.09 ( 94.79)\n","Epoch: [55][150/391]\tTime  0.174 ( 0.175)\tLoss 1.0459e+00 (9.0898e-01)\tAcc@1  67.19 ( 73.26)\tAcc@5  94.53 ( 94.63)\n","Epoch: [55][180/391]\tTime  0.178 ( 0.175)\tLoss 1.0665e+00 (9.1831e-01)\tAcc@1  71.88 ( 72.98)\tAcc@5  89.06 ( 94.50)\n","Epoch: [55][210/391]\tTime  0.175 ( 0.175)\tLoss 9.5803e-01 (9.2606e-01)\tAcc@1  75.78 ( 72.65)\tAcc@5  95.31 ( 94.42)\n","Epoch: [55][240/391]\tTime  0.176 ( 0.175)\tLoss 1.0844e+00 (9.3859e-01)\tAcc@1  67.97 ( 72.34)\tAcc@5  93.75 ( 94.28)\n","Epoch: [55][270/391]\tTime  0.173 ( 0.175)\tLoss 1.1541e+00 (9.4358e-01)\tAcc@1  67.19 ( 72.19)\tAcc@5  90.62 ( 94.24)\n","Epoch: [55][300/391]\tTime  0.176 ( 0.175)\tLoss 1.2166e+00 (9.4884e-01)\tAcc@1  62.50 ( 72.06)\tAcc@5  91.41 ( 94.17)\n","Epoch: [55][330/391]\tTime  0.175 ( 0.175)\tLoss 8.3234e-01 (9.4964e-01)\tAcc@1  73.44 ( 72.05)\tAcc@5  99.22 ( 94.17)\n","Epoch: [55][360/391]\tTime  0.175 ( 0.175)\tLoss 1.1002e+00 (9.5158e-01)\tAcc@1  68.75 ( 71.97)\tAcc@5  91.41 ( 94.16)\n","Epoch: [55][390/391]\tTime  0.158 ( 0.175)\tLoss 8.4216e-01 (9.6002e-01)\tAcc@1  76.25 ( 71.75)\tAcc@5  97.50 ( 94.04)\n","==> Train Accuracy: Acc@1 71.754 || Acc@5 94.038\n","==> Test Accuracy:  Acc@1 58.880 || Acc@5 86.200\n","==> 72.70 seconds to train this epoch\n","\n","\n","----- epoch: 56, lr: 0.1 -----\n","Epoch: [56][  0/391]\tTime  0.229 ( 0.229)\tLoss 1.0336e+00 (1.0336e+00)\tAcc@1  69.53 ( 69.53)\tAcc@5  94.53 ( 94.53)\n","Epoch: [56][ 30/391]\tTime  0.175 ( 0.176)\tLoss 6.8215e-01 (8.2745e-01)\tAcc@1  78.91 ( 74.97)\tAcc@5  92.97 ( 95.97)\n","Epoch: [56][ 60/391]\tTime  0.174 ( 0.175)\tLoss 9.6193e-01 (8.5853e-01)\tAcc@1  72.66 ( 74.55)\tAcc@5  93.75 ( 95.38)\n","Epoch: [56][ 90/391]\tTime  0.177 ( 0.175)\tLoss 7.7798e-01 (8.8396e-01)\tAcc@1  77.34 ( 73.85)\tAcc@5  94.53 ( 95.14)\n","Epoch: [56][120/391]\tTime  0.175 ( 0.175)\tLoss 8.8559e-01 (8.9830e-01)\tAcc@1  75.78 ( 73.53)\tAcc@5  91.41 ( 94.91)\n","Epoch: [56][150/391]\tTime  0.174 ( 0.175)\tLoss 9.7212e-01 (9.0412e-01)\tAcc@1  72.66 ( 73.30)\tAcc@5  92.19 ( 94.77)\n","Epoch: [56][180/391]\tTime  0.174 ( 0.175)\tLoss 8.5056e-01 (9.0405e-01)\tAcc@1  74.22 ( 73.12)\tAcc@5  95.31 ( 94.78)\n","Epoch: [56][210/391]\tTime  0.174 ( 0.175)\tLoss 8.8952e-01 (9.1406e-01)\tAcc@1  72.66 ( 72.82)\tAcc@5  94.53 ( 94.69)\n","Epoch: [56][240/391]\tTime  0.177 ( 0.175)\tLoss 9.4609e-01 (9.2403e-01)\tAcc@1  71.09 ( 72.51)\tAcc@5  95.31 ( 94.54)\n","Epoch: [56][270/391]\tTime  0.174 ( 0.175)\tLoss 8.5567e-01 (9.3118e-01)\tAcc@1  71.88 ( 72.34)\tAcc@5  96.88 ( 94.46)\n","Epoch: [56][300/391]\tTime  0.174 ( 0.175)\tLoss 1.0762e+00 (9.3855e-01)\tAcc@1  68.75 ( 72.20)\tAcc@5  95.31 ( 94.39)\n","Epoch: [56][330/391]\tTime  0.175 ( 0.175)\tLoss 8.4400e-01 (9.4185e-01)\tAcc@1  71.88 ( 72.14)\tAcc@5  94.53 ( 94.30)\n","Epoch: [56][360/391]\tTime  0.177 ( 0.175)\tLoss 9.6305e-01 (9.4533e-01)\tAcc@1  72.66 ( 72.08)\tAcc@5  95.31 ( 94.27)\n","Epoch: [56][390/391]\tTime  0.157 ( 0.175)\tLoss 9.3417e-01 (9.5184e-01)\tAcc@1  65.00 ( 71.94)\tAcc@5  98.75 ( 94.17)\n","==> Train Accuracy: Acc@1 71.942 || Acc@5 94.170\n","==> Test Accuracy:  Acc@1 57.520 || Acc@5 86.300\n","==> 72.65 seconds to train this epoch\n","\n","\n","----- epoch: 57, lr: 0.1 -----\n","Epoch: [57][  0/391]\tTime  0.243 ( 0.243)\tLoss 7.9376e-01 (7.9376e-01)\tAcc@1  76.56 ( 76.56)\tAcc@5  96.88 ( 96.88)\n","Epoch: [57][ 30/391]\tTime  0.175 ( 0.176)\tLoss 9.4072e-01 (8.9831e-01)\tAcc@1  70.31 ( 72.76)\tAcc@5  93.75 ( 95.06)\n","Epoch: [57][ 60/391]\tTime  0.174 ( 0.175)\tLoss 8.9177e-01 (8.7271e-01)\tAcc@1  73.44 ( 73.64)\tAcc@5  93.75 ( 95.56)\n","Epoch: [57][ 90/391]\tTime  0.176 ( 0.175)\tLoss 9.2034e-01 (8.8455e-01)\tAcc@1  69.53 ( 73.40)\tAcc@5  96.09 ( 95.18)\n","Epoch: [57][120/391]\tTime  0.174 ( 0.175)\tLoss 1.1295e+00 (8.9630e-01)\tAcc@1  71.09 ( 73.14)\tAcc@5  93.75 ( 94.97)\n","Epoch: [57][150/391]\tTime  0.176 ( 0.175)\tLoss 1.2839e+00 (9.0676e-01)\tAcc@1  59.38 ( 72.86)\tAcc@5  89.84 ( 94.83)\n","Epoch: [57][180/391]\tTime  0.175 ( 0.175)\tLoss 1.0121e+00 (9.1391e-01)\tAcc@1  68.75 ( 72.73)\tAcc@5  92.19 ( 94.71)\n","Epoch: [57][210/391]\tTime  0.176 ( 0.175)\tLoss 1.0520e+00 (9.2511e-01)\tAcc@1  67.97 ( 72.37)\tAcc@5  94.53 ( 94.54)\n","Epoch: [57][240/391]\tTime  0.176 ( 0.175)\tLoss 1.0973e+00 (9.2987e-01)\tAcc@1  69.53 ( 72.23)\tAcc@5  92.97 ( 94.45)\n","Epoch: [57][270/391]\tTime  0.177 ( 0.175)\tLoss 1.0707e+00 (9.4045e-01)\tAcc@1  71.09 ( 71.94)\tAcc@5  90.62 ( 94.36)\n","Epoch: [57][300/391]\tTime  0.175 ( 0.175)\tLoss 7.8176e-01 (9.4198e-01)\tAcc@1  80.47 ( 71.93)\tAcc@5  94.53 ( 94.29)\n","Epoch: [57][330/391]\tTime  0.175 ( 0.175)\tLoss 9.6390e-01 (9.4211e-01)\tAcc@1  74.22 ( 71.92)\tAcc@5  92.97 ( 94.23)\n","Epoch: [57][360/391]\tTime  0.175 ( 0.175)\tLoss 1.2257e+00 (9.4582e-01)\tAcc@1  64.84 ( 71.86)\tAcc@5  87.50 ( 94.14)\n","Epoch: [57][390/391]\tTime  0.156 ( 0.175)\tLoss 9.0753e-01 (9.5109e-01)\tAcc@1  72.50 ( 71.74)\tAcc@5  95.00 ( 94.09)\n","==> Train Accuracy: Acc@1 71.742 || Acc@5 94.088\n","==> Test Accuracy:  Acc@1 57.730 || Acc@5 85.330\n","==> 72.65 seconds to train this epoch\n","\n","\n","----- epoch: 58, lr: 0.1 -----\n","Epoch: [58][  0/391]\tTime  0.232 ( 0.232)\tLoss 9.8678e-01 (9.8678e-01)\tAcc@1  71.09 ( 71.09)\tAcc@5  95.31 ( 95.31)\n","Epoch: [58][ 30/391]\tTime  0.175 ( 0.176)\tLoss 9.0672e-01 (9.1660e-01)\tAcc@1  74.22 ( 73.61)\tAcc@5  92.97 ( 94.28)\n","Epoch: [58][ 60/391]\tTime  0.176 ( 0.175)\tLoss 8.5275e-01 (8.9013e-01)\tAcc@1  74.22 ( 73.83)\tAcc@5  96.88 ( 94.95)\n","Epoch: [58][ 90/391]\tTime  0.174 ( 0.175)\tLoss 9.8091e-01 (8.8707e-01)\tAcc@1  70.31 ( 73.79)\tAcc@5  94.53 ( 94.78)\n","Epoch: [58][120/391]\tTime  0.175 ( 0.175)\tLoss 9.1785e-01 (9.0690e-01)\tAcc@1  70.31 ( 73.21)\tAcc@5  94.53 ( 94.57)\n","Epoch: [58][150/391]\tTime  0.175 ( 0.175)\tLoss 9.0471e-01 (9.0870e-01)\tAcc@1  75.00 ( 73.18)\tAcc@5  96.88 ( 94.46)\n","Epoch: [58][180/391]\tTime  0.175 ( 0.175)\tLoss 9.5667e-01 (9.1347e-01)\tAcc@1  75.00 ( 73.05)\tAcc@5  92.19 ( 94.39)\n","Epoch: [58][210/391]\tTime  0.174 ( 0.175)\tLoss 1.1536e+00 (9.2099e-01)\tAcc@1  66.41 ( 72.79)\tAcc@5  89.06 ( 94.34)\n","Epoch: [58][240/391]\tTime  0.169 ( 0.175)\tLoss 7.7673e-01 (9.3163e-01)\tAcc@1  77.34 ( 72.54)\tAcc@5  93.75 ( 94.20)\n","Epoch: [58][270/391]\tTime  0.173 ( 0.175)\tLoss 8.3840e-01 (9.3272e-01)\tAcc@1  75.78 ( 72.58)\tAcc@5  94.53 ( 94.21)\n","Epoch: [58][300/391]\tTime  0.174 ( 0.175)\tLoss 7.6519e-01 (9.3544e-01)\tAcc@1  77.34 ( 72.51)\tAcc@5  96.09 ( 94.19)\n","Epoch: [58][330/391]\tTime  0.175 ( 0.175)\tLoss 1.0532e+00 (9.3855e-01)\tAcc@1  66.41 ( 72.35)\tAcc@5  92.19 ( 94.18)\n","Epoch: [58][360/391]\tTime  0.174 ( 0.175)\tLoss 9.5010e-01 (9.4485e-01)\tAcc@1  75.00 ( 72.12)\tAcc@5  93.75 ( 94.11)\n","Epoch: [58][390/391]\tTime  0.158 ( 0.175)\tLoss 1.1188e+00 (9.4983e-01)\tAcc@1  70.00 ( 71.98)\tAcc@5  93.75 ( 94.04)\n","==> Train Accuracy: Acc@1 71.978 || Acc@5 94.044\n","==> Test Accuracy:  Acc@1 57.600 || Acc@5 85.050\n","==> 72.64 seconds to train this epoch\n","\n","\n","----- epoch: 59, lr: 0.1 -----\n","Epoch: [59][  0/391]\tTime  0.236 ( 0.236)\tLoss 9.3305e-01 (9.3305e-01)\tAcc@1  74.22 ( 74.22)\tAcc@5  96.09 ( 96.09)\n","Epoch: [59][ 30/391]\tTime  0.177 ( 0.176)\tLoss 7.3612e-01 (8.9206e-01)\tAcc@1  78.12 ( 73.31)\tAcc@5  96.88 ( 95.29)\n","Epoch: [59][ 60/391]\tTime  0.174 ( 0.176)\tLoss 1.0221e+00 (8.7918e-01)\tAcc@1  67.97 ( 73.49)\tAcc@5  92.97 ( 95.34)\n","Epoch: [59][ 90/391]\tTime  0.174 ( 0.175)\tLoss 1.0194e+00 (9.0530e-01)\tAcc@1  69.53 ( 72.96)\tAcc@5  92.19 ( 94.77)\n","Epoch: [59][120/391]\tTime  0.174 ( 0.175)\tLoss 1.0310e+00 (9.0399e-01)\tAcc@1  69.53 ( 73.02)\tAcc@5  94.53 ( 94.64)\n","Epoch: [59][150/391]\tTime  0.174 ( 0.175)\tLoss 9.4468e-01 (9.1719e-01)\tAcc@1  72.66 ( 72.86)\tAcc@5  91.41 ( 94.40)\n","Epoch: [59][180/391]\tTime  0.175 ( 0.175)\tLoss 7.4977e-01 (9.1731e-01)\tAcc@1  78.12 ( 72.89)\tAcc@5  96.09 ( 94.35)\n","Epoch: [59][210/391]\tTime  0.174 ( 0.175)\tLoss 8.4266e-01 (9.2730e-01)\tAcc@1  77.34 ( 72.70)\tAcc@5  96.09 ( 94.20)\n","Epoch: [59][240/391]\tTime  0.175 ( 0.175)\tLoss 8.8780e-01 (9.3316e-01)\tAcc@1  71.88 ( 72.51)\tAcc@5  97.66 ( 94.19)\n","Epoch: [59][270/391]\tTime  0.170 ( 0.175)\tLoss 9.1651e-01 (9.3974e-01)\tAcc@1  71.88 ( 72.35)\tAcc@5  92.97 ( 94.14)\n","Epoch: [59][300/391]\tTime  0.175 ( 0.175)\tLoss 1.0779e+00 (9.3875e-01)\tAcc@1  67.19 ( 72.35)\tAcc@5  91.41 ( 94.18)\n","Epoch: [59][330/391]\tTime  0.176 ( 0.175)\tLoss 9.8423e-01 (9.4533e-01)\tAcc@1  71.88 ( 72.19)\tAcc@5  94.53 ( 94.17)\n","Epoch: [59][360/391]\tTime  0.177 ( 0.175)\tLoss 1.0758e+00 (9.4682e-01)\tAcc@1  70.31 ( 72.16)\tAcc@5  94.53 ( 94.13)\n","Epoch: [59][390/391]\tTime  0.157 ( 0.175)\tLoss 1.0969e+00 (9.5153e-01)\tAcc@1  71.25 ( 72.07)\tAcc@5  91.25 ( 94.04)\n","==> Train Accuracy: Acc@1 72.072 || Acc@5 94.036\n","==> Test Accuracy:  Acc@1 60.210 || Acc@5 86.510\n","==> 72.84 seconds to train this epoch\n","\n","\n","----- epoch: 60, lr: 0.020000000000000004 -----\n","Epoch: [60][  0/391]\tTime  0.233 ( 0.233)\tLoss 1.0283e+00 (1.0283e+00)\tAcc@1  65.62 ( 65.62)\tAcc@5  92.19 ( 92.19)\n","Epoch: [60][ 30/391]\tTime  0.174 ( 0.177)\tLoss 5.5166e-01 (7.1857e-01)\tAcc@1  85.94 ( 78.33)\tAcc@5  96.88 ( 96.42)\n","Epoch: [60][ 60/391]\tTime  0.176 ( 0.176)\tLoss 4.4206e-01 (6.5454e-01)\tAcc@1  89.06 ( 80.30)\tAcc@5 100.00 ( 97.09)\n","Epoch: [60][ 90/391]\tTime  0.176 ( 0.176)\tLoss 4.9012e-01 (6.1803e-01)\tAcc@1  82.81 ( 81.50)\tAcc@5  98.44 ( 97.31)\n","Epoch: [60][120/391]\tTime  0.176 ( 0.176)\tLoss 5.1022e-01 (5.9026e-01)\tAcc@1  85.16 ( 82.29)\tAcc@5  97.66 ( 97.49)\n","Epoch: [60][150/391]\tTime  0.175 ( 0.176)\tLoss 6.0126e-01 (5.7415e-01)\tAcc@1  84.38 ( 82.93)\tAcc@5  96.88 ( 97.57)\n","Epoch: [60][180/391]\tTime  0.175 ( 0.176)\tLoss 5.2956e-01 (5.5999e-01)\tAcc@1  83.59 ( 83.43)\tAcc@5  98.44 ( 97.65)\n","Epoch: [60][210/391]\tTime  0.176 ( 0.176)\tLoss 5.6995e-01 (5.4743e-01)\tAcc@1  87.50 ( 83.73)\tAcc@5  97.66 ( 97.73)\n","Epoch: [60][240/391]\tTime  0.175 ( 0.176)\tLoss 4.7447e-01 (5.3444e-01)\tAcc@1  85.94 ( 84.14)\tAcc@5  98.44 ( 97.82)\n","Epoch: [60][270/391]\tTime  0.175 ( 0.176)\tLoss 5.2426e-01 (5.2570e-01)\tAcc@1  81.25 ( 84.27)\tAcc@5  96.88 ( 97.90)\n","Epoch: [60][300/391]\tTime  0.179 ( 0.175)\tLoss 4.3022e-01 (5.1850e-01)\tAcc@1  88.28 ( 84.48)\tAcc@5 100.00 ( 97.97)\n","Epoch: [60][330/391]\tTime  0.174 ( 0.175)\tLoss 5.1092e-01 (5.1460e-01)\tAcc@1  82.81 ( 84.56)\tAcc@5  97.66 ( 97.99)\n","Epoch: [60][360/391]\tTime  0.176 ( 0.175)\tLoss 4.3895e-01 (5.1092e-01)\tAcc@1  83.59 ( 84.62)\tAcc@5 100.00 ( 98.00)\n","Epoch: [60][390/391]\tTime  0.158 ( 0.175)\tLoss 3.9412e-01 (5.0473e-01)\tAcc@1  88.75 ( 84.75)\tAcc@5  98.75 ( 98.03)\n","==> Train Accuracy: Acc@1 84.750 || Acc@5 98.034\n","==> Test Accuracy:  Acc@1 74.010 || Acc@5 93.660\n","==> 72.88 seconds to train this epoch\n","\n","\n","----- epoch: 61, lr: 0.020000000000000004 -----\n","Epoch: [61][  0/391]\tTime  0.242 ( 0.242)\tLoss 3.2765e-01 (3.2765e-01)\tAcc@1  91.41 ( 91.41)\tAcc@5  97.66 ( 97.66)\n","Epoch: [61][ 30/391]\tTime  0.174 ( 0.177)\tLoss 2.7371e-01 (3.5307e-01)\tAcc@1  92.19 ( 89.14)\tAcc@5 100.00 ( 98.89)\n","Epoch: [61][ 60/391]\tTime  0.173 ( 0.176)\tLoss 3.8198e-01 (3.4372e-01)\tAcc@1  86.72 ( 89.43)\tAcc@5 100.00 ( 99.03)\n","Epoch: [61][ 90/391]\tTime  0.175 ( 0.175)\tLoss 3.7808e-01 (3.4393e-01)\tAcc@1  89.06 ( 89.35)\tAcc@5  99.22 ( 99.07)\n","Epoch: [61][120/391]\tTime  0.178 ( 0.175)\tLoss 5.2167e-01 (3.4228e-01)\tAcc@1  82.81 ( 89.53)\tAcc@5  97.66 ( 99.12)\n","Epoch: [61][150/391]\tTime  0.175 ( 0.175)\tLoss 3.2070e-01 (3.4426e-01)\tAcc@1  89.84 ( 89.44)\tAcc@5 100.00 ( 99.07)\n","Epoch: [61][180/391]\tTime  0.174 ( 0.175)\tLoss 3.8141e-01 (3.4528e-01)\tAcc@1  90.62 ( 89.41)\tAcc@5  98.44 ( 99.07)\n","Epoch: [61][210/391]\tTime  0.175 ( 0.175)\tLoss 2.7491e-01 (3.4605e-01)\tAcc@1  93.75 ( 89.37)\tAcc@5  96.88 ( 99.05)\n","Epoch: [61][240/391]\tTime  0.175 ( 0.175)\tLoss 2.3291e-01 (3.4677e-01)\tAcc@1  95.31 ( 89.40)\tAcc@5  99.22 ( 99.01)\n","Epoch: [61][270/391]\tTime  0.177 ( 0.175)\tLoss 3.5806e-01 (3.4787e-01)\tAcc@1  89.06 ( 89.35)\tAcc@5 100.00 ( 99.02)\n","Epoch: [61][300/391]\tTime  0.176 ( 0.175)\tLoss 4.1478e-01 (3.4666e-01)\tAcc@1  87.50 ( 89.37)\tAcc@5 100.00 ( 99.04)\n","Epoch: [61][330/391]\tTime  0.172 ( 0.175)\tLoss 3.6697e-01 (3.4666e-01)\tAcc@1  88.28 ( 89.34)\tAcc@5  99.22 ( 99.05)\n","Epoch: [61][360/391]\tTime  0.175 ( 0.175)\tLoss 2.8646e-01 (3.4894e-01)\tAcc@1  89.06 ( 89.24)\tAcc@5  99.22 ( 99.04)\n","Epoch: [61][390/391]\tTime  0.158 ( 0.175)\tLoss 3.9510e-01 (3.5087e-01)\tAcc@1  87.50 ( 89.17)\tAcc@5  96.25 ( 99.02)\n","==> Train Accuracy: Acc@1 89.172 || Acc@5 99.016\n","==> Test Accuracy:  Acc@1 74.080 || Acc@5 93.460\n","==> 72.74 seconds to train this epoch\n","\n","\n","----- epoch: 62, lr: 0.020000000000000004 -----\n","Epoch: [62][  0/391]\tTime  0.226 ( 0.226)\tLoss 2.9263e-01 (2.9263e-01)\tAcc@1  91.41 ( 91.41)\tAcc@5 100.00 (100.00)\n","Epoch: [62][ 30/391]\tTime  0.175 ( 0.176)\tLoss 1.9454e-01 (2.7698e-01)\tAcc@1  92.97 ( 91.63)\tAcc@5 100.00 ( 99.47)\n","Epoch: [62][ 60/391]\tTime  0.175 ( 0.175)\tLoss 2.2724e-01 (2.7008e-01)\tAcc@1  92.97 ( 91.87)\tAcc@5 100.00 ( 99.54)\n","Epoch: [62][ 90/391]\tTime  0.174 ( 0.175)\tLoss 2.6660e-01 (2.7528e-01)\tAcc@1  92.97 ( 91.66)\tAcc@5 100.00 ( 99.48)\n","Epoch: [62][120/391]\tTime  0.176 ( 0.175)\tLoss 3.6463e-01 (2.7882e-01)\tAcc@1  86.72 ( 91.51)\tAcc@5 100.00 ( 99.43)\n","Epoch: [62][150/391]\tTime  0.174 ( 0.175)\tLoss 2.5860e-01 (2.8138e-01)\tAcc@1  92.19 ( 91.42)\tAcc@5 100.00 ( 99.44)\n","Epoch: [62][180/391]\tTime  0.174 ( 0.175)\tLoss 2.4152e-01 (2.8279e-01)\tAcc@1  96.09 ( 91.41)\tAcc@5  99.22 ( 99.43)\n","Epoch: [62][210/391]\tTime  0.174 ( 0.175)\tLoss 2.6189e-01 (2.8819e-01)\tAcc@1  92.19 ( 91.17)\tAcc@5  99.22 ( 99.41)\n","Epoch: [62][240/391]\tTime  0.175 ( 0.175)\tLoss 2.5727e-01 (2.9123e-01)\tAcc@1  91.41 ( 91.08)\tAcc@5 100.00 ( 99.40)\n","Epoch: [62][270/391]\tTime  0.175 ( 0.175)\tLoss 3.0330e-01 (2.9197e-01)\tAcc@1  89.06 ( 91.09)\tAcc@5 100.00 ( 99.39)\n","Epoch: [62][300/391]\tTime  0.177 ( 0.175)\tLoss 2.9674e-01 (2.9392e-01)\tAcc@1  88.28 ( 90.99)\tAcc@5  99.22 ( 99.39)\n","Epoch: [62][330/391]\tTime  0.174 ( 0.175)\tLoss 2.2802e-01 (2.9510e-01)\tAcc@1  92.19 ( 90.93)\tAcc@5 100.00 ( 99.39)\n","Epoch: [62][360/391]\tTime  0.175 ( 0.175)\tLoss 3.8248e-01 (2.9866e-01)\tAcc@1  88.28 ( 90.82)\tAcc@5  98.44 ( 99.35)\n","Epoch: [62][390/391]\tTime  0.158 ( 0.175)\tLoss 2.6908e-01 (2.9945e-01)\tAcc@1  93.75 ( 90.78)\tAcc@5 100.00 ( 99.35)\n","==> Train Accuracy: Acc@1 90.778 || Acc@5 99.350\n","==> Test Accuracy:  Acc@1 74.260 || Acc@5 93.580\n","==> 72.74 seconds to train this epoch\n","\n","\n","----- epoch: 63, lr: 0.020000000000000004 -----\n","Epoch: [63][  0/391]\tTime  0.243 ( 0.243)\tLoss 2.7011e-01 (2.7011e-01)\tAcc@1  92.19 ( 92.19)\tAcc@5  99.22 ( 99.22)\n","Epoch: [63][ 30/391]\tTime  0.176 ( 0.177)\tLoss 2.0865e-01 (2.2443e-01)\tAcc@1  92.97 ( 93.37)\tAcc@5  99.22 ( 99.67)\n","Epoch: [63][ 60/391]\tTime  0.172 ( 0.176)\tLoss 2.5741e-01 (2.2545e-01)\tAcc@1  93.75 ( 93.35)\tAcc@5  99.22 ( 99.64)\n","Epoch: [63][ 90/391]\tTime  0.176 ( 0.176)\tLoss 3.0215e-01 (2.3300e-01)\tAcc@1  90.62 ( 93.09)\tAcc@5  98.44 ( 99.59)\n","Epoch: [63][120/391]\tTime  0.174 ( 0.175)\tLoss 1.9914e-01 (2.3417e-01)\tAcc@1  94.53 ( 92.98)\tAcc@5  99.22 ( 99.61)\n","Epoch: [63][150/391]\tTime  0.175 ( 0.175)\tLoss 3.5620e-01 (2.3630e-01)\tAcc@1  90.62 ( 92.88)\tAcc@5  99.22 ( 99.62)\n","Epoch: [63][180/391]\tTime  0.174 ( 0.175)\tLoss 1.7426e-01 (2.3670e-01)\tAcc@1  95.31 ( 92.83)\tAcc@5  99.22 ( 99.64)\n","Epoch: [63][210/391]\tTime  0.177 ( 0.175)\tLoss 2.1432e-01 (2.4048e-01)\tAcc@1  91.41 ( 92.62)\tAcc@5  99.22 ( 99.63)\n","Epoch: [63][240/391]\tTime  0.175 ( 0.175)\tLoss 1.2370e-01 (2.4039e-01)\tAcc@1  96.88 ( 92.58)\tAcc@5 100.00 ( 99.64)\n","Epoch: [63][270/391]\tTime  0.176 ( 0.175)\tLoss 2.4951e-01 (2.4163e-01)\tAcc@1  90.62 ( 92.52)\tAcc@5 100.00 ( 99.61)\n","Epoch: [63][300/391]\tTime  0.174 ( 0.175)\tLoss 3.5073e-01 (2.4602e-01)\tAcc@1  89.84 ( 92.41)\tAcc@5  98.44 ( 99.59)\n","Epoch: [63][330/391]\tTime  0.175 ( 0.175)\tLoss 2.3865e-01 (2.4992e-01)\tAcc@1  92.19 ( 92.30)\tAcc@5  98.44 ( 99.58)\n","Epoch: [63][360/391]\tTime  0.176 ( 0.175)\tLoss 2.0993e-01 (2.5129e-01)\tAcc@1  93.75 ( 92.26)\tAcc@5 100.00 ( 99.57)\n","Epoch: [63][390/391]\tTime  0.158 ( 0.175)\tLoss 2.8784e-01 (2.5204e-01)\tAcc@1  95.00 ( 92.22)\tAcc@5  96.25 ( 99.55)\n","==> Train Accuracy: Acc@1 92.216 || Acc@5 99.552\n","==> Test Accuracy:  Acc@1 74.210 || Acc@5 93.130\n","==> 72.83 seconds to train this epoch\n","\n","\n","----- epoch: 64, lr: 0.020000000000000004 -----\n","Epoch: [64][  0/391]\tTime  0.239 ( 0.239)\tLoss 1.3007e-01 (1.3007e-01)\tAcc@1  98.44 ( 98.44)\tAcc@5 100.00 (100.00)\n","Epoch: [64][ 30/391]\tTime  0.175 ( 0.177)\tLoss 2.0782e-01 (2.3003e-01)\tAcc@1  91.41 ( 93.17)\tAcc@5 100.00 ( 99.57)\n","Epoch: [64][ 60/391]\tTime  0.174 ( 0.176)\tLoss 1.6081e-01 (2.1318e-01)\tAcc@1  96.88 ( 93.71)\tAcc@5 100.00 ( 99.67)\n","Epoch: [64][ 90/391]\tTime  0.175 ( 0.176)\tLoss 2.8672e-01 (2.1412e-01)\tAcc@1  89.06 ( 93.59)\tAcc@5 100.00 ( 99.71)\n","Epoch: [64][120/391]\tTime  0.176 ( 0.175)\tLoss 2.5293e-01 (2.1389e-01)\tAcc@1  92.97 ( 93.62)\tAcc@5 100.00 ( 99.70)\n","Epoch: [64][150/391]\tTime  0.175 ( 0.175)\tLoss 2.5690e-01 (2.1678e-01)\tAcc@1  91.41 ( 93.46)\tAcc@5  99.22 ( 99.72)\n","Epoch: [64][180/391]\tTime  0.177 ( 0.175)\tLoss 2.6423e-01 (2.1840e-01)\tAcc@1  91.41 ( 93.43)\tAcc@5  98.44 ( 99.69)\n","Epoch: [64][210/391]\tTime  0.175 ( 0.175)\tLoss 1.4774e-01 (2.2129e-01)\tAcc@1  97.66 ( 93.32)\tAcc@5 100.00 ( 99.69)\n","Epoch: [64][240/391]\tTime  0.176 ( 0.175)\tLoss 2.1712e-01 (2.2300e-01)\tAcc@1  92.19 ( 93.22)\tAcc@5 100.00 ( 99.70)\n","Epoch: [64][270/391]\tTime  0.176 ( 0.175)\tLoss 1.4225e-01 (2.2424e-01)\tAcc@1  94.53 ( 93.18)\tAcc@5 100.00 ( 99.70)\n","Epoch: [64][300/391]\tTime  0.174 ( 0.175)\tLoss 2.6796e-01 (2.2619e-01)\tAcc@1  91.41 ( 93.08)\tAcc@5 100.00 ( 99.70)\n","Epoch: [64][330/391]\tTime  0.176 ( 0.175)\tLoss 2.8081e-01 (2.2802e-01)\tAcc@1  93.75 ( 93.01)\tAcc@5 100.00 ( 99.69)\n","Epoch: [64][360/391]\tTime  0.175 ( 0.175)\tLoss 3.5744e-01 (2.2973e-01)\tAcc@1  90.62 ( 92.94)\tAcc@5 100.00 ( 99.68)\n","Epoch: [64][390/391]\tTime  0.157 ( 0.175)\tLoss 3.5314e-01 (2.3277e-01)\tAcc@1  90.00 ( 92.83)\tAcc@5  97.50 ( 99.67)\n","==> Train Accuracy: Acc@1 92.832 || Acc@5 99.672\n","==> Test Accuracy:  Acc@1 73.570 || Acc@5 92.930\n","==> 72.89 seconds to train this epoch\n","\n","\n","----- epoch: 65, lr: 0.020000000000000004 -----\n","Epoch: [65][  0/391]\tTime  0.229 ( 0.229)\tLoss 1.4407e-01 (1.4407e-01)\tAcc@1  96.09 ( 96.09)\tAcc@5 100.00 (100.00)\n","Epoch: [65][ 30/391]\tTime  0.175 ( 0.176)\tLoss 1.8447e-01 (1.9579e-01)\tAcc@1  94.53 ( 94.15)\tAcc@5 100.00 ( 99.80)\n","Epoch: [65][ 60/391]\tTime  0.177 ( 0.176)\tLoss 2.4559e-01 (1.9339e-01)\tAcc@1  92.97 ( 94.24)\tAcc@5  99.22 ( 99.78)\n","Epoch: [65][ 90/391]\tTime  0.178 ( 0.176)\tLoss 2.7379e-01 (1.9411e-01)\tAcc@1  89.84 ( 94.20)\tAcc@5  99.22 ( 99.78)\n","Epoch: [65][120/391]\tTime  0.175 ( 0.176)\tLoss 1.6318e-01 (2.0016e-01)\tAcc@1  96.09 ( 93.92)\tAcc@5 100.00 ( 99.79)\n","Epoch: [65][150/391]\tTime  0.176 ( 0.176)\tLoss 2.3327e-01 (2.0273e-01)\tAcc@1  95.31 ( 93.80)\tAcc@5  98.44 ( 99.76)\n","Epoch: [65][180/391]\tTime  0.176 ( 0.176)\tLoss 2.2160e-01 (2.0554e-01)\tAcc@1  93.75 ( 93.70)\tAcc@5 100.00 ( 99.73)\n","Epoch: [65][210/391]\tTime  0.176 ( 0.176)\tLoss 1.2201e-01 (2.0292e-01)\tAcc@1  96.88 ( 93.81)\tAcc@5 100.00 ( 99.76)\n","Epoch: [65][240/391]\tTime  0.175 ( 0.175)\tLoss 2.8093e-01 (2.0552e-01)\tAcc@1  89.84 ( 93.74)\tAcc@5 100.00 ( 99.75)\n","Epoch: [65][270/391]\tTime  0.176 ( 0.175)\tLoss 1.5108e-01 (2.0583e-01)\tAcc@1  95.31 ( 93.74)\tAcc@5 100.00 ( 99.76)\n","Epoch: [65][300/391]\tTime  0.175 ( 0.175)\tLoss 1.9730e-01 (2.0789e-01)\tAcc@1  92.97 ( 93.62)\tAcc@5 100.00 ( 99.75)\n","Epoch: [65][330/391]\tTime  0.175 ( 0.175)\tLoss 1.3020e-01 (2.0898e-01)\tAcc@1  93.75 ( 93.54)\tAcc@5 100.00 ( 99.75)\n","Epoch: [65][360/391]\tTime  0.176 ( 0.175)\tLoss 2.2873e-01 (2.1126e-01)\tAcc@1  92.19 ( 93.46)\tAcc@5 100.00 ( 99.75)\n","Epoch: [65][390/391]\tTime  0.157 ( 0.175)\tLoss 2.7600e-01 (2.1357e-01)\tAcc@1  92.50 ( 93.36)\tAcc@5 100.00 ( 99.75)\n","==> Train Accuracy: Acc@1 93.360 || Acc@5 99.750\n","==> Test Accuracy:  Acc@1 73.290 || Acc@5 93.030\n","==> 72.92 seconds to train this epoch\n","\n","\n","----- epoch: 66, lr: 0.020000000000000004 -----\n","Epoch: [66][  0/391]\tTime  0.224 ( 0.224)\tLoss 2.0364e-01 (2.0364e-01)\tAcc@1  92.97 ( 92.97)\tAcc@5 100.00 (100.00)\n","Epoch: [66][ 30/391]\tTime  0.173 ( 0.176)\tLoss 1.3386e-01 (1.8813e-01)\tAcc@1  96.09 ( 93.85)\tAcc@5 100.00 ( 99.92)\n","Epoch: [66][ 60/391]\tTime  0.177 ( 0.176)\tLoss 2.3014e-01 (1.8768e-01)\tAcc@1  92.97 ( 93.98)\tAcc@5  99.22 ( 99.85)\n","Epoch: [66][ 90/391]\tTime  0.177 ( 0.176)\tLoss 1.3006e-01 (1.8628e-01)\tAcc@1  96.88 ( 94.09)\tAcc@5 100.00 ( 99.86)\n","Epoch: [66][120/391]\tTime  0.175 ( 0.176)\tLoss 1.5228e-01 (1.8724e-01)\tAcc@1  96.88 ( 94.07)\tAcc@5 100.00 ( 99.85)\n","Epoch: [66][150/391]\tTime  0.174 ( 0.176)\tLoss 2.3860e-01 (1.8628e-01)\tAcc@1  92.97 ( 94.18)\tAcc@5 100.00 ( 99.83)\n","Epoch: [66][180/391]\tTime  0.175 ( 0.176)\tLoss 1.6428e-01 (1.8824e-01)\tAcc@1  96.88 ( 94.11)\tAcc@5 100.00 ( 99.83)\n","Epoch: [66][210/391]\tTime  0.177 ( 0.176)\tLoss 2.0886e-01 (1.8955e-01)\tAcc@1  93.75 ( 94.09)\tAcc@5 100.00 ( 99.84)\n","Epoch: [66][240/391]\tTime  0.176 ( 0.175)\tLoss 2.6035e-01 (1.9164e-01)\tAcc@1  86.72 ( 94.00)\tAcc@5 100.00 ( 99.83)\n","Epoch: [66][270/391]\tTime  0.176 ( 0.175)\tLoss 3.0214e-01 (1.9352e-01)\tAcc@1  90.62 ( 93.97)\tAcc@5 100.00 ( 99.83)\n","Epoch: [66][300/391]\tTime  0.175 ( 0.175)\tLoss 2.1405e-01 (1.9588e-01)\tAcc@1  93.75 ( 93.89)\tAcc@5 100.00 ( 99.82)\n","Epoch: [66][330/391]\tTime  0.174 ( 0.175)\tLoss 2.3679e-01 (1.9742e-01)\tAcc@1  92.19 ( 93.85)\tAcc@5 100.00 ( 99.81)\n","Epoch: [66][360/391]\tTime  0.177 ( 0.175)\tLoss 2.9387e-01 (1.9990e-01)\tAcc@1  88.28 ( 93.74)\tAcc@5 100.00 ( 99.81)\n","Epoch: [66][390/391]\tTime  0.163 ( 0.175)\tLoss 4.0080e-01 (2.0181e-01)\tAcc@1  83.75 ( 93.67)\tAcc@5  98.75 ( 99.80)\n","==> Train Accuracy: Acc@1 93.670 || Acc@5 99.796\n","==> Test Accuracy:  Acc@1 73.070 || Acc@5 92.810\n","==> 72.93 seconds to train this epoch\n","\n","\n","----- epoch: 67, lr: 0.020000000000000004 -----\n","Epoch: [67][  0/391]\tTime  0.232 ( 0.232)\tLoss 1.6291e-01 (1.6291e-01)\tAcc@1  92.97 ( 92.97)\tAcc@5 100.00 (100.00)\n","Epoch: [67][ 30/391]\tTime  0.176 ( 0.177)\tLoss 1.9995e-01 (1.7789e-01)\tAcc@1  92.97 ( 94.13)\tAcc@5 100.00 ( 99.90)\n","Epoch: [67][ 60/391]\tTime  0.176 ( 0.176)\tLoss 1.8508e-01 (1.7819e-01)\tAcc@1  94.53 ( 94.42)\tAcc@5 100.00 ( 99.90)\n","Epoch: [67][ 90/391]\tTime  0.176 ( 0.176)\tLoss 2.6328e-01 (1.8375e-01)\tAcc@1  91.41 ( 94.28)\tAcc@5 100.00 ( 99.85)\n","Epoch: [67][120/391]\tTime  0.174 ( 0.176)\tLoss 2.5687e-01 (1.8652e-01)\tAcc@1  92.19 ( 94.21)\tAcc@5 100.00 ( 99.84)\n","Epoch: [67][150/391]\tTime  0.176 ( 0.176)\tLoss 2.1095e-01 (1.8850e-01)\tAcc@1  93.75 ( 94.10)\tAcc@5 100.00 ( 99.86)\n","Epoch: [67][180/391]\tTime  0.174 ( 0.176)\tLoss 2.2254e-01 (1.9028e-01)\tAcc@1  92.19 ( 94.02)\tAcc@5  99.22 ( 99.83)\n","Epoch: [67][210/391]\tTime  0.176 ( 0.176)\tLoss 2.2468e-01 (1.9329e-01)\tAcc@1  92.19 ( 94.01)\tAcc@5 100.00 ( 99.81)\n","Epoch: [67][240/391]\tTime  0.177 ( 0.176)\tLoss 2.0218e-01 (1.9821e-01)\tAcc@1  92.19 ( 93.81)\tAcc@5 100.00 ( 99.80)\n","Epoch: [67][270/391]\tTime  0.174 ( 0.176)\tLoss 1.6981e-01 (2.0290e-01)\tAcc@1  93.75 ( 93.65)\tAcc@5 100.00 ( 99.78)\n","Epoch: [67][300/391]\tTime  0.176 ( 0.176)\tLoss 2.2799e-01 (2.0468e-01)\tAcc@1  89.06 ( 93.54)\tAcc@5 100.00 ( 99.78)\n","Epoch: [67][330/391]\tTime  0.175 ( 0.176)\tLoss 1.7063e-01 (2.0594e-01)\tAcc@1  94.53 ( 93.50)\tAcc@5 100.00 ( 99.79)\n","Epoch: [67][360/391]\tTime  0.174 ( 0.176)\tLoss 1.7930e-01 (2.0817e-01)\tAcc@1  92.19 ( 93.42)\tAcc@5 100.00 ( 99.78)\n","Epoch: [67][390/391]\tTime  0.157 ( 0.175)\tLoss 2.6884e-01 (2.0984e-01)\tAcc@1  91.25 ( 93.39)\tAcc@5  98.75 ( 99.78)\n","==> Train Accuracy: Acc@1 93.388 || Acc@5 99.780\n","==> Test Accuracy:  Acc@1 72.280 || Acc@5 92.360\n","==> 72.96 seconds to train this epoch\n","\n","\n","----- epoch: 68, lr: 0.020000000000000004 -----\n","Epoch: [68][  0/391]\tTime  0.224 ( 0.224)\tLoss 2.0180e-01 (2.0180e-01)\tAcc@1  94.53 ( 94.53)\tAcc@5 100.00 (100.00)\n","Epoch: [68][ 30/391]\tTime  0.175 ( 0.177)\tLoss 1.9280e-01 (1.8164e-01)\tAcc@1  92.97 ( 94.63)\tAcc@5  99.22 ( 99.82)\n","Epoch: [68][ 60/391]\tTime  0.173 ( 0.176)\tLoss 2.0799e-01 (1.9035e-01)\tAcc@1  94.53 ( 94.39)\tAcc@5 100.00 ( 99.80)\n","Epoch: [68][ 90/391]\tTime  0.172 ( 0.176)\tLoss 2.4659e-01 (1.9256e-01)\tAcc@1  92.19 ( 94.12)\tAcc@5  99.22 ( 99.81)\n","Epoch: [68][120/391]\tTime  0.173 ( 0.176)\tLoss 2.1730e-01 (1.9177e-01)\tAcc@1  92.97 ( 94.15)\tAcc@5 100.00 ( 99.83)\n","Epoch: [68][150/391]\tTime  0.176 ( 0.175)\tLoss 1.6749e-01 (1.9375e-01)\tAcc@1  92.97 ( 94.08)\tAcc@5 100.00 ( 99.80)\n","Epoch: [68][180/391]\tTime  0.174 ( 0.175)\tLoss 2.0618e-01 (1.9690e-01)\tAcc@1  90.62 ( 93.93)\tAcc@5 100.00 ( 99.81)\n","Epoch: [68][210/391]\tTime  0.175 ( 0.175)\tLoss 3.1317e-01 (1.9886e-01)\tAcc@1  89.84 ( 93.83)\tAcc@5  99.22 ( 99.82)\n","Epoch: [68][240/391]\tTime  0.175 ( 0.175)\tLoss 1.6663e-01 (1.9666e-01)\tAcc@1  92.97 ( 93.87)\tAcc@5 100.00 ( 99.83)\n","Epoch: [68][270/391]\tTime  0.176 ( 0.175)\tLoss 2.1194e-01 (1.9540e-01)\tAcc@1  91.41 ( 93.94)\tAcc@5 100.00 ( 99.84)\n","Epoch: [68][300/391]\tTime  0.175 ( 0.175)\tLoss 1.7904e-01 (1.9708e-01)\tAcc@1  91.41 ( 93.85)\tAcc@5 100.00 ( 99.84)\n","Epoch: [68][330/391]\tTime  0.175 ( 0.175)\tLoss 2.0163e-01 (2.0019e-01)\tAcc@1  94.53 ( 93.73)\tAcc@5 100.00 ( 99.83)\n","Epoch: [68][360/391]\tTime  0.175 ( 0.175)\tLoss 2.2981e-01 (2.0330e-01)\tAcc@1  92.19 ( 93.62)\tAcc@5 100.00 ( 99.83)\n","Epoch: [68][390/391]\tTime  0.159 ( 0.175)\tLoss 3.0268e-01 (2.0588e-01)\tAcc@1  91.25 ( 93.54)\tAcc@5 100.00 ( 99.81)\n","==> Train Accuracy: Acc@1 93.542 || Acc@5 99.814\n","==> Test Accuracy:  Acc@1 71.460 || Acc@5 92.070\n","==> 72.89 seconds to train this epoch\n","\n","\n","----- epoch: 69, lr: 0.020000000000000004 -----\n","Epoch: [69][  0/391]\tTime  0.230 ( 0.230)\tLoss 2.6476e-01 (2.6476e-01)\tAcc@1  91.41 ( 91.41)\tAcc@5  99.22 ( 99.22)\n","Epoch: [69][ 30/391]\tTime  0.175 ( 0.177)\tLoss 1.4190e-01 (1.7682e-01)\tAcc@1  95.31 ( 94.53)\tAcc@5 100.00 ( 99.82)\n","Epoch: [69][ 60/391]\tTime  0.176 ( 0.176)\tLoss 2.5008e-01 (1.7775e-01)\tAcc@1  89.06 ( 94.52)\tAcc@5 100.00 ( 99.86)\n","Epoch: [69][ 90/391]\tTime  0.174 ( 0.176)\tLoss 1.8572e-01 (1.7878e-01)\tAcc@1  96.88 ( 94.55)\tAcc@5  99.22 ( 99.85)\n","Epoch: [69][120/391]\tTime  0.174 ( 0.176)\tLoss 2.4518e-01 (1.7994e-01)\tAcc@1  89.06 ( 94.43)\tAcc@5 100.00 ( 99.84)\n","Epoch: [69][150/391]\tTime  0.176 ( 0.176)\tLoss 2.9974e-01 (1.8203e-01)\tAcc@1  89.06 ( 94.35)\tAcc@5 100.00 ( 99.83)\n","Epoch: [69][180/391]\tTime  0.176 ( 0.175)\tLoss 2.6613e-01 (1.8833e-01)\tAcc@1  92.97 ( 94.12)\tAcc@5 100.00 ( 99.83)\n","Epoch: [69][210/391]\tTime  0.175 ( 0.175)\tLoss 2.0877e-01 (1.9155e-01)\tAcc@1  94.53 ( 94.04)\tAcc@5  99.22 ( 99.81)\n","Epoch: [69][240/391]\tTime  0.174 ( 0.175)\tLoss 1.8970e-01 (1.9414e-01)\tAcc@1  93.75 ( 93.95)\tAcc@5 100.00 ( 99.81)\n","Epoch: [69][270/391]\tTime  0.175 ( 0.175)\tLoss 1.0907e-01 (1.9504e-01)\tAcc@1  97.66 ( 93.91)\tAcc@5  99.22 ( 99.81)\n","Epoch: [69][300/391]\tTime  0.177 ( 0.175)\tLoss 2.2397e-01 (1.9700e-01)\tAcc@1  91.41 ( 93.87)\tAcc@5 100.00 ( 99.82)\n","Epoch: [69][330/391]\tTime  0.173 ( 0.175)\tLoss 2.8919e-01 (2.0051e-01)\tAcc@1  90.62 ( 93.78)\tAcc@5  98.44 ( 99.81)\n","Epoch: [69][360/391]\tTime  0.176 ( 0.175)\tLoss 2.7318e-01 (2.0442e-01)\tAcc@1  90.62 ( 93.63)\tAcc@5  99.22 ( 99.80)\n","Epoch: [69][390/391]\tTime  0.158 ( 0.175)\tLoss 2.7131e-01 (2.0585e-01)\tAcc@1  92.50 ( 93.57)\tAcc@5  98.75 ( 99.78)\n","==> Train Accuracy: Acc@1 93.566 || Acc@5 99.782\n","==> Test Accuracy:  Acc@1 72.380 || Acc@5 92.530\n","==> 72.89 seconds to train this epoch\n","\n","\n","----- epoch: 70, lr: 0.020000000000000004 -----\n","Epoch: [70][  0/391]\tTime  0.231 ( 0.231)\tLoss 1.3713e-01 (1.3713e-01)\tAcc@1  96.09 ( 96.09)\tAcc@5 100.00 (100.00)\n","Epoch: [70][ 30/391]\tTime  0.174 ( 0.177)\tLoss 1.5341e-01 (1.8930e-01)\tAcc@1  95.31 ( 94.10)\tAcc@5 100.00 ( 99.85)\n","Epoch: [70][ 60/391]\tTime  0.174 ( 0.176)\tLoss 1.9998e-01 (1.8929e-01)\tAcc@1  94.53 ( 94.20)\tAcc@5 100.00 ( 99.82)\n","Epoch: [70][ 90/391]\tTime  0.175 ( 0.176)\tLoss 1.4363e-01 (1.8988e-01)\tAcc@1  96.09 ( 94.17)\tAcc@5 100.00 ( 99.84)\n","Epoch: [70][120/391]\tTime  0.174 ( 0.176)\tLoss 1.8909e-01 (1.9429e-01)\tAcc@1  94.53 ( 94.00)\tAcc@5 100.00 ( 99.84)\n","Epoch: [70][150/391]\tTime  0.175 ( 0.176)\tLoss 2.3015e-01 (1.9722e-01)\tAcc@1  90.62 ( 93.82)\tAcc@5 100.00 ( 99.85)\n","Epoch: [70][180/391]\tTime  0.175 ( 0.175)\tLoss 2.0500e-01 (1.9756e-01)\tAcc@1  93.75 ( 93.83)\tAcc@5 100.00 ( 99.85)\n","Epoch: [70][210/391]\tTime  0.171 ( 0.175)\tLoss 2.8839e-01 (1.9910e-01)\tAcc@1  88.28 ( 93.76)\tAcc@5  99.22 ( 99.83)\n","Epoch: [70][240/391]\tTime  0.175 ( 0.175)\tLoss 1.7879e-01 (2.0125e-01)\tAcc@1  94.53 ( 93.69)\tAcc@5 100.00 ( 99.82)\n","Epoch: [70][270/391]\tTime  0.174 ( 0.175)\tLoss 1.7105e-01 (2.0452e-01)\tAcc@1  95.31 ( 93.57)\tAcc@5 100.00 ( 99.81)\n","Epoch: [70][300/391]\tTime  0.176 ( 0.175)\tLoss 2.7379e-01 (2.0846e-01)\tAcc@1  90.62 ( 93.44)\tAcc@5  99.22 ( 99.78)\n","Epoch: [70][330/391]\tTime  0.175 ( 0.175)\tLoss 3.0592e-01 (2.1350e-01)\tAcc@1  90.62 ( 93.26)\tAcc@5  99.22 ( 99.76)\n","Epoch: [70][360/391]\tTime  0.176 ( 0.175)\tLoss 2.5082e-01 (2.1670e-01)\tAcc@1  89.84 ( 93.14)\tAcc@5 100.00 ( 99.77)\n","Epoch: [70][390/391]\tTime  0.158 ( 0.175)\tLoss 2.3414e-01 (2.1853e-01)\tAcc@1  91.25 ( 93.09)\tAcc@5 100.00 ( 99.77)\n","==> Train Accuracy: Acc@1 93.092 || Acc@5 99.766\n","==> Test Accuracy:  Acc@1 72.450 || Acc@5 91.850\n","==> 72.88 seconds to train this epoch\n","\n","\n","----- epoch: 71, lr: 0.020000000000000004 -----\n","Epoch: [71][  0/391]\tTime  0.234 ( 0.234)\tLoss 1.4085e-01 (1.4085e-01)\tAcc@1  94.53 ( 94.53)\tAcc@5 100.00 (100.00)\n","Epoch: [71][ 30/391]\tTime  0.174 ( 0.177)\tLoss 1.3319e-01 (1.9039e-01)\tAcc@1  97.66 ( 93.90)\tAcc@5 100.00 ( 99.90)\n","Epoch: [71][ 60/391]\tTime  0.176 ( 0.176)\tLoss 2.0234e-01 (1.8716e-01)\tAcc@1  95.31 ( 94.15)\tAcc@5 100.00 ( 99.88)\n","Epoch: [71][ 90/391]\tTime  0.175 ( 0.176)\tLoss 2.0955e-01 (1.9195e-01)\tAcc@1  93.75 ( 94.07)\tAcc@5 100.00 ( 99.85)\n","Epoch: [71][120/391]\tTime  0.177 ( 0.176)\tLoss 2.6972e-01 (1.9355e-01)\tAcc@1  91.41 ( 94.06)\tAcc@5  99.22 ( 99.82)\n","Epoch: [71][150/391]\tTime  0.177 ( 0.176)\tLoss 2.8708e-01 (1.9774e-01)\tAcc@1  90.62 ( 93.88)\tAcc@5 100.00 ( 99.80)\n","Epoch: [71][180/391]\tTime  0.175 ( 0.175)\tLoss 2.6721e-01 (2.0148e-01)\tAcc@1  92.97 ( 93.75)\tAcc@5  99.22 ( 99.79)\n","Epoch: [71][210/391]\tTime  0.176 ( 0.175)\tLoss 1.9764e-01 (2.0064e-01)\tAcc@1  91.41 ( 93.78)\tAcc@5 100.00 ( 99.78)\n","Epoch: [71][240/391]\tTime  0.170 ( 0.175)\tLoss 1.9218e-01 (2.0203e-01)\tAcc@1  91.41 ( 93.68)\tAcc@5 100.00 ( 99.80)\n","Epoch: [71][270/391]\tTime  0.176 ( 0.175)\tLoss 2.7188e-01 (2.0475e-01)\tAcc@1  91.41 ( 93.60)\tAcc@5 100.00 ( 99.79)\n","Epoch: [71][300/391]\tTime  0.175 ( 0.175)\tLoss 2.4719e-01 (2.0802e-01)\tAcc@1  89.84 ( 93.47)\tAcc@5 100.00 ( 99.78)\n","Epoch: [71][330/391]\tTime  0.176 ( 0.175)\tLoss 1.6652e-01 (2.1126e-01)\tAcc@1  96.09 ( 93.32)\tAcc@5  98.44 ( 99.78)\n","Epoch: [71][360/391]\tTime  0.175 ( 0.175)\tLoss 2.6292e-01 (2.1399e-01)\tAcc@1  91.41 ( 93.23)\tAcc@5 100.00 ( 99.77)\n","Epoch: [71][390/391]\tTime  0.157 ( 0.175)\tLoss 4.6262e-01 (2.1788e-01)\tAcc@1  85.00 ( 93.10)\tAcc@5 100.00 ( 99.77)\n","==> Train Accuracy: Acc@1 93.096 || Acc@5 99.768\n","==> Test Accuracy:  Acc@1 71.360 || Acc@5 91.890\n","==> 72.83 seconds to train this epoch\n","\n","\n","----- epoch: 72, lr: 0.020000000000000004 -----\n","Epoch: [72][  0/391]\tTime  0.233 ( 0.233)\tLoss 2.2337e-01 (2.2337e-01)\tAcc@1  92.97 ( 92.97)\tAcc@5  98.44 ( 98.44)\n","Epoch: [72][ 30/391]\tTime  0.176 ( 0.176)\tLoss 1.7429e-01 (1.9862e-01)\tAcc@1  92.19 ( 93.95)\tAcc@5 100.00 ( 99.77)\n","Epoch: [72][ 60/391]\tTime  0.174 ( 0.176)\tLoss 1.9205e-01 (2.0107e-01)\tAcc@1  91.41 ( 93.81)\tAcc@5 100.00 ( 99.76)\n","Epoch: [72][ 90/391]\tTime  0.176 ( 0.176)\tLoss 2.2499e-01 (2.0369e-01)\tAcc@1  92.19 ( 93.64)\tAcc@5 100.00 ( 99.77)\n","Epoch: [72][120/391]\tTime  0.175 ( 0.176)\tLoss 2.1866e-01 (1.9945e-01)\tAcc@1  93.75 ( 93.87)\tAcc@5  99.22 ( 99.78)\n","Epoch: [72][150/391]\tTime  0.173 ( 0.175)\tLoss 2.1057e-01 (1.9897e-01)\tAcc@1  92.97 ( 93.86)\tAcc@5 100.00 ( 99.79)\n","Epoch: [72][180/391]\tTime  0.175 ( 0.175)\tLoss 1.3779e-01 (2.0083e-01)\tAcc@1  92.97 ( 93.83)\tAcc@5 100.00 ( 99.80)\n","Epoch: [72][210/391]\tTime  0.176 ( 0.175)\tLoss 2.8168e-01 (2.0377e-01)\tAcc@1  90.62 ( 93.70)\tAcc@5  99.22 ( 99.79)\n","Epoch: [72][240/391]\tTime  0.176 ( 0.175)\tLoss 3.1559e-01 (2.1092e-01)\tAcc@1  90.62 ( 93.43)\tAcc@5 100.00 ( 99.78)\n","Epoch: [72][270/391]\tTime  0.175 ( 0.175)\tLoss 2.0327e-01 (2.1611e-01)\tAcc@1  92.19 ( 93.31)\tAcc@5  99.22 ( 99.76)\n","Epoch: [72][300/391]\tTime  0.176 ( 0.175)\tLoss 2.4333e-01 (2.2073e-01)\tAcc@1  92.19 ( 93.17)\tAcc@5 100.00 ( 99.75)\n","Epoch: [72][330/391]\tTime  0.175 ( 0.175)\tLoss 1.6367e-01 (2.2420e-01)\tAcc@1  94.53 ( 93.07)\tAcc@5  99.22 ( 99.75)\n","Epoch: [72][360/391]\tTime  0.175 ( 0.175)\tLoss 1.9879e-01 (2.2787e-01)\tAcc@1  92.97 ( 92.92)\tAcc@5 100.00 ( 99.75)\n","Epoch: [72][390/391]\tTime  0.158 ( 0.175)\tLoss 2.2086e-01 (2.3173e-01)\tAcc@1  95.00 ( 92.79)\tAcc@5 100.00 ( 99.74)\n","==> Train Accuracy: Acc@1 92.794 || Acc@5 99.736\n","==> Test Accuracy:  Acc@1 71.000 || Acc@5 91.550\n","==> 72.85 seconds to train this epoch\n","\n","\n","----- epoch: 73, lr: 0.020000000000000004 -----\n","Epoch: [73][  0/391]\tTime  0.237 ( 0.237)\tLoss 2.3926e-01 (2.3926e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5 100.00 (100.00)\n","Epoch: [73][ 30/391]\tTime  0.176 ( 0.177)\tLoss 2.3672e-01 (2.1554e-01)\tAcc@1  92.19 ( 93.50)\tAcc@5 100.00 ( 99.85)\n","Epoch: [73][ 60/391]\tTime  0.176 ( 0.176)\tLoss 1.5499e-01 (1.9797e-01)\tAcc@1  96.09 ( 94.06)\tAcc@5 100.00 ( 99.87)\n","Epoch: [73][ 90/391]\tTime  0.176 ( 0.176)\tLoss 2.1856e-01 (2.0036e-01)\tAcc@1  90.62 ( 93.86)\tAcc@5 100.00 ( 99.85)\n","Epoch: [73][120/391]\tTime  0.177 ( 0.176)\tLoss 2.6183e-01 (2.0126e-01)\tAcc@1  89.84 ( 93.80)\tAcc@5 100.00 ( 99.85)\n","Epoch: [73][150/391]\tTime  0.174 ( 0.175)\tLoss 1.7743e-01 (2.0372e-01)\tAcc@1  95.31 ( 93.78)\tAcc@5 100.00 ( 99.87)\n","Epoch: [73][180/391]\tTime  0.177 ( 0.175)\tLoss 2.2758e-01 (2.0381e-01)\tAcc@1  92.19 ( 93.85)\tAcc@5 100.00 ( 99.84)\n","Epoch: [73][210/391]\tTime  0.175 ( 0.175)\tLoss 3.9408e-01 (2.1291e-01)\tAcc@1  89.06 ( 93.55)\tAcc@5 100.00 ( 99.83)\n","Epoch: [73][240/391]\tTime  0.174 ( 0.175)\tLoss 2.6510e-01 (2.1816e-01)\tAcc@1  92.19 ( 93.37)\tAcc@5  99.22 ( 99.79)\n","Epoch: [73][270/391]\tTime  0.174 ( 0.175)\tLoss 2.7250e-01 (2.2304e-01)\tAcc@1  93.75 ( 93.19)\tAcc@5 100.00 ( 99.77)\n","Epoch: [73][300/391]\tTime  0.175 ( 0.175)\tLoss 3.3207e-01 (2.2622e-01)\tAcc@1  89.06 ( 93.05)\tAcc@5  99.22 ( 99.76)\n","Epoch: [73][330/391]\tTime  0.174 ( 0.175)\tLoss 2.9747e-01 (2.2987e-01)\tAcc@1  91.41 ( 92.92)\tAcc@5  98.44 ( 99.73)\n","Epoch: [73][360/391]\tTime  0.175 ( 0.175)\tLoss 1.7492e-01 (2.3192e-01)\tAcc@1  94.53 ( 92.81)\tAcc@5 100.00 ( 99.73)\n","Epoch: [73][390/391]\tTime  0.158 ( 0.175)\tLoss 3.4650e-01 (2.3260e-01)\tAcc@1  86.25 ( 92.74)\tAcc@5 100.00 ( 99.73)\n","==> Train Accuracy: Acc@1 92.744 || Acc@5 99.728\n","==> Test Accuracy:  Acc@1 70.700 || Acc@5 91.740\n","==> 72.84 seconds to train this epoch\n","\n","\n","----- epoch: 74, lr: 0.020000000000000004 -----\n","Epoch: [74][  0/391]\tTime  0.219 ( 0.219)\tLoss 1.9581e-01 (1.9581e-01)\tAcc@1  94.53 ( 94.53)\tAcc@5 100.00 (100.00)\n","Epoch: [74][ 30/391]\tTime  0.174 ( 0.176)\tLoss 2.0870e-01 (2.1286e-01)\tAcc@1  92.97 ( 93.27)\tAcc@5 100.00 ( 99.80)\n","Epoch: [74][ 60/391]\tTime  0.176 ( 0.176)\tLoss 1.5987e-01 (1.9814e-01)\tAcc@1  93.75 ( 93.84)\tAcc@5 100.00 ( 99.86)\n","Epoch: [74][ 90/391]\tTime  0.177 ( 0.176)\tLoss 1.9877e-01 (2.0024e-01)\tAcc@1  92.19 ( 93.71)\tAcc@5 100.00 ( 99.87)\n","Epoch: [74][120/391]\tTime  0.174 ( 0.175)\tLoss 2.2371e-01 (1.9875e-01)\tAcc@1  91.41 ( 93.68)\tAcc@5 100.00 ( 99.86)\n","Epoch: [74][150/391]\tTime  0.175 ( 0.175)\tLoss 2.4264e-01 (1.9955e-01)\tAcc@1  91.41 ( 93.68)\tAcc@5 100.00 ( 99.86)\n","Epoch: [74][180/391]\tTime  0.175 ( 0.175)\tLoss 1.7845e-01 (2.0416e-01)\tAcc@1  95.31 ( 93.60)\tAcc@5 100.00 ( 99.85)\n","Epoch: [74][210/391]\tTime  0.174 ( 0.175)\tLoss 1.6649e-01 (2.0767e-01)\tAcc@1  95.31 ( 93.46)\tAcc@5 100.00 ( 99.81)\n","Epoch: [74][240/391]\tTime  0.176 ( 0.175)\tLoss 2.3503e-01 (2.0988e-01)\tAcc@1  92.19 ( 93.41)\tAcc@5 100.00 ( 99.79)\n","Epoch: [74][270/391]\tTime  0.175 ( 0.175)\tLoss 2.4307e-01 (2.1321e-01)\tAcc@1  93.75 ( 93.32)\tAcc@5 100.00 ( 99.78)\n","Epoch: [74][300/391]\tTime  0.175 ( 0.175)\tLoss 2.7218e-01 (2.1724e-01)\tAcc@1  93.75 ( 93.17)\tAcc@5 100.00 ( 99.77)\n","Epoch: [74][330/391]\tTime  0.174 ( 0.175)\tLoss 1.9625e-01 (2.2098e-01)\tAcc@1  93.75 ( 93.02)\tAcc@5  99.22 ( 99.77)\n","Epoch: [74][360/391]\tTime  0.176 ( 0.175)\tLoss 4.4038e-01 (2.2676e-01)\tAcc@1  86.72 ( 92.84)\tAcc@5  99.22 ( 99.74)\n","Epoch: [74][390/391]\tTime  0.159 ( 0.175)\tLoss 4.5436e-01 (2.3184e-01)\tAcc@1  86.25 ( 92.67)\tAcc@5  97.50 ( 99.71)\n","==> Train Accuracy: Acc@1 92.670 || Acc@5 99.714\n","==> Test Accuracy:  Acc@1 69.990 || Acc@5 90.460\n","==> 72.84 seconds to train this epoch\n","\n","\n","----- epoch: 75, lr: 0.020000000000000004 -----\n","Epoch: [75][  0/391]\tTime  0.226 ( 0.226)\tLoss 1.2686e-01 (1.2686e-01)\tAcc@1  96.88 ( 96.88)\tAcc@5 100.00 (100.00)\n","Epoch: [75][ 30/391]\tTime  0.175 ( 0.176)\tLoss 1.5889e-01 (2.1058e-01)\tAcc@1  94.53 ( 93.52)\tAcc@5  99.22 ( 99.80)\n","Epoch: [75][ 60/391]\tTime  0.174 ( 0.176)\tLoss 2.0909e-01 (2.2259e-01)\tAcc@1  94.53 ( 93.12)\tAcc@5 100.00 ( 99.83)\n","Epoch: [75][ 90/391]\tTime  0.175 ( 0.175)\tLoss 1.7275e-01 (2.2296e-01)\tAcc@1  93.75 ( 92.99)\tAcc@5 100.00 ( 99.83)\n","Epoch: [75][120/391]\tTime  0.177 ( 0.175)\tLoss 1.2987e-01 (2.1804e-01)\tAcc@1  96.09 ( 93.16)\tAcc@5 100.00 ( 99.81)\n","Epoch: [75][150/391]\tTime  0.175 ( 0.175)\tLoss 2.4816e-01 (2.1597e-01)\tAcc@1  90.62 ( 93.19)\tAcc@5 100.00 ( 99.82)\n","Epoch: [75][180/391]\tTime  0.175 ( 0.175)\tLoss 2.8653e-01 (2.1777e-01)\tAcc@1  91.41 ( 93.12)\tAcc@5 100.00 ( 99.83)\n","Epoch: [75][210/391]\tTime  0.177 ( 0.175)\tLoss 2.1221e-01 (2.2195e-01)\tAcc@1  93.75 ( 92.97)\tAcc@5 100.00 ( 99.80)\n","Epoch: [75][240/391]\tTime  0.175 ( 0.175)\tLoss 2.9197e-01 (2.2804e-01)\tAcc@1  89.84 ( 92.76)\tAcc@5 100.00 ( 99.79)\n","Epoch: [75][270/391]\tTime  0.175 ( 0.175)\tLoss 3.5455e-01 (2.3300e-01)\tAcc@1  89.84 ( 92.59)\tAcc@5  99.22 ( 99.75)\n","Epoch: [75][300/391]\tTime  0.177 ( 0.175)\tLoss 2.0464e-01 (2.3748e-01)\tAcc@1  95.31 ( 92.42)\tAcc@5 100.00 ( 99.74)\n","Epoch: [75][330/391]\tTime  0.175 ( 0.175)\tLoss 2.3275e-01 (2.4068e-01)\tAcc@1  90.62 ( 92.30)\tAcc@5  99.22 ( 99.72)\n","Epoch: [75][360/391]\tTime  0.175 ( 0.175)\tLoss 2.6945e-01 (2.4344e-01)\tAcc@1  92.97 ( 92.27)\tAcc@5  99.22 ( 99.70)\n","Epoch: [75][390/391]\tTime  0.159 ( 0.175)\tLoss 5.0908e-01 (2.4725e-01)\tAcc@1  81.25 ( 92.13)\tAcc@5  98.75 ( 99.69)\n","==> Train Accuracy: Acc@1 92.134 || Acc@5 99.694\n","==> Test Accuracy:  Acc@1 69.830 || Acc@5 91.120\n","==> 72.82 seconds to train this epoch\n","\n","\n","----- epoch: 76, lr: 0.020000000000000004 -----\n","Epoch: [76][  0/391]\tTime  0.227 ( 0.227)\tLoss 1.9058e-01 (1.9058e-01)\tAcc@1  94.53 ( 94.53)\tAcc@5 100.00 (100.00)\n","Epoch: [76][ 30/391]\tTime  0.175 ( 0.176)\tLoss 1.1568e-01 (2.1102e-01)\tAcc@1  96.88 ( 93.50)\tAcc@5 100.00 ( 99.90)\n","Epoch: [76][ 60/391]\tTime  0.174 ( 0.176)\tLoss 1.9363e-01 (2.0554e-01)\tAcc@1  95.31 ( 93.92)\tAcc@5 100.00 ( 99.82)\n","Epoch: [76][ 90/391]\tTime  0.176 ( 0.175)\tLoss 2.6586e-01 (2.0008e-01)\tAcc@1  90.62 ( 94.04)\tAcc@5 100.00 ( 99.87)\n","Epoch: [76][120/391]\tTime  0.176 ( 0.175)\tLoss 3.2566e-01 (2.0174e-01)\tAcc@1  89.06 ( 94.00)\tAcc@5  99.22 ( 99.86)\n","Epoch: [76][150/391]\tTime  0.177 ( 0.175)\tLoss 1.8328e-01 (2.1092e-01)\tAcc@1  92.97 ( 93.65)\tAcc@5 100.00 ( 99.80)\n","Epoch: [76][180/391]\tTime  0.175 ( 0.175)\tLoss 2.3722e-01 (2.1322e-01)\tAcc@1  90.62 ( 93.53)\tAcc@5 100.00 ( 99.81)\n","Epoch: [76][210/391]\tTime  0.177 ( 0.175)\tLoss 2.5041e-01 (2.1863e-01)\tAcc@1  92.97 ( 93.33)\tAcc@5  99.22 ( 99.79)\n","Epoch: [76][240/391]\tTime  0.176 ( 0.175)\tLoss 1.5126e-01 (2.2201e-01)\tAcc@1  96.88 ( 93.20)\tAcc@5 100.00 ( 99.78)\n","Epoch: [76][270/391]\tTime  0.178 ( 0.175)\tLoss 2.7669e-01 (2.2601e-01)\tAcc@1  92.97 ( 93.04)\tAcc@5 100.00 ( 99.77)\n","Epoch: [76][300/391]\tTime  0.175 ( 0.175)\tLoss 2.3875e-01 (2.2914e-01)\tAcc@1  92.97 ( 92.95)\tAcc@5  99.22 ( 99.76)\n","Epoch: [76][330/391]\tTime  0.175 ( 0.175)\tLoss 3.2624e-01 (2.3558e-01)\tAcc@1  89.06 ( 92.72)\tAcc@5 100.00 ( 99.74)\n","Epoch: [76][360/391]\tTime  0.176 ( 0.175)\tLoss 2.6390e-01 (2.3875e-01)\tAcc@1  87.50 ( 92.60)\tAcc@5 100.00 ( 99.74)\n","Epoch: [76][390/391]\tTime  0.157 ( 0.175)\tLoss 3.1144e-01 (2.4095e-01)\tAcc@1  90.00 ( 92.51)\tAcc@5  98.75 ( 99.73)\n","==> Train Accuracy: Acc@1 92.508 || Acc@5 99.732\n","==> Test Accuracy:  Acc@1 69.380 || Acc@5 91.110\n","==> 72.84 seconds to train this epoch\n","\n","\n","----- epoch: 77, lr: 0.020000000000000004 -----\n","Epoch: [77][  0/391]\tTime  0.229 ( 0.229)\tLoss 2.5925e-01 (2.5925e-01)\tAcc@1  92.19 ( 92.19)\tAcc@5  99.22 ( 99.22)\n","Epoch: [77][ 30/391]\tTime  0.174 ( 0.177)\tLoss 1.5319e-01 (2.1722e-01)\tAcc@1  96.09 ( 93.32)\tAcc@5 100.00 ( 99.87)\n","Epoch: [77][ 60/391]\tTime  0.179 ( 0.176)\tLoss 1.7430e-01 (2.0457e-01)\tAcc@1  93.75 ( 93.81)\tAcc@5 100.00 ( 99.85)\n","Epoch: [77][ 90/391]\tTime  0.176 ( 0.176)\tLoss 1.4901e-01 (2.0676e-01)\tAcc@1  97.66 ( 93.69)\tAcc@5 100.00 ( 99.83)\n","Epoch: [77][120/391]\tTime  0.181 ( 0.176)\tLoss 1.9936e-01 (2.0691e-01)\tAcc@1  93.75 ( 93.58)\tAcc@5 100.00 ( 99.83)\n","Epoch: [77][150/391]\tTime  0.175 ( 0.176)\tLoss 3.1709e-01 (2.1236e-01)\tAcc@1  91.41 ( 93.44)\tAcc@5 100.00 ( 99.83)\n","Epoch: [77][180/391]\tTime  0.175 ( 0.176)\tLoss 1.5940e-01 (2.1654e-01)\tAcc@1  96.09 ( 93.28)\tAcc@5 100.00 ( 99.83)\n","Epoch: [77][210/391]\tTime  0.174 ( 0.176)\tLoss 1.6283e-01 (2.1974e-01)\tAcc@1  93.75 ( 93.23)\tAcc@5 100.00 ( 99.83)\n","Epoch: [77][240/391]\tTime  0.176 ( 0.175)\tLoss 1.7876e-01 (2.2037e-01)\tAcc@1  92.19 ( 93.18)\tAcc@5 100.00 ( 99.83)\n","Epoch: [77][270/391]\tTime  0.176 ( 0.175)\tLoss 2.6483e-01 (2.2233e-01)\tAcc@1  91.41 ( 93.07)\tAcc@5 100.00 ( 99.82)\n","Epoch: [77][300/391]\tTime  0.175 ( 0.175)\tLoss 3.4313e-01 (2.2673e-01)\tAcc@1  90.62 ( 92.97)\tAcc@5  98.44 ( 99.80)\n","Epoch: [77][330/391]\tTime  0.176 ( 0.175)\tLoss 2.4283e-01 (2.2916e-01)\tAcc@1  90.62 ( 92.86)\tAcc@5 100.00 ( 99.79)\n","Epoch: [77][360/391]\tTime  0.176 ( 0.175)\tLoss 2.8941e-01 (2.3492e-01)\tAcc@1  89.84 ( 92.64)\tAcc@5  99.22 ( 99.79)\n","Epoch: [77][390/391]\tTime  0.158 ( 0.175)\tLoss 1.9628e-01 (2.4030e-01)\tAcc@1  93.75 ( 92.45)\tAcc@5 100.00 ( 99.76)\n","==> Train Accuracy: Acc@1 92.454 || Acc@5 99.764\n","==> Test Accuracy:  Acc@1 69.700 || Acc@5 91.090\n","==> 72.91 seconds to train this epoch\n","\n","\n","----- epoch: 78, lr: 0.020000000000000004 -----\n","Epoch: [78][  0/391]\tTime  0.227 ( 0.227)\tLoss 2.0309e-01 (2.0309e-01)\tAcc@1  92.97 ( 92.97)\tAcc@5  99.22 ( 99.22)\n","Epoch: [78][ 30/391]\tTime  0.176 ( 0.177)\tLoss 1.7062e-01 (2.3775e-01)\tAcc@1  95.31 ( 92.82)\tAcc@5 100.00 ( 99.62)\n","Epoch: [78][ 60/391]\tTime  0.175 ( 0.176)\tLoss 2.2684e-01 (2.3378e-01)\tAcc@1  91.41 ( 93.10)\tAcc@5 100.00 ( 99.65)\n","Epoch: [78][ 90/391]\tTime  0.175 ( 0.176)\tLoss 3.0300e-01 (2.3670e-01)\tAcc@1  91.41 ( 92.96)\tAcc@5  99.22 ( 99.71)\n","Epoch: [78][120/391]\tTime  0.174 ( 0.176)\tLoss 1.8449e-01 (2.3721e-01)\tAcc@1  92.19 ( 92.94)\tAcc@5 100.00 ( 99.70)\n","Epoch: [78][150/391]\tTime  0.176 ( 0.176)\tLoss 2.1306e-01 (2.3912e-01)\tAcc@1  95.31 ( 92.87)\tAcc@5 100.00 ( 99.68)\n","Epoch: [78][180/391]\tTime  0.175 ( 0.175)\tLoss 1.7985e-01 (2.4121e-01)\tAcc@1  92.97 ( 92.64)\tAcc@5 100.00 ( 99.70)\n","Epoch: [78][210/391]\tTime  0.174 ( 0.175)\tLoss 1.6983e-01 (2.4209e-01)\tAcc@1  93.75 ( 92.59)\tAcc@5  99.22 ( 99.69)\n","Epoch: [78][240/391]\tTime  0.176 ( 0.175)\tLoss 2.8418e-01 (2.4319e-01)\tAcc@1  92.19 ( 92.53)\tAcc@5  99.22 ( 99.69)\n","Epoch: [78][270/391]\tTime  0.174 ( 0.175)\tLoss 2.8786e-01 (2.4498e-01)\tAcc@1  90.62 ( 92.48)\tAcc@5  99.22 ( 99.69)\n","Epoch: [78][300/391]\tTime  0.179 ( 0.175)\tLoss 1.8887e-01 (2.4700e-01)\tAcc@1  95.31 ( 92.38)\tAcc@5 100.00 ( 99.68)\n","Epoch: [78][330/391]\tTime  0.176 ( 0.175)\tLoss 3.3819e-01 (2.4985e-01)\tAcc@1  89.06 ( 92.29)\tAcc@5 100.00 ( 99.67)\n","Epoch: [78][360/391]\tTime  0.176 ( 0.175)\tLoss 3.1253e-01 (2.5182e-01)\tAcc@1  91.41 ( 92.21)\tAcc@5  98.44 ( 99.66)\n","Epoch: [78][390/391]\tTime  0.159 ( 0.175)\tLoss 3.2582e-01 (2.5149e-01)\tAcc@1  87.50 ( 92.21)\tAcc@5 100.00 ( 99.65)\n","==> Train Accuracy: Acc@1 92.214 || Acc@5 99.654\n","==> Test Accuracy:  Acc@1 68.340 || Acc@5 89.960\n","==> 72.91 seconds to train this epoch\n","\n","\n","----- epoch: 79, lr: 0.020000000000000004 -----\n","Epoch: [79][  0/391]\tTime  0.231 ( 0.231)\tLoss 2.3666e-01 (2.3666e-01)\tAcc@1  90.62 ( 90.62)\tAcc@5 100.00 (100.00)\n","Epoch: [79][ 30/391]\tTime  0.176 ( 0.176)\tLoss 1.8244e-01 (2.0960e-01)\tAcc@1  94.53 ( 93.40)\tAcc@5  99.22 ( 99.62)\n","Epoch: [79][ 60/391]\tTime  0.176 ( 0.176)\tLoss 1.5598e-01 (2.0398e-01)\tAcc@1  95.31 ( 93.80)\tAcc@5  99.22 ( 99.76)\n","Epoch: [79][ 90/391]\tTime  0.175 ( 0.176)\tLoss 2.3514e-01 (2.0232e-01)\tAcc@1  93.75 ( 93.93)\tAcc@5 100.00 ( 99.79)\n","Epoch: [79][120/391]\tTime  0.175 ( 0.176)\tLoss 1.8828e-01 (2.0439e-01)\tAcc@1  92.19 ( 93.79)\tAcc@5 100.00 ( 99.80)\n","Epoch: [79][150/391]\tTime  0.175 ( 0.175)\tLoss 2.0833e-01 (2.1028e-01)\tAcc@1  92.19 ( 93.53)\tAcc@5 100.00 ( 99.78)\n","Epoch: [79][180/391]\tTime  0.176 ( 0.175)\tLoss 1.8904e-01 (2.1289e-01)\tAcc@1  93.75 ( 93.40)\tAcc@5 100.00 ( 99.76)\n","Epoch: [79][210/391]\tTime  0.175 ( 0.175)\tLoss 1.3173e-01 (2.1495e-01)\tAcc@1  96.88 ( 93.34)\tAcc@5 100.00 ( 99.76)\n","Epoch: [79][240/391]\tTime  0.174 ( 0.175)\tLoss 1.5055e-01 (2.1555e-01)\tAcc@1  93.75 ( 93.31)\tAcc@5 100.00 ( 99.76)\n","Epoch: [79][270/391]\tTime  0.176 ( 0.175)\tLoss 2.3690e-01 (2.2023e-01)\tAcc@1  92.97 ( 93.16)\tAcc@5 100.00 ( 99.73)\n","Epoch: [79][300/391]\tTime  0.176 ( 0.175)\tLoss 2.4093e-01 (2.2472e-01)\tAcc@1  89.06 ( 92.95)\tAcc@5  99.22 ( 99.72)\n","Epoch: [79][330/391]\tTime  0.175 ( 0.175)\tLoss 1.7487e-01 (2.2814e-01)\tAcc@1  94.53 ( 92.86)\tAcc@5 100.00 ( 99.71)\n","Epoch: [79][360/391]\tTime  0.177 ( 0.175)\tLoss 3.4939e-01 (2.3352e-01)\tAcc@1  89.84 ( 92.69)\tAcc@5  99.22 ( 99.70)\n","Epoch: [79][390/391]\tTime  0.157 ( 0.175)\tLoss 2.7007e-01 (2.3703e-01)\tAcc@1  92.50 ( 92.57)\tAcc@5  98.75 ( 99.69)\n","==> Train Accuracy: Acc@1 92.570 || Acc@5 99.694\n","==> Test Accuracy:  Acc@1 67.610 || Acc@5 90.140\n","==> 72.87 seconds to train this epoch\n","\n","\n","----- epoch: 80, lr: 0.020000000000000004 -----\n","Epoch: [80][  0/391]\tTime  0.232 ( 0.232)\tLoss 2.0047e-01 (2.0047e-01)\tAcc@1  92.97 ( 92.97)\tAcc@5  99.22 ( 99.22)\n","Epoch: [80][ 30/391]\tTime  0.176 ( 0.177)\tLoss 2.4260e-01 (2.3649e-01)\tAcc@1  89.84 ( 92.31)\tAcc@5 100.00 ( 99.75)\n","Epoch: [80][ 60/391]\tTime  0.175 ( 0.176)\tLoss 2.2889e-01 (2.2727e-01)\tAcc@1  92.19 ( 92.75)\tAcc@5 100.00 ( 99.78)\n","Epoch: [80][ 90/391]\tTime  0.175 ( 0.176)\tLoss 2.6028e-01 (2.3430e-01)\tAcc@1  91.41 ( 92.51)\tAcc@5 100.00 ( 99.76)\n","Epoch: [80][120/391]\tTime  0.177 ( 0.176)\tLoss 2.6556e-01 (2.3362e-01)\tAcc@1  91.41 ( 92.56)\tAcc@5 100.00 ( 99.78)\n","Epoch: [80][150/391]\tTime  0.175 ( 0.175)\tLoss 2.1600e-01 (2.3934e-01)\tAcc@1  94.53 ( 92.44)\tAcc@5 100.00 ( 99.79)\n","Epoch: [80][180/391]\tTime  0.175 ( 0.175)\tLoss 4.0046e-01 (2.4095e-01)\tAcc@1  84.38 ( 92.42)\tAcc@5 100.00 ( 99.77)\n","Epoch: [80][210/391]\tTime  0.176 ( 0.175)\tLoss 1.9028e-01 (2.4423e-01)\tAcc@1  94.53 ( 92.25)\tAcc@5 100.00 ( 99.77)\n","Epoch: [80][240/391]\tTime  0.176 ( 0.175)\tLoss 2.6597e-01 (2.4578e-01)\tAcc@1  89.06 ( 92.19)\tAcc@5 100.00 ( 99.75)\n","Epoch: [80][270/391]\tTime  0.175 ( 0.175)\tLoss 2.2823e-01 (2.4725e-01)\tAcc@1  92.97 ( 92.18)\tAcc@5 100.00 ( 99.73)\n","Epoch: [80][300/391]\tTime  0.174 ( 0.175)\tLoss 3.4749e-01 (2.4705e-01)\tAcc@1  88.28 ( 92.17)\tAcc@5  99.22 ( 99.74)\n","Epoch: [80][330/391]\tTime  0.175 ( 0.175)\tLoss 2.8956e-01 (2.5092e-01)\tAcc@1  89.84 ( 92.09)\tAcc@5  98.44 ( 99.71)\n","Epoch: [80][360/391]\tTime  0.175 ( 0.175)\tLoss 2.7357e-01 (2.5415e-01)\tAcc@1  90.62 ( 91.95)\tAcc@5 100.00 ( 99.70)\n","Epoch: [80][390/391]\tTime  0.159 ( 0.175)\tLoss 3.2743e-01 (2.5596e-01)\tAcc@1  88.75 ( 91.86)\tAcc@5 100.00 ( 99.70)\n","==> Train Accuracy: Acc@1 91.860 || Acc@5 99.696\n","==> Test Accuracy:  Acc@1 68.210 || Acc@5 89.870\n","==> 72.84 seconds to train this epoch\n","\n","\n","----- epoch: 81, lr: 0.020000000000000004 -----\n","Epoch: [81][  0/391]\tTime  0.223 ( 0.223)\tLoss 3.3799e-01 (3.3799e-01)\tAcc@1  88.28 ( 88.28)\tAcc@5 100.00 (100.00)\n","Epoch: [81][ 30/391]\tTime  0.175 ( 0.176)\tLoss 2.9818e-01 (2.2551e-01)\tAcc@1  91.41 ( 92.97)\tAcc@5 100.00 ( 99.75)\n","Epoch: [81][ 60/391]\tTime  0.176 ( 0.176)\tLoss 2.5398e-01 (2.1705e-01)\tAcc@1  91.41 ( 93.51)\tAcc@5 100.00 ( 99.77)\n","Epoch: [81][ 90/391]\tTime  0.174 ( 0.176)\tLoss 1.3791e-01 (2.1470e-01)\tAcc@1  96.88 ( 93.61)\tAcc@5 100.00 ( 99.81)\n","Epoch: [81][120/391]\tTime  0.175 ( 0.175)\tLoss 1.4847e-01 (2.1634e-01)\tAcc@1  92.97 ( 93.52)\tAcc@5 100.00 ( 99.78)\n","Epoch: [81][150/391]\tTime  0.176 ( 0.175)\tLoss 3.0359e-01 (2.2331e-01)\tAcc@1  89.06 ( 93.23)\tAcc@5  99.22 ( 99.78)\n","Epoch: [81][180/391]\tTime  0.174 ( 0.175)\tLoss 1.2581e-01 (2.2148e-01)\tAcc@1  96.09 ( 93.24)\tAcc@5 100.00 ( 99.78)\n","Epoch: [81][210/391]\tTime  0.175 ( 0.175)\tLoss 1.8951e-01 (2.2164e-01)\tAcc@1  95.31 ( 93.22)\tAcc@5 100.00 ( 99.79)\n","Epoch: [81][240/391]\tTime  0.175 ( 0.175)\tLoss 2.2250e-01 (2.2335e-01)\tAcc@1  94.53 ( 93.17)\tAcc@5 100.00 ( 99.79)\n","Epoch: [81][270/391]\tTime  0.176 ( 0.175)\tLoss 2.5707e-01 (2.2606e-01)\tAcc@1  94.53 ( 93.07)\tAcc@5  99.22 ( 99.76)\n","Epoch: [81][300/391]\tTime  0.174 ( 0.175)\tLoss 3.2530e-01 (2.3108e-01)\tAcc@1  89.84 ( 92.85)\tAcc@5  99.22 ( 99.74)\n","Epoch: [81][330/391]\tTime  0.174 ( 0.175)\tLoss 1.9148e-01 (2.3424e-01)\tAcc@1  93.75 ( 92.75)\tAcc@5 100.00 ( 99.73)\n","Epoch: [81][360/391]\tTime  0.174 ( 0.175)\tLoss 2.4818e-01 (2.3798e-01)\tAcc@1  89.84 ( 92.64)\tAcc@5 100.00 ( 99.72)\n","Epoch: [81][390/391]\tTime  0.153 ( 0.175)\tLoss 5.3603e-01 (2.4280e-01)\tAcc@1  78.75 ( 92.50)\tAcc@5  98.75 ( 99.69)\n","==> Train Accuracy: Acc@1 92.498 || Acc@5 99.694\n","==> Test Accuracy:  Acc@1 69.190 || Acc@5 90.100\n","==> 72.82 seconds to train this epoch\n","\n","\n","----- epoch: 82, lr: 0.020000000000000004 -----\n","Epoch: [82][  0/391]\tTime  0.245 ( 0.245)\tLoss 1.8182e-01 (1.8182e-01)\tAcc@1  92.97 ( 92.97)\tAcc@5 100.00 (100.00)\n","Epoch: [82][ 30/391]\tTime  0.174 ( 0.177)\tLoss 1.6420e-01 (2.2941e-01)\tAcc@1  96.88 ( 93.07)\tAcc@5 100.00 ( 99.72)\n","Epoch: [82][ 60/391]\tTime  0.173 ( 0.176)\tLoss 1.7472e-01 (2.3339e-01)\tAcc@1  96.09 ( 93.19)\tAcc@5 100.00 ( 99.68)\n","Epoch: [82][ 90/391]\tTime  0.175 ( 0.175)\tLoss 2.3399e-01 (2.3004e-01)\tAcc@1  92.19 ( 93.15)\tAcc@5 100.00 ( 99.72)\n","Epoch: [82][120/391]\tTime  0.175 ( 0.175)\tLoss 1.7282e-01 (2.2834e-01)\tAcc@1  96.09 ( 93.15)\tAcc@5 100.00 ( 99.74)\n","Epoch: [82][150/391]\tTime  0.175 ( 0.175)\tLoss 1.3574e-01 (2.2678e-01)\tAcc@1  97.66 ( 93.18)\tAcc@5 100.00 ( 99.76)\n","Epoch: [82][180/391]\tTime  0.176 ( 0.175)\tLoss 1.7301e-01 (2.2746e-01)\tAcc@1  92.19 ( 93.09)\tAcc@5 100.00 ( 99.75)\n","Epoch: [82][210/391]\tTime  0.174 ( 0.175)\tLoss 1.7881e-01 (2.2961e-01)\tAcc@1  95.31 ( 93.02)\tAcc@5 100.00 ( 99.75)\n","Epoch: [82][240/391]\tTime  0.175 ( 0.175)\tLoss 3.0572e-01 (2.3413e-01)\tAcc@1  93.75 ( 92.81)\tAcc@5  98.44 ( 99.74)\n","Epoch: [82][270/391]\tTime  0.174 ( 0.175)\tLoss 2.3030e-01 (2.3687e-01)\tAcc@1  94.53 ( 92.69)\tAcc@5 100.00 ( 99.71)\n","Epoch: [82][300/391]\tTime  0.175 ( 0.175)\tLoss 2.3693e-01 (2.3953e-01)\tAcc@1  91.41 ( 92.63)\tAcc@5 100.00 ( 99.70)\n","Epoch: [82][330/391]\tTime  0.175 ( 0.175)\tLoss 2.8753e-01 (2.4068e-01)\tAcc@1  91.41 ( 92.57)\tAcc@5 100.00 ( 99.70)\n","Epoch: [82][360/391]\tTime  0.175 ( 0.175)\tLoss 3.2716e-01 (2.4203e-01)\tAcc@1  90.62 ( 92.52)\tAcc@5 100.00 ( 99.70)\n","Epoch: [82][390/391]\tTime  0.158 ( 0.175)\tLoss 1.8232e-01 (2.4526e-01)\tAcc@1  92.50 ( 92.40)\tAcc@5 100.00 ( 99.69)\n","==> Train Accuracy: Acc@1 92.396 || Acc@5 99.692\n","==> Test Accuracy:  Acc@1 68.420 || Acc@5 90.720\n","==> 72.72 seconds to train this epoch\n","\n","\n","----- epoch: 83, lr: 0.020000000000000004 -----\n","Epoch: [83][  0/391]\tTime  0.223 ( 0.223)\tLoss 1.7745e-01 (1.7745e-01)\tAcc@1  95.31 ( 95.31)\tAcc@5  99.22 ( 99.22)\n","Epoch: [83][ 30/391]\tTime  0.174 ( 0.176)\tLoss 1.5346e-01 (2.2220e-01)\tAcc@1  96.88 ( 93.07)\tAcc@5 100.00 ( 99.75)\n","Epoch: [83][ 60/391]\tTime  0.175 ( 0.175)\tLoss 8.8773e-02 (2.1462e-01)\tAcc@1  98.44 ( 93.29)\tAcc@5 100.00 ( 99.76)\n","Epoch: [83][ 90/391]\tTime  0.176 ( 0.175)\tLoss 1.7318e-01 (2.1570e-01)\tAcc@1  93.75 ( 93.16)\tAcc@5 100.00 ( 99.78)\n","Epoch: [83][120/391]\tTime  0.174 ( 0.175)\tLoss 2.5764e-01 (2.1398e-01)\tAcc@1  90.62 ( 93.25)\tAcc@5 100.00 ( 99.80)\n","Epoch: [83][150/391]\tTime  0.176 ( 0.175)\tLoss 2.2392e-01 (2.1324e-01)\tAcc@1  92.19 ( 93.32)\tAcc@5 100.00 ( 99.82)\n","Epoch: [83][180/391]\tTime  0.175 ( 0.175)\tLoss 1.9975e-01 (2.1228e-01)\tAcc@1  94.53 ( 93.43)\tAcc@5 100.00 ( 99.82)\n","Epoch: [83][210/391]\tTime  0.171 ( 0.175)\tLoss 2.8309e-01 (2.1352e-01)\tAcc@1  92.19 ( 93.36)\tAcc@5 100.00 ( 99.81)\n","Epoch: [83][240/391]\tTime  0.174 ( 0.175)\tLoss 2.4666e-01 (2.1666e-01)\tAcc@1  92.97 ( 93.26)\tAcc@5 100.00 ( 99.81)\n","Epoch: [83][270/391]\tTime  0.175 ( 0.175)\tLoss 2.3409e-01 (2.2082e-01)\tAcc@1  91.41 ( 93.17)\tAcc@5 100.00 ( 99.80)\n","Epoch: [83][300/391]\tTime  0.174 ( 0.175)\tLoss 3.6398e-01 (2.2754e-01)\tAcc@1  89.84 ( 92.95)\tAcc@5 100.00 ( 99.77)\n","Epoch: [83][330/391]\tTime  0.176 ( 0.175)\tLoss 3.3410e-01 (2.3203e-01)\tAcc@1  91.41 ( 92.78)\tAcc@5  98.44 ( 99.75)\n","Epoch: [83][360/391]\tTime  0.175 ( 0.175)\tLoss 2.4719e-01 (2.3398e-01)\tAcc@1  92.19 ( 92.73)\tAcc@5  99.22 ( 99.76)\n","Epoch: [83][390/391]\tTime  0.158 ( 0.175)\tLoss 1.8745e-01 (2.3649e-01)\tAcc@1  96.25 ( 92.67)\tAcc@5 100.00 ( 99.75)\n","==> Train Accuracy: Acc@1 92.670 || Acc@5 99.754\n","==> Test Accuracy:  Acc@1 70.600 || Acc@5 91.120\n","==> 72.69 seconds to train this epoch\n","\n","\n","----- epoch: 84, lr: 0.020000000000000004 -----\n","Epoch: [84][  0/391]\tTime  0.228 ( 0.228)\tLoss 1.9765e-01 (1.9765e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5 100.00 (100.00)\n","Epoch: [84][ 30/391]\tTime  0.175 ( 0.176)\tLoss 2.7236e-01 (1.9696e-01)\tAcc@1  90.62 ( 93.80)\tAcc@5 100.00 ( 99.82)\n","Epoch: [84][ 60/391]\tTime  0.173 ( 0.176)\tLoss 2.3280e-01 (2.0403e-01)\tAcc@1  89.84 ( 93.76)\tAcc@5 100.00 ( 99.78)\n","Epoch: [84][ 90/391]\tTime  0.176 ( 0.175)\tLoss 2.4856e-01 (2.0810e-01)\tAcc@1  92.97 ( 93.63)\tAcc@5 100.00 ( 99.80)\n","Epoch: [84][120/391]\tTime  0.173 ( 0.175)\tLoss 2.0639e-01 (2.0539e-01)\tAcc@1  92.19 ( 93.71)\tAcc@5 100.00 ( 99.81)\n","Epoch: [84][150/391]\tTime  0.174 ( 0.175)\tLoss 2.8068e-01 (2.0901e-01)\tAcc@1  90.62 ( 93.50)\tAcc@5  99.22 ( 99.81)\n","Epoch: [84][180/391]\tTime  0.175 ( 0.175)\tLoss 2.0329e-01 (2.1410e-01)\tAcc@1  96.88 ( 93.37)\tAcc@5  98.44 ( 99.77)\n","Epoch: [84][210/391]\tTime  0.175 ( 0.175)\tLoss 2.0943e-01 (2.1843e-01)\tAcc@1  93.75 ( 93.21)\tAcc@5 100.00 ( 99.77)\n","Epoch: [84][240/391]\tTime  0.174 ( 0.175)\tLoss 2.2145e-01 (2.2416e-01)\tAcc@1  94.53 ( 93.01)\tAcc@5 100.00 ( 99.75)\n","Epoch: [84][270/391]\tTime  0.175 ( 0.175)\tLoss 1.8030e-01 (2.2830e-01)\tAcc@1  96.09 ( 92.90)\tAcc@5 100.00 ( 99.75)\n","Epoch: [84][300/391]\tTime  0.175 ( 0.175)\tLoss 2.3919e-01 (2.3153e-01)\tAcc@1  91.41 ( 92.77)\tAcc@5 100.00 ( 99.75)\n","Epoch: [84][330/391]\tTime  0.176 ( 0.175)\tLoss 2.5756e-01 (2.3412e-01)\tAcc@1  91.41 ( 92.67)\tAcc@5  98.44 ( 99.74)\n","Epoch: [84][360/391]\tTime  0.172 ( 0.175)\tLoss 2.7259e-01 (2.3658e-01)\tAcc@1  92.97 ( 92.61)\tAcc@5  98.44 ( 99.73)\n","Epoch: [84][390/391]\tTime  0.159 ( 0.175)\tLoss 3.7639e-01 (2.4170e-01)\tAcc@1  86.25 ( 92.41)\tAcc@5  98.75 ( 99.71)\n","==> Train Accuracy: Acc@1 92.414 || Acc@5 99.714\n","==> Test Accuracy:  Acc@1 69.730 || Acc@5 90.570\n","==> 72.72 seconds to train this epoch\n","\n","\n","----- epoch: 85, lr: 0.020000000000000004 -----\n","Epoch: [85][  0/391]\tTime  0.212 ( 0.212)\tLoss 1.5042e-01 (1.5042e-01)\tAcc@1  94.53 ( 94.53)\tAcc@5 100.00 (100.00)\n","Epoch: [85][ 30/391]\tTime  0.175 ( 0.176)\tLoss 2.1841e-01 (2.0113e-01)\tAcc@1  94.53 ( 93.80)\tAcc@5  99.22 ( 99.82)\n","Epoch: [85][ 60/391]\tTime  0.175 ( 0.175)\tLoss 1.9500e-01 (2.1136e-01)\tAcc@1  92.97 ( 93.55)\tAcc@5 100.00 ( 99.85)\n","Epoch: [85][ 90/391]\tTime  0.177 ( 0.175)\tLoss 1.9067e-01 (2.1601e-01)\tAcc@1  92.97 ( 93.36)\tAcc@5 100.00 ( 99.80)\n","Epoch: [85][120/391]\tTime  0.175 ( 0.175)\tLoss 1.4907e-01 (2.1770e-01)\tAcc@1  96.88 ( 93.39)\tAcc@5  99.22 ( 99.79)\n","Epoch: [85][150/391]\tTime  0.174 ( 0.175)\tLoss 2.3028e-01 (2.1797e-01)\tAcc@1  90.62 ( 93.36)\tAcc@5  99.22 ( 99.78)\n","Epoch: [85][180/391]\tTime  0.175 ( 0.175)\tLoss 1.4643e-01 (2.1765e-01)\tAcc@1  96.09 ( 93.35)\tAcc@5 100.00 ( 99.78)\n","Epoch: [85][210/391]\tTime  0.174 ( 0.175)\tLoss 2.5312e-01 (2.1943e-01)\tAcc@1  92.97 ( 93.26)\tAcc@5 100.00 ( 99.77)\n","Epoch: [85][240/391]\tTime  0.174 ( 0.175)\tLoss 2.0602e-01 (2.1980e-01)\tAcc@1  92.97 ( 93.26)\tAcc@5 100.00 ( 99.78)\n","Epoch: [85][270/391]\tTime  0.174 ( 0.175)\tLoss 3.8140e-01 (2.2507e-01)\tAcc@1  87.50 ( 93.12)\tAcc@5  99.22 ( 99.75)\n","Epoch: [85][300/391]\tTime  0.174 ( 0.175)\tLoss 2.7303e-01 (2.2816e-01)\tAcc@1  92.19 ( 92.96)\tAcc@5  98.44 ( 99.74)\n","Epoch: [85][330/391]\tTime  0.176 ( 0.175)\tLoss 3.0436e-01 (2.3045e-01)\tAcc@1  92.97 ( 92.94)\tAcc@5 100.00 ( 99.73)\n","Epoch: [85][360/391]\tTime  0.175 ( 0.175)\tLoss 2.4661e-01 (2.3313e-01)\tAcc@1  90.62 ( 92.84)\tAcc@5 100.00 ( 99.71)\n","Epoch: [85][390/391]\tTime  0.157 ( 0.175)\tLoss 1.5971e-01 (2.3614e-01)\tAcc@1  95.00 ( 92.73)\tAcc@5 100.00 ( 99.71)\n","==> Train Accuracy: Acc@1 92.734 || Acc@5 99.706\n","==> Test Accuracy:  Acc@1 69.330 || Acc@5 90.550\n","==> 72.72 seconds to train this epoch\n","\n","\n","----- epoch: 86, lr: 0.020000000000000004 -----\n","Epoch: [86][  0/391]\tTime  0.223 ( 0.223)\tLoss 1.6579e-01 (1.6579e-01)\tAcc@1  96.88 ( 96.88)\tAcc@5 100.00 (100.00)\n","Epoch: [86][ 30/391]\tTime  0.174 ( 0.176)\tLoss 1.6890e-01 (2.1945e-01)\tAcc@1  95.31 ( 93.42)\tAcc@5 100.00 ( 99.67)\n","Epoch: [86][ 60/391]\tTime  0.173 ( 0.175)\tLoss 2.0158e-01 (2.1076e-01)\tAcc@1  93.75 ( 93.57)\tAcc@5 100.00 ( 99.76)\n","Epoch: [86][ 90/391]\tTime  0.175 ( 0.175)\tLoss 1.5182e-01 (2.1080e-01)\tAcc@1  95.31 ( 93.62)\tAcc@5 100.00 ( 99.79)\n","Epoch: [86][120/391]\tTime  0.176 ( 0.175)\tLoss 2.1079e-01 (2.1276e-01)\tAcc@1  91.41 ( 93.53)\tAcc@5 100.00 ( 99.81)\n","Epoch: [86][150/391]\tTime  0.175 ( 0.175)\tLoss 2.7291e-01 (2.1357e-01)\tAcc@1  89.84 ( 93.48)\tAcc@5 100.00 ( 99.79)\n","Epoch: [86][180/391]\tTime  0.173 ( 0.175)\tLoss 1.6369e-01 (2.1271e-01)\tAcc@1  93.75 ( 93.46)\tAcc@5 100.00 ( 99.77)\n","Epoch: [86][210/391]\tTime  0.174 ( 0.175)\tLoss 1.6113e-01 (2.1116e-01)\tAcc@1  93.75 ( 93.49)\tAcc@5 100.00 ( 99.77)\n","Epoch: [86][240/391]\tTime  0.175 ( 0.175)\tLoss 2.5983e-01 (2.1387e-01)\tAcc@1  92.97 ( 93.33)\tAcc@5 100.00 ( 99.78)\n","Epoch: [86][270/391]\tTime  0.176 ( 0.175)\tLoss 3.1972e-01 (2.1821e-01)\tAcc@1  90.62 ( 93.16)\tAcc@5 100.00 ( 99.76)\n","Epoch: [86][300/391]\tTime  0.175 ( 0.175)\tLoss 2.0992e-01 (2.2130e-01)\tAcc@1  93.75 ( 93.07)\tAcc@5 100.00 ( 99.75)\n","Epoch: [86][330/391]\tTime  0.176 ( 0.175)\tLoss 2.7136e-01 (2.2434e-01)\tAcc@1  89.84 ( 92.92)\tAcc@5 100.00 ( 99.75)\n","Epoch: [86][360/391]\tTime  0.176 ( 0.175)\tLoss 2.4451e-01 (2.2898e-01)\tAcc@1  92.97 ( 92.74)\tAcc@5  99.22 ( 99.73)\n","Epoch: [86][390/391]\tTime  0.159 ( 0.175)\tLoss 2.9270e-01 (2.3267e-01)\tAcc@1  93.75 ( 92.59)\tAcc@5 100.00 ( 99.73)\n","==> Train Accuracy: Acc@1 92.594 || Acc@5 99.726\n","==> Test Accuracy:  Acc@1 69.260 || Acc@5 89.880\n","==> 72.74 seconds to train this epoch\n","\n","\n","----- epoch: 87, lr: 0.020000000000000004 -----\n","Epoch: [87][  0/391]\tTime  0.225 ( 0.225)\tLoss 1.7395e-01 (1.7395e-01)\tAcc@1  95.31 ( 95.31)\tAcc@5  99.22 ( 99.22)\n","Epoch: [87][ 30/391]\tTime  0.175 ( 0.176)\tLoss 2.1280e-01 (2.2251e-01)\tAcc@1  92.19 ( 92.62)\tAcc@5  99.22 ( 99.77)\n","Epoch: [87][ 60/391]\tTime  0.174 ( 0.176)\tLoss 1.9885e-01 (2.0714e-01)\tAcc@1  93.75 ( 93.40)\tAcc@5 100.00 ( 99.81)\n","Epoch: [87][ 90/391]\tTime  0.176 ( 0.175)\tLoss 1.2348e-01 (2.0558e-01)\tAcc@1  97.66 ( 93.62)\tAcc@5 100.00 ( 99.79)\n","Epoch: [87][120/391]\tTime  0.175 ( 0.175)\tLoss 2.0804e-01 (2.0477e-01)\tAcc@1  90.62 ( 93.65)\tAcc@5 100.00 ( 99.79)\n","Epoch: [87][150/391]\tTime  0.175 ( 0.175)\tLoss 2.0303e-01 (2.0459e-01)\tAcc@1  94.53 ( 93.71)\tAcc@5 100.00 ( 99.78)\n","Epoch: [87][180/391]\tTime  0.176 ( 0.175)\tLoss 1.7276e-01 (2.0475e-01)\tAcc@1  96.09 ( 93.69)\tAcc@5 100.00 ( 99.79)\n","Epoch: [87][210/391]\tTime  0.178 ( 0.175)\tLoss 2.4488e-01 (2.0862e-01)\tAcc@1  93.75 ( 93.58)\tAcc@5 100.00 ( 99.77)\n","Epoch: [87][240/391]\tTime  0.174 ( 0.175)\tLoss 2.4836e-01 (2.1445e-01)\tAcc@1  92.97 ( 93.41)\tAcc@5  99.22 ( 99.77)\n","Epoch: [87][270/391]\tTime  0.177 ( 0.175)\tLoss 2.5532e-01 (2.1910e-01)\tAcc@1  92.19 ( 93.21)\tAcc@5  99.22 ( 99.76)\n","Epoch: [87][300/391]\tTime  0.176 ( 0.175)\tLoss 2.1453e-01 (2.2270e-01)\tAcc@1  92.19 ( 93.03)\tAcc@5 100.00 ( 99.76)\n","Epoch: [87][330/391]\tTime  0.176 ( 0.175)\tLoss 2.0060e-01 (2.2599e-01)\tAcc@1  92.97 ( 92.94)\tAcc@5  99.22 ( 99.75)\n","Epoch: [87][360/391]\tTime  0.175 ( 0.175)\tLoss 2.3783e-01 (2.2853e-01)\tAcc@1  92.97 ( 92.84)\tAcc@5 100.00 ( 99.76)\n","Epoch: [87][390/391]\tTime  0.157 ( 0.175)\tLoss 4.5510e-01 (2.3392e-01)\tAcc@1  87.50 ( 92.66)\tAcc@5 100.00 ( 99.75)\n","==> Train Accuracy: Acc@1 92.658 || Acc@5 99.748\n","==> Test Accuracy:  Acc@1 68.260 || Acc@5 89.870\n","==> 72.79 seconds to train this epoch\n","\n","\n","----- epoch: 88, lr: 0.020000000000000004 -----\n","Epoch: [88][  0/391]\tTime  0.211 ( 0.211)\tLoss 2.6216e-01 (2.6216e-01)\tAcc@1  90.62 ( 90.62)\tAcc@5 100.00 (100.00)\n","Epoch: [88][ 30/391]\tTime  0.176 ( 0.176)\tLoss 2.1974e-01 (2.3100e-01)\tAcc@1  92.19 ( 92.57)\tAcc@5 100.00 ( 99.75)\n","Epoch: [88][ 60/391]\tTime  0.175 ( 0.175)\tLoss 2.8841e-01 (2.2373e-01)\tAcc@1  89.84 ( 92.88)\tAcc@5 100.00 ( 99.82)\n","Epoch: [88][ 90/391]\tTime  0.174 ( 0.175)\tLoss 1.0051e-01 (2.1281e-01)\tAcc@1  98.44 ( 93.45)\tAcc@5 100.00 ( 99.82)\n","Epoch: [88][120/391]\tTime  0.174 ( 0.175)\tLoss 2.0313e-01 (2.1170e-01)\tAcc@1  90.62 ( 93.32)\tAcc@5 100.00 ( 99.81)\n","Epoch: [88][150/391]\tTime  0.173 ( 0.175)\tLoss 2.2760e-01 (2.1088e-01)\tAcc@1  91.41 ( 93.38)\tAcc@5 100.00 ( 99.82)\n","Epoch: [88][180/391]\tTime  0.174 ( 0.175)\tLoss 2.7182e-01 (2.1512e-01)\tAcc@1  90.62 ( 93.27)\tAcc@5 100.00 ( 99.79)\n","Epoch: [88][210/391]\tTime  0.177 ( 0.175)\tLoss 3.4454e-01 (2.1928e-01)\tAcc@1  89.84 ( 93.13)\tAcc@5  99.22 ( 99.77)\n","Epoch: [88][240/391]\tTime  0.175 ( 0.175)\tLoss 2.6086e-01 (2.2440e-01)\tAcc@1  92.19 ( 92.92)\tAcc@5 100.00 ( 99.77)\n","Epoch: [88][270/391]\tTime  0.175 ( 0.175)\tLoss 2.6470e-01 (2.2827e-01)\tAcc@1  92.19 ( 92.82)\tAcc@5 100.00 ( 99.76)\n","Epoch: [88][300/391]\tTime  0.176 ( 0.175)\tLoss 2.7296e-01 (2.3278e-01)\tAcc@1  90.62 ( 92.65)\tAcc@5 100.00 ( 99.75)\n","Epoch: [88][330/391]\tTime  0.172 ( 0.175)\tLoss 3.1693e-01 (2.3369e-01)\tAcc@1  89.84 ( 92.59)\tAcc@5 100.00 ( 99.75)\n","Epoch: [88][360/391]\tTime  0.175 ( 0.175)\tLoss 2.4797e-01 (2.3511e-01)\tAcc@1  90.62 ( 92.53)\tAcc@5 100.00 ( 99.74)\n","Epoch: [88][390/391]\tTime  0.157 ( 0.175)\tLoss 3.0117e-01 (2.3672e-01)\tAcc@1  91.25 ( 92.49)\tAcc@5 100.00 ( 99.73)\n","==> Train Accuracy: Acc@1 92.488 || Acc@5 99.732\n","==> Test Accuracy:  Acc@1 68.630 || Acc@5 90.110\n","==> 72.75 seconds to train this epoch\n","\n","\n","----- epoch: 89, lr: 0.020000000000000004 -----\n","Epoch: [89][  0/391]\tTime  0.229 ( 0.229)\tLoss 9.2913e-02 (9.2913e-02)\tAcc@1  98.44 ( 98.44)\tAcc@5 100.00 (100.00)\n","Epoch: [89][ 30/391]\tTime  0.175 ( 0.176)\tLoss 2.3589e-01 (1.9918e-01)\tAcc@1  93.75 ( 93.75)\tAcc@5  99.22 ( 99.85)\n","Epoch: [89][ 60/391]\tTime  0.176 ( 0.176)\tLoss 2.4394e-01 (2.0088e-01)\tAcc@1  89.06 ( 93.66)\tAcc@5 100.00 ( 99.82)\n","Epoch: [89][ 90/391]\tTime  0.176 ( 0.176)\tLoss 1.5593e-01 (1.9787e-01)\tAcc@1  96.88 ( 93.84)\tAcc@5 100.00 ( 99.82)\n","Epoch: [89][120/391]\tTime  0.174 ( 0.175)\tLoss 1.3268e-01 (1.9950e-01)\tAcc@1  95.31 ( 93.85)\tAcc@5 100.00 ( 99.83)\n","Epoch: [89][150/391]\tTime  0.176 ( 0.175)\tLoss 3.0512e-01 (2.0846e-01)\tAcc@1  90.62 ( 93.54)\tAcc@5 100.00 ( 99.81)\n","Epoch: [89][180/391]\tTime  0.175 ( 0.175)\tLoss 1.9359e-01 (2.1382e-01)\tAcc@1  93.75 ( 93.40)\tAcc@5  99.22 ( 99.78)\n","Epoch: [89][210/391]\tTime  0.174 ( 0.175)\tLoss 2.7095e-01 (2.1609e-01)\tAcc@1  90.62 ( 93.34)\tAcc@5  99.22 ( 99.77)\n","Epoch: [89][240/391]\tTime  0.175 ( 0.175)\tLoss 1.7457e-01 (2.1761e-01)\tAcc@1  92.97 ( 93.28)\tAcc@5 100.00 ( 99.77)\n","Epoch: [89][270/391]\tTime  0.176 ( 0.175)\tLoss 2.6068e-01 (2.2014e-01)\tAcc@1  92.19 ( 93.20)\tAcc@5 100.00 ( 99.76)\n","Epoch: [89][300/391]\tTime  0.175 ( 0.175)\tLoss 1.7956e-01 (2.2258e-01)\tAcc@1  96.88 ( 93.08)\tAcc@5 100.00 ( 99.75)\n","Epoch: [89][330/391]\tTime  0.175 ( 0.175)\tLoss 2.8971e-01 (2.2729e-01)\tAcc@1  91.41 ( 92.97)\tAcc@5  99.22 ( 99.74)\n","Epoch: [89][360/391]\tTime  0.175 ( 0.175)\tLoss 3.2653e-01 (2.3178e-01)\tAcc@1  92.97 ( 92.82)\tAcc@5 100.00 ( 99.73)\n","Epoch: [89][390/391]\tTime  0.158 ( 0.175)\tLoss 2.4299e-01 (2.3655e-01)\tAcc@1  95.00 ( 92.65)\tAcc@5  98.75 ( 99.72)\n","==> Train Accuracy: Acc@1 92.652 || Acc@5 99.716\n","==> Test Accuracy:  Acc@1 68.430 || Acc@5 90.360\n","==> 72.84 seconds to train this epoch\n","\n","\n","----- epoch: 90, lr: 0.004000000000000001 -----\n","Epoch: [90][  0/391]\tTime  0.230 ( 0.230)\tLoss 1.8887e-01 (1.8887e-01)\tAcc@1  95.31 ( 95.31)\tAcc@5 100.00 (100.00)\n","Epoch: [90][ 30/391]\tTime  0.176 ( 0.177)\tLoss 1.3896e-01 (1.8020e-01)\tAcc@1  96.09 ( 94.46)\tAcc@5 100.00 ( 99.87)\n","Epoch: [90][ 60/391]\tTime  0.175 ( 0.176)\tLoss 8.1181e-02 (1.5212e-01)\tAcc@1  97.66 ( 95.65)\tAcc@5 100.00 ( 99.90)\n","Epoch: [90][ 90/391]\tTime  0.176 ( 0.176)\tLoss 8.9070e-02 (1.3544e-01)\tAcc@1  97.66 ( 96.21)\tAcc@5 100.00 ( 99.92)\n","Epoch: [90][120/391]\tTime  0.175 ( 0.176)\tLoss 8.8007e-02 (1.2743e-01)\tAcc@1  98.44 ( 96.44)\tAcc@5 100.00 ( 99.94)\n","Epoch: [90][150/391]\tTime  0.174 ( 0.175)\tLoss 8.6014e-02 (1.2030e-01)\tAcc@1  98.44 ( 96.66)\tAcc@5 100.00 ( 99.95)\n","Epoch: [90][180/391]\tTime  0.175 ( 0.175)\tLoss 6.0330e-02 (1.1504e-01)\tAcc@1  98.44 ( 96.81)\tAcc@5 100.00 ( 99.95)\n","Epoch: [90][210/391]\tTime  0.174 ( 0.175)\tLoss 4.6759e-02 (1.0942e-01)\tAcc@1  99.22 ( 97.00)\tAcc@5 100.00 ( 99.96)\n","Epoch: [90][240/391]\tTime  0.175 ( 0.175)\tLoss 5.5692e-02 (1.0569e-01)\tAcc@1  97.66 ( 97.11)\tAcc@5 100.00 ( 99.96)\n","Epoch: [90][270/391]\tTime  0.176 ( 0.175)\tLoss 4.3465e-02 (1.0219e-01)\tAcc@1 100.00 ( 97.22)\tAcc@5 100.00 ( 99.96)\n","Epoch: [90][300/391]\tTime  0.176 ( 0.175)\tLoss 6.4837e-02 (9.8453e-02)\tAcc@1  98.44 ( 97.33)\tAcc@5 100.00 ( 99.96)\n","Epoch: [90][330/391]\tTime  0.176 ( 0.175)\tLoss 7.3346e-02 (9.6170e-02)\tAcc@1  98.44 ( 97.41)\tAcc@5 100.00 ( 99.97)\n","Epoch: [90][360/391]\tTime  0.173 ( 0.175)\tLoss 4.2509e-02 (9.3736e-02)\tAcc@1  99.22 ( 97.49)\tAcc@5 100.00 ( 99.97)\n","Epoch: [90][390/391]\tTime  0.158 ( 0.175)\tLoss 8.3624e-02 (9.1729e-02)\tAcc@1  96.25 ( 97.53)\tAcc@5 100.00 ( 99.97)\n","==> Train Accuracy: Acc@1 97.534 || Acc@5 99.972\n","==> Test Accuracy:  Acc@1 75.660 || Acc@5 93.600\n","==> 72.85 seconds to train this epoch\n","\n","\n","----- epoch: 91, lr: 0.004000000000000001 -----\n","Epoch: [91][  0/391]\tTime  0.237 ( 0.237)\tLoss 3.0785e-02 (3.0785e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [91][ 30/391]\tTime  0.181 ( 0.177)\tLoss 3.9886e-02 (4.9177e-02)\tAcc@1  98.44 ( 99.02)\tAcc@5 100.00 ( 99.97)\n","Epoch: [91][ 60/391]\tTime  0.172 ( 0.176)\tLoss 5.0359e-02 (5.2278e-02)\tAcc@1  99.22 ( 98.86)\tAcc@5 100.00 ( 99.99)\n","Epoch: [91][ 90/391]\tTime  0.176 ( 0.176)\tLoss 4.0987e-02 (4.8325e-02)\tAcc@1  99.22 ( 99.02)\tAcc@5 100.00 ( 99.99)\n","Epoch: [91][120/391]\tTime  0.176 ( 0.175)\tLoss 4.3611e-02 (4.8075e-02)\tAcc@1  99.22 ( 99.06)\tAcc@5 100.00 ( 99.99)\n","Epoch: [91][150/391]\tTime  0.175 ( 0.175)\tLoss 2.6315e-02 (4.6164e-02)\tAcc@1 100.00 ( 99.12)\tAcc@5 100.00 ( 99.99)\n","Epoch: [91][180/391]\tTime  0.175 ( 0.175)\tLoss 5.2035e-02 (4.6026e-02)\tAcc@1  99.22 ( 99.12)\tAcc@5 100.00 (100.00)\n","Epoch: [91][210/391]\tTime  0.175 ( 0.175)\tLoss 1.9229e-02 (4.5449e-02)\tAcc@1 100.00 ( 99.12)\tAcc@5 100.00 (100.00)\n","Epoch: [91][240/391]\tTime  0.174 ( 0.175)\tLoss 4.4649e-02 (4.5563e-02)\tAcc@1  99.22 ( 99.10)\tAcc@5 100.00 (100.00)\n","Epoch: [91][270/391]\tTime  0.175 ( 0.175)\tLoss 2.1820e-02 (4.5637e-02)\tAcc@1 100.00 ( 99.08)\tAcc@5 100.00 (100.00)\n","Epoch: [91][300/391]\tTime  0.175 ( 0.175)\tLoss 4.1432e-02 (4.5230e-02)\tAcc@1  99.22 ( 99.07)\tAcc@5 100.00 (100.00)\n","Epoch: [91][330/391]\tTime  0.175 ( 0.175)\tLoss 4.5466e-02 (4.4693e-02)\tAcc@1 100.00 ( 99.09)\tAcc@5 100.00 (100.00)\n","Epoch: [91][360/391]\tTime  0.175 ( 0.175)\tLoss 2.5600e-02 (4.4573e-02)\tAcc@1  99.22 ( 99.09)\tAcc@5 100.00 (100.00)\n","Epoch: [91][390/391]\tTime  0.158 ( 0.175)\tLoss 2.5583e-02 (4.4615e-02)\tAcc@1 100.00 ( 99.10)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.096 || Acc@5 99.996\n","==> Test Accuracy:  Acc@1 75.980 || Acc@5 93.700\n","==> 72.81 seconds to train this epoch\n","\n","\n","----- epoch: 92, lr: 0.004000000000000001 -----\n","Epoch: [92][  0/391]\tTime  0.236 ( 0.236)\tLoss 1.1287e-02 (1.1287e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [92][ 30/391]\tTime  0.176 ( 0.177)\tLoss 2.8954e-02 (3.0967e-02)\tAcc@1 100.00 ( 99.50)\tAcc@5 100.00 (100.00)\n","Epoch: [92][ 60/391]\tTime  0.168 ( 0.176)\tLoss 3.0448e-02 (3.2735e-02)\tAcc@1 100.00 ( 99.44)\tAcc@5 100.00 ( 99.99)\n","Epoch: [92][ 90/391]\tTime  0.173 ( 0.176)\tLoss 2.4250e-02 (3.2949e-02)\tAcc@1 100.00 ( 99.41)\tAcc@5 100.00 ( 99.99)\n","Epoch: [92][120/391]\tTime  0.177 ( 0.175)\tLoss 2.3720e-02 (3.2196e-02)\tAcc@1  99.22 ( 99.43)\tAcc@5 100.00 ( 99.99)\n","Epoch: [92][150/391]\tTime  0.175 ( 0.175)\tLoss 2.3596e-02 (3.2936e-02)\tAcc@1 100.00 ( 99.38)\tAcc@5 100.00 ( 99.99)\n","Epoch: [92][180/391]\tTime  0.175 ( 0.175)\tLoss 2.7391e-02 (3.2979e-02)\tAcc@1  99.22 ( 99.39)\tAcc@5 100.00 (100.00)\n","Epoch: [92][210/391]\tTime  0.174 ( 0.175)\tLoss 2.7264e-02 (3.2761e-02)\tAcc@1 100.00 ( 99.40)\tAcc@5 100.00 (100.00)\n","Epoch: [92][240/391]\tTime  0.176 ( 0.175)\tLoss 2.2041e-02 (3.2522e-02)\tAcc@1 100.00 ( 99.40)\tAcc@5 100.00 (100.00)\n","Epoch: [92][270/391]\tTime  0.175 ( 0.175)\tLoss 1.8453e-02 (3.2527e-02)\tAcc@1  99.22 ( 99.39)\tAcc@5 100.00 (100.00)\n","Epoch: [92][300/391]\tTime  0.175 ( 0.175)\tLoss 3.4202e-02 (3.2587e-02)\tAcc@1  98.44 ( 99.39)\tAcc@5 100.00 (100.00)\n","Epoch: [92][330/391]\tTime  0.176 ( 0.175)\tLoss 1.5664e-02 (3.2407e-02)\tAcc@1 100.00 ( 99.40)\tAcc@5 100.00 (100.00)\n","Epoch: [92][360/391]\tTime  0.176 ( 0.175)\tLoss 3.7813e-02 (3.2602e-02)\tAcc@1  97.66 ( 99.38)\tAcc@5 100.00 (100.00)\n","Epoch: [92][390/391]\tTime  0.160 ( 0.175)\tLoss 4.7899e-02 (3.2419e-02)\tAcc@1 100.00 ( 99.37)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.374 || Acc@5 99.996\n","==> Test Accuracy:  Acc@1 75.860 || Acc@5 93.610\n","==> 72.81 seconds to train this epoch\n","\n","\n","----- epoch: 93, lr: 0.004000000000000001 -----\n","Epoch: [93][  0/391]\tTime  0.230 ( 0.230)\tLoss 3.7954e-02 (3.7954e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5 100.00 (100.00)\n","Epoch: [93][ 30/391]\tTime  0.179 ( 0.177)\tLoss 3.9939e-02 (2.9945e-02)\tAcc@1  98.44 ( 99.52)\tAcc@5 100.00 (100.00)\n","Epoch: [93][ 60/391]\tTime  0.175 ( 0.176)\tLoss 1.9867e-02 (2.8662e-02)\tAcc@1 100.00 ( 99.49)\tAcc@5 100.00 (100.00)\n","Epoch: [93][ 90/391]\tTime  0.177 ( 0.176)\tLoss 1.4812e-02 (2.7439e-02)\tAcc@1 100.00 ( 99.51)\tAcc@5 100.00 (100.00)\n","Epoch: [93][120/391]\tTime  0.175 ( 0.176)\tLoss 1.7221e-02 (2.7788e-02)\tAcc@1 100.00 ( 99.52)\tAcc@5 100.00 (100.00)\n","Epoch: [93][150/391]\tTime  0.175 ( 0.175)\tLoss 1.9739e-02 (2.7341e-02)\tAcc@1 100.00 ( 99.54)\tAcc@5 100.00 (100.00)\n","Epoch: [93][180/391]\tTime  0.174 ( 0.175)\tLoss 2.6284e-02 (2.7442e-02)\tAcc@1 100.00 ( 99.54)\tAcc@5 100.00 (100.00)\n","Epoch: [93][210/391]\tTime  0.174 ( 0.175)\tLoss 6.4514e-02 (2.7763e-02)\tAcc@1  97.66 ( 99.55)\tAcc@5 100.00 (100.00)\n","Epoch: [93][240/391]\tTime  0.175 ( 0.175)\tLoss 2.2173e-02 (2.7472e-02)\tAcc@1 100.00 ( 99.56)\tAcc@5 100.00 (100.00)\n","Epoch: [93][270/391]\tTime  0.175 ( 0.175)\tLoss 1.7356e-02 (2.7812e-02)\tAcc@1 100.00 ( 99.54)\tAcc@5 100.00 (100.00)\n","Epoch: [93][300/391]\tTime  0.176 ( 0.175)\tLoss 1.5443e-02 (2.7797e-02)\tAcc@1 100.00 ( 99.54)\tAcc@5 100.00 (100.00)\n","Epoch: [93][330/391]\tTime  0.176 ( 0.175)\tLoss 3.7519e-02 (2.7915e-02)\tAcc@1  98.44 ( 99.51)\tAcc@5 100.00 (100.00)\n","Epoch: [93][360/391]\tTime  0.176 ( 0.175)\tLoss 3.4401e-02 (2.7930e-02)\tAcc@1  99.22 ( 99.52)\tAcc@5 100.00 (100.00)\n","Epoch: [93][390/391]\tTime  0.158 ( 0.175)\tLoss 3.4671e-02 (2.8318e-02)\tAcc@1  98.75 ( 99.50)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.502 || Acc@5 99.998\n","==> Test Accuracy:  Acc@1 75.910 || Acc@5 93.750\n","==> 72.92 seconds to train this epoch\n","\n","\n","----- epoch: 94, lr: 0.004000000000000001 -----\n","Epoch: [94][  0/391]\tTime  0.240 ( 0.240)\tLoss 1.5449e-02 (1.5449e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [94][ 30/391]\tTime  0.176 ( 0.177)\tLoss 1.1404e-02 (2.0224e-02)\tAcc@1 100.00 ( 99.70)\tAcc@5 100.00 (100.00)\n","Epoch: [94][ 60/391]\tTime  0.174 ( 0.176)\tLoss 1.2453e-02 (2.1949e-02)\tAcc@1 100.00 ( 99.67)\tAcc@5 100.00 (100.00)\n","Epoch: [94][ 90/391]\tTime  0.175 ( 0.176)\tLoss 8.7600e-03 (2.1926e-02)\tAcc@1 100.00 ( 99.64)\tAcc@5 100.00 (100.00)\n","Epoch: [94][120/391]\tTime  0.175 ( 0.176)\tLoss 2.4706e-02 (2.1972e-02)\tAcc@1 100.00 ( 99.65)\tAcc@5 100.00 (100.00)\n","Epoch: [94][150/391]\tTime  0.175 ( 0.176)\tLoss 1.7259e-02 (2.2372e-02)\tAcc@1 100.00 ( 99.65)\tAcc@5 100.00 (100.00)\n","Epoch: [94][180/391]\tTime  0.175 ( 0.176)\tLoss 1.9442e-02 (2.2169e-02)\tAcc@1  99.22 ( 99.66)\tAcc@5 100.00 (100.00)\n","Epoch: [94][210/391]\tTime  0.174 ( 0.176)\tLoss 9.9922e-03 (2.2256e-02)\tAcc@1 100.00 ( 99.66)\tAcc@5 100.00 (100.00)\n","Epoch: [94][240/391]\tTime  0.176 ( 0.176)\tLoss 1.1639e-02 (2.1911e-02)\tAcc@1 100.00 ( 99.68)\tAcc@5 100.00 (100.00)\n","Epoch: [94][270/391]\tTime  0.176 ( 0.176)\tLoss 1.4590e-02 (2.2317e-02)\tAcc@1 100.00 ( 99.67)\tAcc@5 100.00 (100.00)\n","Epoch: [94][300/391]\tTime  0.174 ( 0.176)\tLoss 2.5640e-02 (2.2071e-02)\tAcc@1  99.22 ( 99.68)\tAcc@5 100.00 (100.00)\n","Epoch: [94][330/391]\tTime  0.175 ( 0.176)\tLoss 1.5866e-02 (2.2176e-02)\tAcc@1 100.00 ( 99.67)\tAcc@5 100.00 (100.00)\n","Epoch: [94][360/391]\tTime  0.176 ( 0.175)\tLoss 2.7079e-02 (2.2289e-02)\tAcc@1  99.22 ( 99.66)\tAcc@5 100.00 (100.00)\n","Epoch: [94][390/391]\tTime  0.156 ( 0.175)\tLoss 1.5230e-02 (2.2292e-02)\tAcc@1 100.00 ( 99.66)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.658 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 76.070 || Acc@5 93.860\n","==> 72.92 seconds to train this epoch\n","\n","\n","----- epoch: 95, lr: 0.004000000000000001 -----\n","Epoch: [95][  0/391]\tTime  0.246 ( 0.246)\tLoss 1.5230e-02 (1.5230e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [95][ 30/391]\tTime  0.174 ( 0.177)\tLoss 3.7835e-02 (1.9153e-02)\tAcc@1  97.66 ( 99.65)\tAcc@5 100.00 (100.00)\n","Epoch: [95][ 60/391]\tTime  0.173 ( 0.176)\tLoss 1.7546e-02 (1.9948e-02)\tAcc@1 100.00 ( 99.69)\tAcc@5 100.00 (100.00)\n","Epoch: [95][ 90/391]\tTime  0.176 ( 0.176)\tLoss 2.6228e-02 (1.9787e-02)\tAcc@1  98.44 ( 99.74)\tAcc@5 100.00 (100.00)\n","Epoch: [95][120/391]\tTime  0.169 ( 0.176)\tLoss 1.1864e-02 (1.9391e-02)\tAcc@1 100.00 ( 99.76)\tAcc@5 100.00 (100.00)\n","Epoch: [95][150/391]\tTime  0.177 ( 0.176)\tLoss 1.2896e-02 (1.9371e-02)\tAcc@1 100.00 ( 99.74)\tAcc@5 100.00 (100.00)\n","Epoch: [95][180/391]\tTime  0.176 ( 0.176)\tLoss 2.5791e-02 (1.9333e-02)\tAcc@1 100.00 ( 99.75)\tAcc@5 100.00 (100.00)\n","Epoch: [95][210/391]\tTime  0.175 ( 0.176)\tLoss 3.8015e-02 (1.9574e-02)\tAcc@1  99.22 ( 99.75)\tAcc@5 100.00 (100.00)\n","Epoch: [95][240/391]\tTime  0.174 ( 0.176)\tLoss 1.9768e-02 (1.9411e-02)\tAcc@1 100.00 ( 99.76)\tAcc@5 100.00 (100.00)\n","Epoch: [95][270/391]\tTime  0.176 ( 0.176)\tLoss 1.2879e-02 (1.9601e-02)\tAcc@1 100.00 ( 99.74)\tAcc@5 100.00 (100.00)\n","Epoch: [95][300/391]\tTime  0.175 ( 0.176)\tLoss 1.1233e-02 (1.9789e-02)\tAcc@1 100.00 ( 99.75)\tAcc@5 100.00 (100.00)\n","Epoch: [95][330/391]\tTime  0.174 ( 0.176)\tLoss 2.8196e-02 (1.9838e-02)\tAcc@1 100.00 ( 99.74)\tAcc@5 100.00 (100.00)\n","Epoch: [95][360/391]\tTime  0.176 ( 0.176)\tLoss 1.1859e-02 (1.9702e-02)\tAcc@1 100.00 ( 99.74)\tAcc@5 100.00 (100.00)\n","Epoch: [95][390/391]\tTime  0.158 ( 0.175)\tLoss 1.2697e-02 (1.9689e-02)\tAcc@1 100.00 ( 99.73)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.732 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 76.230 || Acc@5 93.960\n","==> 72.95 seconds to train this epoch\n","\n","\n","----- epoch: 96, lr: 0.004000000000000001 -----\n","Epoch: [96][  0/391]\tTime  0.233 ( 0.233)\tLoss 1.1918e-02 (1.1918e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [96][ 30/391]\tTime  0.175 ( 0.176)\tLoss 1.4874e-02 (1.8704e-02)\tAcc@1 100.00 ( 99.82)\tAcc@5 100.00 (100.00)\n","Epoch: [96][ 60/391]\tTime  0.175 ( 0.176)\tLoss 1.2243e-02 (1.7753e-02)\tAcc@1 100.00 ( 99.78)\tAcc@5 100.00 (100.00)\n","Epoch: [96][ 90/391]\tTime  0.175 ( 0.176)\tLoss 1.6959e-02 (1.7892e-02)\tAcc@1 100.00 ( 99.78)\tAcc@5 100.00 (100.00)\n","Epoch: [96][120/391]\tTime  0.175 ( 0.175)\tLoss 2.8828e-02 (1.8456e-02)\tAcc@1  99.22 ( 99.74)\tAcc@5 100.00 (100.00)\n","Epoch: [96][150/391]\tTime  0.175 ( 0.175)\tLoss 7.5085e-03 (1.8602e-02)\tAcc@1 100.00 ( 99.74)\tAcc@5 100.00 (100.00)\n","Epoch: [96][180/391]\tTime  0.175 ( 0.175)\tLoss 1.3148e-02 (1.8590e-02)\tAcc@1 100.00 ( 99.74)\tAcc@5 100.00 (100.00)\n","Epoch: [96][210/391]\tTime  0.175 ( 0.175)\tLoss 1.3853e-02 (1.8595e-02)\tAcc@1 100.00 ( 99.74)\tAcc@5 100.00 (100.00)\n","Epoch: [96][240/391]\tTime  0.176 ( 0.175)\tLoss 1.2418e-02 (1.8516e-02)\tAcc@1 100.00 ( 99.74)\tAcc@5 100.00 (100.00)\n","Epoch: [96][270/391]\tTime  0.174 ( 0.175)\tLoss 1.4655e-02 (1.8617e-02)\tAcc@1 100.00 ( 99.74)\tAcc@5 100.00 (100.00)\n","Epoch: [96][300/391]\tTime  0.175 ( 0.175)\tLoss 2.1031e-02 (1.8903e-02)\tAcc@1 100.00 ( 99.72)\tAcc@5 100.00 (100.00)\n","Epoch: [96][330/391]\tTime  0.176 ( 0.175)\tLoss 2.2254e-02 (1.9125e-02)\tAcc@1 100.00 ( 99.72)\tAcc@5 100.00 (100.00)\n","Epoch: [96][360/391]\tTime  0.174 ( 0.175)\tLoss 2.0503e-02 (1.9104e-02)\tAcc@1  99.22 ( 99.72)\tAcc@5 100.00 (100.00)\n","Epoch: [96][390/391]\tTime  0.159 ( 0.175)\tLoss 2.9555e-02 (1.9251e-02)\tAcc@1  98.75 ( 99.70)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.704 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 76.760 || Acc@5 93.880\n","==> 72.82 seconds to train this epoch\n","\n","\n","----- epoch: 97, lr: 0.004000000000000001 -----\n","Epoch: [97][  0/391]\tTime  0.238 ( 0.238)\tLoss 1.0726e-02 (1.0726e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [97][ 30/391]\tTime  0.176 ( 0.176)\tLoss 1.0369e-02 (1.6369e-02)\tAcc@1 100.00 ( 99.72)\tAcc@5 100.00 (100.00)\n","Epoch: [97][ 60/391]\tTime  0.177 ( 0.176)\tLoss 1.7221e-02 (1.7407e-02)\tAcc@1 100.00 ( 99.72)\tAcc@5 100.00 (100.00)\n","Epoch: [97][ 90/391]\tTime  0.175 ( 0.175)\tLoss 8.1346e-03 (1.6649e-02)\tAcc@1 100.00 ( 99.74)\tAcc@5 100.00 (100.00)\n","Epoch: [97][120/391]\tTime  0.175 ( 0.175)\tLoss 1.1733e-02 (1.6715e-02)\tAcc@1 100.00 ( 99.74)\tAcc@5 100.00 (100.00)\n","Epoch: [97][150/391]\tTime  0.176 ( 0.175)\tLoss 8.2118e-03 (1.6481e-02)\tAcc@1 100.00 ( 99.75)\tAcc@5 100.00 (100.00)\n","Epoch: [97][180/391]\tTime  0.176 ( 0.175)\tLoss 1.3506e-02 (1.6456e-02)\tAcc@1 100.00 ( 99.76)\tAcc@5 100.00 (100.00)\n","Epoch: [97][210/391]\tTime  0.175 ( 0.175)\tLoss 3.7034e-02 (1.6387e-02)\tAcc@1  99.22 ( 99.77)\tAcc@5 100.00 (100.00)\n","Epoch: [97][240/391]\tTime  0.180 ( 0.175)\tLoss 1.9506e-02 (1.6359e-02)\tAcc@1 100.00 ( 99.78)\tAcc@5 100.00 (100.00)\n","Epoch: [97][270/391]\tTime  0.174 ( 0.175)\tLoss 2.9866e-02 (1.6679e-02)\tAcc@1 100.00 ( 99.75)\tAcc@5 100.00 (100.00)\n","Epoch: [97][300/391]\tTime  0.176 ( 0.175)\tLoss 9.8497e-03 (1.6484e-02)\tAcc@1 100.00 ( 99.76)\tAcc@5 100.00 (100.00)\n","Epoch: [97][330/391]\tTime  0.174 ( 0.175)\tLoss 2.4647e-02 (1.6482e-02)\tAcc@1 100.00 ( 99.75)\tAcc@5 100.00 (100.00)\n","Epoch: [97][360/391]\tTime  0.175 ( 0.175)\tLoss 1.2026e-02 (1.6566e-02)\tAcc@1 100.00 ( 99.76)\tAcc@5 100.00 (100.00)\n","Epoch: [97][390/391]\tTime  0.159 ( 0.175)\tLoss 1.4897e-02 (1.6593e-02)\tAcc@1 100.00 ( 99.75)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.754 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 76.540 || Acc@5 93.870\n","==> 72.86 seconds to train this epoch\n","\n","\n","----- epoch: 98, lr: 0.004000000000000001 -----\n","Epoch: [98][  0/391]\tTime  0.234 ( 0.234)\tLoss 1.1657e-02 (1.1657e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [98][ 30/391]\tTime  0.175 ( 0.176)\tLoss 1.4834e-02 (1.5525e-02)\tAcc@1 100.00 ( 99.80)\tAcc@5 100.00 (100.00)\n","Epoch: [98][ 60/391]\tTime  0.175 ( 0.176)\tLoss 1.6718e-02 (1.4681e-02)\tAcc@1  99.22 ( 99.85)\tAcc@5 100.00 (100.00)\n","Epoch: [98][ 90/391]\tTime  0.175 ( 0.175)\tLoss 1.1521e-02 (1.4679e-02)\tAcc@1 100.00 ( 99.84)\tAcc@5 100.00 (100.00)\n","Epoch: [98][120/391]\tTime  0.175 ( 0.175)\tLoss 8.9223e-03 (1.4791e-02)\tAcc@1 100.00 ( 99.83)\tAcc@5 100.00 (100.00)\n","Epoch: [98][150/391]\tTime  0.177 ( 0.175)\tLoss 1.3614e-02 (1.4904e-02)\tAcc@1 100.00 ( 99.83)\tAcc@5 100.00 (100.00)\n","Epoch: [98][180/391]\tTime  0.174 ( 0.175)\tLoss 1.3296e-02 (1.4708e-02)\tAcc@1 100.00 ( 99.82)\tAcc@5 100.00 (100.00)\n","Epoch: [98][210/391]\tTime  0.179 ( 0.175)\tLoss 1.7510e-02 (1.5181e-02)\tAcc@1 100.00 ( 99.80)\tAcc@5 100.00 (100.00)\n","Epoch: [98][240/391]\tTime  0.175 ( 0.175)\tLoss 1.5306e-02 (1.5252e-02)\tAcc@1 100.00 ( 99.80)\tAcc@5 100.00 (100.00)\n","Epoch: [98][270/391]\tTime  0.175 ( 0.175)\tLoss 2.0993e-02 (1.5330e-02)\tAcc@1  99.22 ( 99.80)\tAcc@5 100.00 (100.00)\n","Epoch: [98][300/391]\tTime  0.174 ( 0.175)\tLoss 1.9248e-02 (1.5470e-02)\tAcc@1  99.22 ( 99.80)\tAcc@5 100.00 (100.00)\n","Epoch: [98][330/391]\tTime  0.173 ( 0.175)\tLoss 1.7228e-02 (1.5494e-02)\tAcc@1 100.00 ( 99.81)\tAcc@5 100.00 (100.00)\n","Epoch: [98][360/391]\tTime  0.175 ( 0.175)\tLoss 2.3327e-02 (1.5509e-02)\tAcc@1  99.22 ( 99.81)\tAcc@5 100.00 (100.00)\n","Epoch: [98][390/391]\tTime  0.158 ( 0.175)\tLoss 1.1338e-02 (1.5835e-02)\tAcc@1 100.00 ( 99.79)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.794 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 76.650 || Acc@5 93.830\n","==> 72.73 seconds to train this epoch\n","\n","\n","----- epoch: 99, lr: 0.004000000000000001 -----\n","Epoch: [99][  0/391]\tTime  0.219 ( 0.219)\tLoss 4.3931e-02 (4.3931e-02)\tAcc@1  98.44 ( 98.44)\tAcc@5 100.00 (100.00)\n","Epoch: [99][ 30/391]\tTime  0.174 ( 0.176)\tLoss 1.0475e-02 (1.7388e-02)\tAcc@1 100.00 ( 99.60)\tAcc@5 100.00 (100.00)\n","Epoch: [99][ 60/391]\tTime  0.175 ( 0.175)\tLoss 6.7625e-03 (1.6198e-02)\tAcc@1 100.00 ( 99.69)\tAcc@5 100.00 (100.00)\n","Epoch: [99][ 90/391]\tTime  0.174 ( 0.175)\tLoss 7.8199e-03 (1.5405e-02)\tAcc@1 100.00 ( 99.74)\tAcc@5 100.00 (100.00)\n","Epoch: [99][120/391]\tTime  0.175 ( 0.175)\tLoss 3.1167e-02 (1.5294e-02)\tAcc@1 100.00 ( 99.75)\tAcc@5 100.00 (100.00)\n","Epoch: [99][150/391]\tTime  0.175 ( 0.175)\tLoss 2.0945e-02 (1.5028e-02)\tAcc@1  99.22 ( 99.78)\tAcc@5 100.00 (100.00)\n","Epoch: [99][180/391]\tTime  0.179 ( 0.175)\tLoss 9.6626e-03 (1.5072e-02)\tAcc@1 100.00 ( 99.78)\tAcc@5 100.00 (100.00)\n","Epoch: [99][210/391]\tTime  0.174 ( 0.175)\tLoss 1.3271e-02 (1.5037e-02)\tAcc@1 100.00 ( 99.78)\tAcc@5 100.00 (100.00)\n","Epoch: [99][240/391]\tTime  0.176 ( 0.175)\tLoss 1.2165e-02 (1.4926e-02)\tAcc@1 100.00 ( 99.79)\tAcc@5 100.00 (100.00)\n","Epoch: [99][270/391]\tTime  0.175 ( 0.175)\tLoss 1.4454e-02 (1.5003e-02)\tAcc@1 100.00 ( 99.80)\tAcc@5 100.00 (100.00)\n","Epoch: [99][300/391]\tTime  0.177 ( 0.175)\tLoss 2.5002e-02 (1.4876e-02)\tAcc@1  99.22 ( 99.80)\tAcc@5 100.00 (100.00)\n","Epoch: [99][330/391]\tTime  0.174 ( 0.175)\tLoss 1.5880e-02 (1.4957e-02)\tAcc@1 100.00 ( 99.79)\tAcc@5 100.00 (100.00)\n","Epoch: [99][360/391]\tTime  0.174 ( 0.175)\tLoss 9.5544e-03 (1.4934e-02)\tAcc@1 100.00 ( 99.80)\tAcc@5 100.00 (100.00)\n","Epoch: [99][390/391]\tTime  0.157 ( 0.175)\tLoss 1.2283e-02 (1.4814e-02)\tAcc@1 100.00 ( 99.80)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.800 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 76.750 || Acc@5 93.730\n","==> 72.65 seconds to train this epoch\n","\n","\n","----- epoch: 100, lr: 0.004000000000000001 -----\n","Epoch: [100][  0/391]\tTime  0.226 ( 0.226)\tLoss 7.5173e-03 (7.5173e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [100][ 30/391]\tTime  0.174 ( 0.176)\tLoss 8.0348e-03 (1.3038e-02)\tAcc@1 100.00 ( 99.82)\tAcc@5 100.00 (100.00)\n","Epoch: [100][ 60/391]\tTime  0.174 ( 0.175)\tLoss 2.5398e-02 (1.3276e-02)\tAcc@1  99.22 ( 99.82)\tAcc@5 100.00 (100.00)\n","Epoch: [100][ 90/391]\tTime  0.175 ( 0.175)\tLoss 1.3903e-02 (1.3459e-02)\tAcc@1 100.00 ( 99.81)\tAcc@5 100.00 (100.00)\n","Epoch: [100][120/391]\tTime  0.174 ( 0.175)\tLoss 3.3881e-02 (1.4171e-02)\tAcc@1  98.44 ( 99.78)\tAcc@5 100.00 (100.00)\n","Epoch: [100][150/391]\tTime  0.172 ( 0.175)\tLoss 1.7153e-02 (1.4663e-02)\tAcc@1  99.22 ( 99.77)\tAcc@5 100.00 (100.00)\n","Epoch: [100][180/391]\tTime  0.174 ( 0.175)\tLoss 2.3848e-02 (1.4949e-02)\tAcc@1 100.00 ( 99.75)\tAcc@5 100.00 (100.00)\n","Epoch: [100][210/391]\tTime  0.174 ( 0.175)\tLoss 1.9442e-02 (1.4779e-02)\tAcc@1 100.00 ( 99.78)\tAcc@5 100.00 (100.00)\n","Epoch: [100][240/391]\tTime  0.175 ( 0.175)\tLoss 8.3617e-03 (1.5134e-02)\tAcc@1 100.00 ( 99.77)\tAcc@5 100.00 (100.00)\n","Epoch: [100][270/391]\tTime  0.175 ( 0.175)\tLoss 8.4455e-03 (1.4879e-02)\tAcc@1 100.00 ( 99.79)\tAcc@5 100.00 (100.00)\n","Epoch: [100][300/391]\tTime  0.174 ( 0.175)\tLoss 1.7174e-02 (1.4716e-02)\tAcc@1 100.00 ( 99.80)\tAcc@5 100.00 (100.00)\n","Epoch: [100][330/391]\tTime  0.174 ( 0.175)\tLoss 1.0732e-02 (1.4553e-02)\tAcc@1 100.00 ( 99.81)\tAcc@5 100.00 (100.00)\n","Epoch: [100][360/391]\tTime  0.175 ( 0.175)\tLoss 1.1304e-02 (1.4734e-02)\tAcc@1 100.00 ( 99.80)\tAcc@5 100.00 (100.00)\n","Epoch: [100][390/391]\tTime  0.157 ( 0.175)\tLoss 1.3673e-02 (1.4618e-02)\tAcc@1 100.00 ( 99.80)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.804 || Acc@5 99.998\n","==> Test Accuracy:  Acc@1 76.500 || Acc@5 93.650\n","==> 72.61 seconds to train this epoch\n","\n","\n","----- epoch: 101, lr: 0.004000000000000001 -----\n","Epoch: [101][  0/391]\tTime  0.245 ( 0.245)\tLoss 2.5437e-02 (2.5437e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5 100.00 (100.00)\n","Epoch: [101][ 30/391]\tTime  0.175 ( 0.176)\tLoss 1.4315e-02 (1.4331e-02)\tAcc@1 100.00 ( 99.72)\tAcc@5 100.00 (100.00)\n","Epoch: [101][ 60/391]\tTime  0.174 ( 0.175)\tLoss 1.0731e-02 (1.3617e-02)\tAcc@1 100.00 ( 99.78)\tAcc@5 100.00 (100.00)\n","Epoch: [101][ 90/391]\tTime  0.174 ( 0.175)\tLoss 1.6369e-02 (1.3941e-02)\tAcc@1 100.00 ( 99.81)\tAcc@5 100.00 (100.00)\n","Epoch: [101][120/391]\tTime  0.173 ( 0.175)\tLoss 9.6203e-03 (1.3089e-02)\tAcc@1 100.00 ( 99.85)\tAcc@5 100.00 (100.00)\n","Epoch: [101][150/391]\tTime  0.173 ( 0.175)\tLoss 8.7450e-03 (1.3332e-02)\tAcc@1 100.00 ( 99.84)\tAcc@5 100.00 (100.00)\n","Epoch: [101][180/391]\tTime  0.174 ( 0.175)\tLoss 7.8386e-03 (1.3291e-02)\tAcc@1 100.00 ( 99.86)\tAcc@5 100.00 (100.00)\n","Epoch: [101][210/391]\tTime  0.173 ( 0.175)\tLoss 6.8396e-03 (1.3358e-02)\tAcc@1 100.00 ( 99.85)\tAcc@5 100.00 (100.00)\n","Epoch: [101][240/391]\tTime  0.174 ( 0.175)\tLoss 1.0387e-02 (1.3129e-02)\tAcc@1 100.00 ( 99.86)\tAcc@5 100.00 (100.00)\n","Epoch: [101][270/391]\tTime  0.174 ( 0.175)\tLoss 1.4562e-02 (1.3234e-02)\tAcc@1  99.22 ( 99.86)\tAcc@5 100.00 (100.00)\n","Epoch: [101][300/391]\tTime  0.175 ( 0.175)\tLoss 1.5499e-02 (1.3367e-02)\tAcc@1  99.22 ( 99.85)\tAcc@5 100.00 (100.00)\n","Epoch: [101][330/391]\tTime  0.175 ( 0.175)\tLoss 7.2355e-03 (1.3366e-02)\tAcc@1 100.00 ( 99.85)\tAcc@5 100.00 (100.00)\n","Epoch: [101][360/391]\tTime  0.175 ( 0.175)\tLoss 7.0681e-03 (1.3206e-02)\tAcc@1 100.00 ( 99.86)\tAcc@5 100.00 (100.00)\n","Epoch: [101][390/391]\tTime  0.157 ( 0.175)\tLoss 1.2195e-02 (1.3149e-02)\tAcc@1 100.00 ( 99.86)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.858 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 76.490 || Acc@5 93.750\n","==> 72.66 seconds to train this epoch\n","\n","\n","----- epoch: 102, lr: 0.004000000000000001 -----\n","Epoch: [102][  0/391]\tTime  0.233 ( 0.233)\tLoss 1.5395e-02 (1.5395e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5 100.00 (100.00)\n","Epoch: [102][ 30/391]\tTime  0.174 ( 0.176)\tLoss 8.8476e-03 (1.0862e-02)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n","Epoch: [102][ 60/391]\tTime  0.174 ( 0.175)\tLoss 1.8224e-02 (1.1643e-02)\tAcc@1  99.22 ( 99.88)\tAcc@5 100.00 (100.00)\n","Epoch: [102][ 90/391]\tTime  0.175 ( 0.175)\tLoss 9.5920e-03 (1.2047e-02)\tAcc@1 100.00 ( 99.88)\tAcc@5 100.00 (100.00)\n","Epoch: [102][120/391]\tTime  0.176 ( 0.175)\tLoss 1.4973e-02 (1.2174e-02)\tAcc@1 100.00 ( 99.88)\tAcc@5 100.00 (100.00)\n","Epoch: [102][150/391]\tTime  0.174 ( 0.175)\tLoss 8.1396e-03 (1.2319e-02)\tAcc@1 100.00 ( 99.87)\tAcc@5 100.00 (100.00)\n","Epoch: [102][180/391]\tTime  0.174 ( 0.175)\tLoss 3.3662e-02 (1.2590e-02)\tAcc@1  99.22 ( 99.87)\tAcc@5 100.00 (100.00)\n","Epoch: [102][210/391]\tTime  0.177 ( 0.175)\tLoss 2.4764e-02 (1.2759e-02)\tAcc@1  99.22 ( 99.85)\tAcc@5 100.00 (100.00)\n","Epoch: [102][240/391]\tTime  0.175 ( 0.175)\tLoss 7.5963e-03 (1.2591e-02)\tAcc@1 100.00 ( 99.86)\tAcc@5 100.00 (100.00)\n","Epoch: [102][270/391]\tTime  0.176 ( 0.175)\tLoss 1.5379e-02 (1.2754e-02)\tAcc@1 100.00 ( 99.86)\tAcc@5 100.00 (100.00)\n","Epoch: [102][300/391]\tTime  0.176 ( 0.175)\tLoss 8.0618e-03 (1.2798e-02)\tAcc@1 100.00 ( 99.86)\tAcc@5 100.00 (100.00)\n","Epoch: [102][330/391]\tTime  0.176 ( 0.175)\tLoss 1.8202e-02 (1.2841e-02)\tAcc@1  99.22 ( 99.85)\tAcc@5 100.00 (100.00)\n","Epoch: [102][360/391]\tTime  0.173 ( 0.175)\tLoss 8.5734e-03 (1.2682e-02)\tAcc@1 100.00 ( 99.86)\tAcc@5 100.00 (100.00)\n","Epoch: [102][390/391]\tTime  0.158 ( 0.175)\tLoss 1.4042e-02 (1.2684e-02)\tAcc@1 100.00 ( 99.86)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.860 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 76.610 || Acc@5 93.840\n","==> 72.74 seconds to train this epoch\n","\n","\n","----- epoch: 103, lr: 0.004000000000000001 -----\n","Epoch: [103][  0/391]\tTime  0.235 ( 0.235)\tLoss 1.2740e-02 (1.2740e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [103][ 30/391]\tTime  0.175 ( 0.176)\tLoss 8.8145e-03 (1.1068e-02)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n","Epoch: [103][ 60/391]\tTime  0.174 ( 0.175)\tLoss 9.1516e-03 (1.1577e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n","Epoch: [103][ 90/391]\tTime  0.174 ( 0.175)\tLoss 1.0456e-02 (1.1963e-02)\tAcc@1 100.00 ( 99.88)\tAcc@5 100.00 (100.00)\n","Epoch: [103][120/391]\tTime  0.175 ( 0.175)\tLoss 1.1060e-02 (1.1529e-02)\tAcc@1 100.00 ( 99.88)\tAcc@5 100.00 (100.00)\n","Epoch: [103][150/391]\tTime  0.175 ( 0.175)\tLoss 1.6088e-02 (1.1372e-02)\tAcc@1  99.22 ( 99.88)\tAcc@5 100.00 (100.00)\n","Epoch: [103][180/391]\tTime  0.175 ( 0.175)\tLoss 1.3423e-02 (1.1347e-02)\tAcc@1 100.00 ( 99.88)\tAcc@5 100.00 (100.00)\n","Epoch: [103][210/391]\tTime  0.176 ( 0.175)\tLoss 7.7570e-03 (1.1677e-02)\tAcc@1 100.00 ( 99.87)\tAcc@5 100.00 (100.00)\n","Epoch: [103][240/391]\tTime  0.176 ( 0.175)\tLoss 6.1605e-03 (1.1918e-02)\tAcc@1 100.00 ( 99.86)\tAcc@5 100.00 (100.00)\n","Epoch: [103][270/391]\tTime  0.175 ( 0.175)\tLoss 1.2720e-02 (1.1939e-02)\tAcc@1 100.00 ( 99.87)\tAcc@5 100.00 (100.00)\n","Epoch: [103][300/391]\tTime  0.175 ( 0.175)\tLoss 2.0969e-02 (1.2234e-02)\tAcc@1 100.00 ( 99.87)\tAcc@5 100.00 (100.00)\n","Epoch: [103][330/391]\tTime  0.174 ( 0.175)\tLoss 1.3966e-02 (1.2252e-02)\tAcc@1 100.00 ( 99.87)\tAcc@5 100.00 (100.00)\n","Epoch: [103][360/391]\tTime  0.178 ( 0.175)\tLoss 9.7711e-03 (1.2134e-02)\tAcc@1 100.00 ( 99.87)\tAcc@5 100.00 (100.00)\n","Epoch: [103][390/391]\tTime  0.158 ( 0.175)\tLoss 2.2489e-02 (1.2068e-02)\tAcc@1  98.75 ( 99.87)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.872 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 76.670 || Acc@5 93.910\n","==> 72.74 seconds to train this epoch\n","\n","\n","----- epoch: 104, lr: 0.004000000000000001 -----\n","Epoch: [104][  0/391]\tTime  0.233 ( 0.233)\tLoss 8.6136e-03 (8.6136e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [104][ 30/391]\tTime  0.174 ( 0.176)\tLoss 1.4517e-02 (1.2243e-02)\tAcc@1 100.00 ( 99.77)\tAcc@5 100.00 (100.00)\n","Epoch: [104][ 60/391]\tTime  0.174 ( 0.176)\tLoss 1.1884e-02 (1.1724e-02)\tAcc@1 100.00 ( 99.82)\tAcc@5 100.00 (100.00)\n","Epoch: [104][ 90/391]\tTime  0.171 ( 0.175)\tLoss 5.8933e-03 (1.2183e-02)\tAcc@1 100.00 ( 99.83)\tAcc@5 100.00 (100.00)\n","Epoch: [104][120/391]\tTime  0.174 ( 0.175)\tLoss 5.4072e-03 (1.1322e-02)\tAcc@1 100.00 ( 99.86)\tAcc@5 100.00 (100.00)\n","Epoch: [104][150/391]\tTime  0.175 ( 0.175)\tLoss 1.3881e-02 (1.1998e-02)\tAcc@1 100.00 ( 99.83)\tAcc@5 100.00 (100.00)\n","Epoch: [104][180/391]\tTime  0.175 ( 0.175)\tLoss 5.0830e-03 (1.1825e-02)\tAcc@1 100.00 ( 99.84)\tAcc@5 100.00 (100.00)\n","Epoch: [104][210/391]\tTime  0.175 ( 0.175)\tLoss 1.5807e-02 (1.1633e-02)\tAcc@1  99.22 ( 99.85)\tAcc@5 100.00 (100.00)\n","Epoch: [104][240/391]\tTime  0.174 ( 0.175)\tLoss 9.8528e-03 (1.1626e-02)\tAcc@1 100.00 ( 99.85)\tAcc@5 100.00 (100.00)\n","Epoch: [104][270/391]\tTime  0.175 ( 0.175)\tLoss 1.0406e-02 (1.1731e-02)\tAcc@1 100.00 ( 99.85)\tAcc@5 100.00 (100.00)\n","Epoch: [104][300/391]\tTime  0.175 ( 0.175)\tLoss 1.0909e-02 (1.1781e-02)\tAcc@1 100.00 ( 99.85)\tAcc@5 100.00 (100.00)\n","Epoch: [104][330/391]\tTime  0.175 ( 0.175)\tLoss 1.0557e-02 (1.1964e-02)\tAcc@1 100.00 ( 99.85)\tAcc@5 100.00 (100.00)\n","Epoch: [104][360/391]\tTime  0.175 ( 0.175)\tLoss 3.4087e-02 (1.2036e-02)\tAcc@1  99.22 ( 99.84)\tAcc@5 100.00 (100.00)\n","Epoch: [104][390/391]\tTime  0.159 ( 0.175)\tLoss 2.2816e-02 (1.2023e-02)\tAcc@1  98.75 ( 99.85)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.848 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 76.580 || Acc@5 93.910\n","==> 72.76 seconds to train this epoch\n","\n","\n","----- epoch: 105, lr: 0.004000000000000001 -----\n","Epoch: [105][  0/391]\tTime  0.245 ( 0.245)\tLoss 1.0910e-02 (1.0910e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [105][ 30/391]\tTime  0.177 ( 0.177)\tLoss 1.1479e-02 (1.0573e-02)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [105][ 60/391]\tTime  0.174 ( 0.176)\tLoss 5.2795e-03 (1.0796e-02)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n","Epoch: [105][ 90/391]\tTime  0.174 ( 0.176)\tLoss 7.9122e-03 (1.0872e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n","Epoch: [105][120/391]\tTime  0.175 ( 0.175)\tLoss 9.4951e-03 (1.1103e-02)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n","Epoch: [105][150/391]\tTime  0.177 ( 0.175)\tLoss 7.9436e-03 (1.1009e-02)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n","Epoch: [105][180/391]\tTime  0.176 ( 0.175)\tLoss 1.9320e-02 (1.1523e-02)\tAcc@1 100.00 ( 99.88)\tAcc@5 100.00 (100.00)\n","Epoch: [105][210/391]\tTime  0.179 ( 0.175)\tLoss 6.5218e-03 (1.1553e-02)\tAcc@1 100.00 ( 99.89)\tAcc@5 100.00 (100.00)\n","Epoch: [105][240/391]\tTime  0.174 ( 0.175)\tLoss 1.4778e-02 (1.1337e-02)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n","Epoch: [105][270/391]\tTime  0.170 ( 0.175)\tLoss 5.4236e-03 (1.1337e-02)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n","Epoch: [105][300/391]\tTime  0.175 ( 0.175)\tLoss 8.4366e-03 (1.1366e-02)\tAcc@1 100.00 ( 99.89)\tAcc@5 100.00 (100.00)\n","Epoch: [105][330/391]\tTime  0.174 ( 0.175)\tLoss 9.5274e-03 (1.1411e-02)\tAcc@1 100.00 ( 99.88)\tAcc@5 100.00 (100.00)\n","Epoch: [105][360/391]\tTime  0.174 ( 0.175)\tLoss 7.3452e-03 (1.1585e-02)\tAcc@1 100.00 ( 99.88)\tAcc@5 100.00 (100.00)\n","Epoch: [105][390/391]\tTime  0.156 ( 0.175)\tLoss 9.7298e-03 (1.1568e-02)\tAcc@1 100.00 ( 99.88)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.878 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 76.520 || Acc@5 93.700\n","==> 72.83 seconds to train this epoch\n","\n","\n","----- epoch: 106, lr: 0.004000000000000001 -----\n","Epoch: [106][  0/391]\tTime  0.240 ( 0.240)\tLoss 8.7091e-03 (8.7091e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [106][ 30/391]\tTime  0.177 ( 0.177)\tLoss 8.1781e-03 (1.0625e-02)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n","Epoch: [106][ 60/391]\tTime  0.176 ( 0.176)\tLoss 8.9817e-03 (1.0919e-02)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n","Epoch: [106][ 90/391]\tTime  0.173 ( 0.176)\tLoss 7.2713e-03 (1.0589e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n","Epoch: [106][120/391]\tTime  0.175 ( 0.176)\tLoss 4.6202e-02 (1.0774e-02)\tAcc@1  98.44 ( 99.90)\tAcc@5 100.00 (100.00)\n","Epoch: [106][150/391]\tTime  0.176 ( 0.176)\tLoss 1.6506e-02 (1.0562e-02)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n","Epoch: [106][180/391]\tTime  0.176 ( 0.176)\tLoss 8.2875e-03 (1.0394e-02)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n","Epoch: [106][210/391]\tTime  0.174 ( 0.176)\tLoss 4.0016e-02 (1.0496e-02)\tAcc@1  98.44 ( 99.89)\tAcc@5 100.00 (100.00)\n","Epoch: [106][240/391]\tTime  0.175 ( 0.176)\tLoss 8.2698e-03 (1.0517e-02)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n","Epoch: [106][270/391]\tTime  0.174 ( 0.176)\tLoss 1.2777e-02 (1.0447e-02)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n","Epoch: [106][300/391]\tTime  0.176 ( 0.176)\tLoss 7.7092e-03 (1.0495e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n","Epoch: [106][330/391]\tTime  0.175 ( 0.175)\tLoss 9.0182e-03 (1.0478e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n","Epoch: [106][360/391]\tTime  0.175 ( 0.175)\tLoss 5.1175e-03 (1.0500e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n","Epoch: [106][390/391]\tTime  0.158 ( 0.175)\tLoss 1.0624e-02 (1.0440e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.912 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 76.770 || Acc@5 93.710\n","==> 72.89 seconds to train this epoch\n","\n","\n","----- epoch: 107, lr: 0.004000000000000001 -----\n","Epoch: [107][  0/391]\tTime  0.234 ( 0.234)\tLoss 8.5143e-03 (8.5143e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [107][ 30/391]\tTime  0.176 ( 0.176)\tLoss 5.3366e-03 (9.2184e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [107][ 60/391]\tTime  0.173 ( 0.176)\tLoss 1.4526e-02 (9.4873e-03)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n","Epoch: [107][ 90/391]\tTime  0.177 ( 0.175)\tLoss 8.9774e-03 (9.5540e-03)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n","Epoch: [107][120/391]\tTime  0.172 ( 0.175)\tLoss 1.6168e-02 (9.6361e-03)\tAcc@1  99.22 ( 99.92)\tAcc@5 100.00 (100.00)\n","Epoch: [107][150/391]\tTime  0.174 ( 0.175)\tLoss 5.4828e-03 (1.0048e-02)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n","Epoch: [107][180/391]\tTime  0.176 ( 0.175)\tLoss 7.4327e-03 (1.0374e-02)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n","Epoch: [107][210/391]\tTime  0.175 ( 0.175)\tLoss 1.0793e-02 (1.0403e-02)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n","Epoch: [107][240/391]\tTime  0.175 ( 0.175)\tLoss 1.9982e-02 (1.0499e-02)\tAcc@1  99.22 ( 99.90)\tAcc@5 100.00 (100.00)\n","Epoch: [107][270/391]\tTime  0.179 ( 0.175)\tLoss 1.5445e-02 (1.0366e-02)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n","Epoch: [107][300/391]\tTime  0.171 ( 0.175)\tLoss 1.0971e-02 (1.0472e-02)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n","Epoch: [107][330/391]\tTime  0.176 ( 0.175)\tLoss 9.9952e-03 (1.0644e-02)\tAcc@1 100.00 ( 99.89)\tAcc@5 100.00 (100.00)\n","Epoch: [107][360/391]\tTime  0.174 ( 0.175)\tLoss 1.4950e-02 (1.0503e-02)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n","Epoch: [107][390/391]\tTime  0.160 ( 0.175)\tLoss 5.7276e-03 (1.0515e-02)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.900 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 76.650 || Acc@5 93.810\n","==> 72.77 seconds to train this epoch\n","\n","\n","----- epoch: 108, lr: 0.004000000000000001 -----\n","Epoch: [108][  0/391]\tTime  0.230 ( 0.230)\tLoss 7.8359e-03 (7.8359e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [108][ 30/391]\tTime  0.174 ( 0.176)\tLoss 9.3736e-03 (8.3348e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [108][ 60/391]\tTime  0.174 ( 0.176)\tLoss 8.3589e-03 (8.5418e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [108][ 90/391]\tTime  0.173 ( 0.175)\tLoss 6.0135e-03 (8.7035e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [108][120/391]\tTime  0.176 ( 0.175)\tLoss 5.3949e-03 (8.9489e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [108][150/391]\tTime  0.182 ( 0.175)\tLoss 9.6170e-03 (9.1868e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [108][180/391]\tTime  0.170 ( 0.175)\tLoss 9.8044e-03 (9.3650e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [108][210/391]\tTime  0.176 ( 0.175)\tLoss 8.3644e-03 (9.5837e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [108][240/391]\tTime  0.175 ( 0.175)\tLoss 8.4133e-03 (9.7551e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [108][270/391]\tTime  0.175 ( 0.175)\tLoss 4.6267e-03 (9.9002e-03)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n","Epoch: [108][300/391]\tTime  0.180 ( 0.175)\tLoss 5.1598e-03 (9.9939e-03)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n","Epoch: [108][330/391]\tTime  0.174 ( 0.175)\tLoss 1.0499e-02 (1.0059e-02)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n","Epoch: [108][360/391]\tTime  0.174 ( 0.175)\tLoss 9.4618e-03 (1.0215e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n","Epoch: [108][390/391]\tTime  0.156 ( 0.175)\tLoss 8.9510e-03 (1.0297e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.914 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 76.680 || Acc@5 93.970\n","==> 72.65 seconds to train this epoch\n","\n","\n","----- epoch: 109, lr: 0.004000000000000001 -----\n","Epoch: [109][  0/391]\tTime  0.231 ( 0.231)\tLoss 8.1486e-03 (8.1486e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [109][ 30/391]\tTime  0.174 ( 0.176)\tLoss 1.3028e-02 (1.1485e-02)\tAcc@1  99.22 ( 99.82)\tAcc@5 100.00 (100.00)\n","Epoch: [109][ 60/391]\tTime  0.173 ( 0.175)\tLoss 7.1149e-03 (1.0659e-02)\tAcc@1 100.00 ( 99.86)\tAcc@5 100.00 (100.00)\n","Epoch: [109][ 90/391]\tTime  0.175 ( 0.175)\tLoss 9.5553e-03 (1.0767e-02)\tAcc@1 100.00 ( 99.87)\tAcc@5 100.00 (100.00)\n","Epoch: [109][120/391]\tTime  0.174 ( 0.175)\tLoss 6.0717e-03 (1.0017e-02)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n","Epoch: [109][150/391]\tTime  0.174 ( 0.175)\tLoss 4.5381e-03 (1.0078e-02)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n","Epoch: [109][180/391]\tTime  0.176 ( 0.175)\tLoss 3.7362e-03 (1.0160e-02)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n","Epoch: [109][210/391]\tTime  0.175 ( 0.175)\tLoss 7.5959e-03 (1.0173e-02)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n","Epoch: [109][240/391]\tTime  0.175 ( 0.175)\tLoss 9.8756e-03 (1.0107e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n","Epoch: [109][270/391]\tTime  0.173 ( 0.175)\tLoss 1.1901e-02 (1.0101e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n","Epoch: [109][300/391]\tTime  0.177 ( 0.175)\tLoss 8.6030e-03 (1.0229e-02)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n","Epoch: [109][330/391]\tTime  0.174 ( 0.175)\tLoss 1.2095e-02 (1.0170e-02)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n","Epoch: [109][360/391]\tTime  0.175 ( 0.175)\tLoss 1.0388e-02 (1.0266e-02)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n","Epoch: [109][390/391]\tTime  0.158 ( 0.175)\tLoss 4.0442e-02 (1.0443e-02)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.920 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 76.830 || Acc@5 93.840\n","==> 72.65 seconds to train this epoch\n","\n","\n","----- epoch: 110, lr: 0.004000000000000001 -----\n","Epoch: [110][  0/391]\tTime  0.237 ( 0.237)\tLoss 7.1443e-03 (7.1443e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [110][ 30/391]\tTime  0.174 ( 0.176)\tLoss 4.8852e-03 (1.1070e-02)\tAcc@1 100.00 ( 99.87)\tAcc@5 100.00 (100.00)\n","Epoch: [110][ 60/391]\tTime  0.178 ( 0.176)\tLoss 5.3415e-03 (1.0471e-02)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n","Epoch: [110][ 90/391]\tTime  0.175 ( 0.176)\tLoss 9.8277e-03 (9.6665e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [110][120/391]\tTime  0.173 ( 0.175)\tLoss 7.6161e-03 (1.0000e-02)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n","Epoch: [110][150/391]\tTime  0.175 ( 0.175)\tLoss 9.4738e-03 (1.0127e-02)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n","Epoch: [110][180/391]\tTime  0.175 ( 0.175)\tLoss 6.0226e-03 (1.0228e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n","Epoch: [110][210/391]\tTime  0.176 ( 0.175)\tLoss 6.5295e-03 (1.0325e-02)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n","Epoch: [110][240/391]\tTime  0.175 ( 0.175)\tLoss 5.0092e-03 (1.0168e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n","Epoch: [110][270/391]\tTime  0.176 ( 0.175)\tLoss 9.0341e-03 (1.0234e-02)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n","Epoch: [110][300/391]\tTime  0.175 ( 0.175)\tLoss 2.1393e-02 (1.0206e-02)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n","Epoch: [110][330/391]\tTime  0.175 ( 0.175)\tLoss 6.4360e-03 (1.0132e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n","Epoch: [110][360/391]\tTime  0.177 ( 0.175)\tLoss 4.1233e-03 (1.0219e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n","Epoch: [110][390/391]\tTime  0.157 ( 0.175)\tLoss 1.5757e-02 (1.0090e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.912 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 76.670 || Acc@5 93.970\n","==> 72.80 seconds to train this epoch\n","\n","\n","----- epoch: 111, lr: 0.004000000000000001 -----\n","Epoch: [111][  0/391]\tTime  0.241 ( 0.241)\tLoss 1.3076e-02 (1.3076e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [111][ 30/391]\tTime  0.176 ( 0.177)\tLoss 2.0882e-02 (9.2884e-03)\tAcc@1  99.22 ( 99.92)\tAcc@5 100.00 (100.00)\n","Epoch: [111][ 60/391]\tTime  0.173 ( 0.176)\tLoss 6.8724e-03 (9.6477e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [111][ 90/391]\tTime  0.175 ( 0.176)\tLoss 1.1112e-02 (9.2818e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [111][120/391]\tTime  0.175 ( 0.175)\tLoss 1.4016e-02 (1.0114e-02)\tAcc@1  99.22 ( 99.92)\tAcc@5 100.00 (100.00)\n","Epoch: [111][150/391]\tTime  0.177 ( 0.175)\tLoss 4.0697e-02 (1.0240e-02)\tAcc@1  99.22 ( 99.92)\tAcc@5 100.00 (100.00)\n","Epoch: [111][180/391]\tTime  0.174 ( 0.175)\tLoss 7.0704e-03 (1.0029e-02)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n","Epoch: [111][210/391]\tTime  0.174 ( 0.175)\tLoss 7.8928e-03 (1.0203e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n","Epoch: [111][240/391]\tTime  0.175 ( 0.175)\tLoss 6.2686e-03 (1.0260e-02)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n","Epoch: [111][270/391]\tTime  0.174 ( 0.175)\tLoss 7.5015e-03 (1.0170e-02)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n","Epoch: [111][300/391]\tTime  0.174 ( 0.175)\tLoss 1.0091e-02 (1.0132e-02)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n","Epoch: [111][330/391]\tTime  0.176 ( 0.175)\tLoss 3.7248e-03 (1.0025e-02)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n","Epoch: [111][360/391]\tTime  0.176 ( 0.175)\tLoss 1.1891e-02 (9.9924e-03)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n","Epoch: [111][390/391]\tTime  0.158 ( 0.175)\tLoss 1.4460e-02 (9.9378e-03)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.920 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 76.710 || Acc@5 93.730\n","==> 72.80 seconds to train this epoch\n","\n","\n","----- epoch: 112, lr: 0.004000000000000001 -----\n","Epoch: [112][  0/391]\tTime  0.219 ( 0.219)\tLoss 7.3319e-03 (7.3319e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [112][ 30/391]\tTime  0.177 ( 0.176)\tLoss 6.9097e-03 (1.0249e-02)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n","Epoch: [112][ 60/391]\tTime  0.174 ( 0.176)\tLoss 7.3182e-03 (9.6781e-03)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n","Epoch: [112][ 90/391]\tTime  0.177 ( 0.175)\tLoss 5.9030e-03 (9.8331e-03)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n","Epoch: [112][120/391]\tTime  0.176 ( 0.175)\tLoss 9.1543e-03 (9.5597e-03)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n","Epoch: [112][150/391]\tTime  0.178 ( 0.175)\tLoss 6.1208e-03 (9.6703e-03)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n","Epoch: [112][180/391]\tTime  0.176 ( 0.175)\tLoss 7.4642e-03 (9.5413e-03)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n","Epoch: [112][210/391]\tTime  0.176 ( 0.175)\tLoss 5.0748e-03 (9.7268e-03)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n","Epoch: [112][240/391]\tTime  0.175 ( 0.175)\tLoss 5.9880e-03 (9.8103e-03)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n","Epoch: [112][270/391]\tTime  0.176 ( 0.175)\tLoss 7.3790e-03 (9.9055e-03)\tAcc@1 100.00 ( 99.89)\tAcc@5 100.00 (100.00)\n","Epoch: [112][300/391]\tTime  0.175 ( 0.175)\tLoss 7.6411e-03 (9.8020e-03)\tAcc@1 100.00 ( 99.89)\tAcc@5 100.00 (100.00)\n","Epoch: [112][330/391]\tTime  0.175 ( 0.175)\tLoss 6.8054e-03 (9.8094e-03)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n","Epoch: [112][360/391]\tTime  0.175 ( 0.175)\tLoss 1.5977e-02 (9.7186e-03)\tAcc@1  99.22 ( 99.90)\tAcc@5 100.00 (100.00)\n","Epoch: [112][390/391]\tTime  0.161 ( 0.175)\tLoss 1.1993e-02 (9.7317e-03)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.906 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 76.980 || Acc@5 94.020\n","==> 72.78 seconds to train this epoch\n","\n","\n","----- epoch: 113, lr: 0.004000000000000001 -----\n","Epoch: [113][  0/391]\tTime  0.234 ( 0.234)\tLoss 1.5920e-02 (1.5920e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5 100.00 (100.00)\n","Epoch: [113][ 30/391]\tTime  0.178 ( 0.176)\tLoss 9.2672e-03 (1.0052e-02)\tAcc@1 100.00 ( 99.87)\tAcc@5 100.00 (100.00)\n","Epoch: [113][ 60/391]\tTime  0.168 ( 0.176)\tLoss 7.3598e-03 (1.0017e-02)\tAcc@1 100.00 ( 99.87)\tAcc@5 100.00 (100.00)\n","Epoch: [113][ 90/391]\tTime  0.176 ( 0.175)\tLoss 8.8491e-03 (9.8617e-03)\tAcc@1 100.00 ( 99.88)\tAcc@5 100.00 (100.00)\n","Epoch: [113][120/391]\tTime  0.175 ( 0.175)\tLoss 1.4136e-02 (9.5760e-03)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n","Epoch: [113][150/391]\tTime  0.174 ( 0.175)\tLoss 1.1120e-02 (9.5697e-03)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n","Epoch: [113][180/391]\tTime  0.176 ( 0.175)\tLoss 1.1298e-02 (9.3434e-03)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n","Epoch: [113][210/391]\tTime  0.174 ( 0.175)\tLoss 2.0834e-02 (9.4801e-03)\tAcc@1  99.22 ( 99.91)\tAcc@5 100.00 (100.00)\n","Epoch: [113][240/391]\tTime  0.176 ( 0.175)\tLoss 7.4921e-03 (9.5251e-03)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n","Epoch: [113][270/391]\tTime  0.172 ( 0.175)\tLoss 5.4161e-03 (9.5020e-03)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n","Epoch: [113][300/391]\tTime  0.174 ( 0.175)\tLoss 7.1540e-03 (9.4214e-03)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n","Epoch: [113][330/391]\tTime  0.176 ( 0.175)\tLoss 6.2490e-03 (9.5484e-03)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n","Epoch: [113][360/391]\tTime  0.176 ( 0.175)\tLoss 7.2662e-03 (9.5765e-03)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n","Epoch: [113][390/391]\tTime  0.157 ( 0.175)\tLoss 3.0792e-02 (9.7663e-03)\tAcc@1  98.75 ( 99.91)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.908 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 76.990 || Acc@5 93.890\n","==> 72.85 seconds to train this epoch\n","\n","\n","----- epoch: 114, lr: 0.004000000000000001 -----\n","Epoch: [114][  0/391]\tTime  0.232 ( 0.232)\tLoss 3.1565e-02 (3.1565e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5 100.00 (100.00)\n","Epoch: [114][ 30/391]\tTime  0.179 ( 0.177)\tLoss 7.1893e-03 (8.6864e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [114][ 60/391]\tTime  0.173 ( 0.176)\tLoss 9.9571e-03 (9.3857e-03)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n","Epoch: [114][ 90/391]\tTime  0.174 ( 0.176)\tLoss 1.4514e-02 (8.9274e-03)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n","Epoch: [114][120/391]\tTime  0.174 ( 0.176)\tLoss 5.2006e-03 (8.6960e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [114][150/391]\tTime  0.176 ( 0.176)\tLoss 1.2988e-02 (8.8011e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [114][180/391]\tTime  0.175 ( 0.175)\tLoss 1.1777e-02 (9.0986e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [114][210/391]\tTime  0.174 ( 0.175)\tLoss 1.0141e-02 (9.3409e-03)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n","Epoch: [114][240/391]\tTime  0.175 ( 0.175)\tLoss 4.2342e-03 (9.3935e-03)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n","Epoch: [114][270/391]\tTime  0.176 ( 0.175)\tLoss 8.2312e-03 (9.4579e-03)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n","Epoch: [114][300/391]\tTime  0.175 ( 0.175)\tLoss 1.3262e-02 (9.5878e-03)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n","Epoch: [114][330/391]\tTime  0.174 ( 0.175)\tLoss 5.8043e-03 (9.7148e-03)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n","Epoch: [114][360/391]\tTime  0.176 ( 0.175)\tLoss 6.1081e-03 (9.7480e-03)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n","Epoch: [114][390/391]\tTime  0.158 ( 0.175)\tLoss 1.0321e-02 (9.8301e-03)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.918 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 77.050 || Acc@5 94.010\n","==> 72.86 seconds to train this epoch\n","\n","\n","----- epoch: 115, lr: 0.004000000000000001 -----\n","Epoch: [115][  0/391]\tTime  0.241 ( 0.241)\tLoss 4.2178e-03 (4.2178e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [115][ 30/391]\tTime  0.175 ( 0.177)\tLoss 8.7814e-03 (8.8635e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [115][ 60/391]\tTime  0.171 ( 0.176)\tLoss 5.9058e-03 (9.2159e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [115][ 90/391]\tTime  0.175 ( 0.176)\tLoss 5.4481e-03 (8.9770e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [115][120/391]\tTime  0.175 ( 0.176)\tLoss 8.3356e-03 (9.2100e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [115][150/391]\tTime  0.175 ( 0.175)\tLoss 8.4334e-03 (9.2016e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [115][180/391]\tTime  0.175 ( 0.175)\tLoss 5.8322e-03 (9.1536e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [115][210/391]\tTime  0.175 ( 0.175)\tLoss 9.7407e-03 (9.1536e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [115][240/391]\tTime  0.176 ( 0.175)\tLoss 1.7460e-02 (9.2317e-03)\tAcc@1  99.22 ( 99.93)\tAcc@5 100.00 (100.00)\n","Epoch: [115][270/391]\tTime  0.177 ( 0.175)\tLoss 9.4482e-03 (9.1314e-03)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n","Epoch: [115][300/391]\tTime  0.176 ( 0.175)\tLoss 1.6652e-02 (9.0275e-03)\tAcc@1  99.22 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [115][330/391]\tTime  0.175 ( 0.175)\tLoss 7.1506e-03 (9.0332e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [115][360/391]\tTime  0.174 ( 0.175)\tLoss 1.0978e-02 (8.9667e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [115][390/391]\tTime  0.160 ( 0.175)\tLoss 1.5996e-02 (9.0798e-03)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.934 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 77.110 || Acc@5 93.890\n","==> 72.91 seconds to train this epoch\n","\n","\n","----- epoch: 116, lr: 0.004000000000000001 -----\n","Epoch: [116][  0/391]\tTime  0.246 ( 0.246)\tLoss 5.9802e-03 (5.9802e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [116][ 30/391]\tTime  0.174 ( 0.177)\tLoss 5.4632e-03 (8.2646e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [116][ 60/391]\tTime  0.175 ( 0.176)\tLoss 8.1241e-03 (7.8185e-03)\tAcc@1 100.00 ( 99.99)\tAcc@5 100.00 (100.00)\n","Epoch: [116][ 90/391]\tTime  0.174 ( 0.176)\tLoss 6.4199e-03 (7.9484e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [116][120/391]\tTime  0.177 ( 0.176)\tLoss 9.4071e-03 (8.4540e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [116][150/391]\tTime  0.176 ( 0.176)\tLoss 1.5219e-02 (8.5988e-03)\tAcc@1  99.22 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [116][180/391]\tTime  0.177 ( 0.176)\tLoss 7.7506e-03 (8.5984e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [116][210/391]\tTime  0.175 ( 0.176)\tLoss 5.9851e-03 (8.5798e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [116][240/391]\tTime  0.174 ( 0.176)\tLoss 1.1705e-02 (8.7701e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [116][270/391]\tTime  0.176 ( 0.176)\tLoss 1.4884e-02 (8.8217e-03)\tAcc@1  99.22 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [116][300/391]\tTime  0.175 ( 0.176)\tLoss 1.4635e-02 (9.0075e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [116][330/391]\tTime  0.178 ( 0.176)\tLoss 7.9085e-03 (8.9633e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [116][360/391]\tTime  0.175 ( 0.176)\tLoss 1.6398e-02 (9.0278e-03)\tAcc@1  99.22 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [116][390/391]\tTime  0.160 ( 0.175)\tLoss 7.1037e-03 (8.9392e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.946 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 77.000 || Acc@5 93.920\n","==> 72.96 seconds to train this epoch\n","\n","\n","----- epoch: 117, lr: 0.004000000000000001 -----\n","Epoch: [117][  0/391]\tTime  0.228 ( 0.228)\tLoss 6.4453e-03 (6.4453e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [117][ 30/391]\tTime  0.179 ( 0.177)\tLoss 7.4797e-03 (1.0326e-02)\tAcc@1 100.00 ( 99.85)\tAcc@5 100.00 (100.00)\n","Epoch: [117][ 60/391]\tTime  0.173 ( 0.176)\tLoss 5.8970e-03 (9.9985e-03)\tAcc@1 100.00 ( 99.88)\tAcc@5 100.00 (100.00)\n","Epoch: [117][ 90/391]\tTime  0.176 ( 0.176)\tLoss 8.5122e-03 (9.3809e-03)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n","Epoch: [117][120/391]\tTime  0.175 ( 0.176)\tLoss 7.3724e-03 (9.3454e-03)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n","Epoch: [117][150/391]\tTime  0.176 ( 0.176)\tLoss 8.5506e-03 (9.0792e-03)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n","Epoch: [117][180/391]\tTime  0.175 ( 0.175)\tLoss 8.1280e-03 (9.1474e-03)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n","Epoch: [117][210/391]\tTime  0.176 ( 0.175)\tLoss 7.4991e-03 (9.0723e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [117][240/391]\tTime  0.175 ( 0.175)\tLoss 1.0357e-02 (9.0052e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [117][270/391]\tTime  0.175 ( 0.175)\tLoss 9.7398e-03 (8.9802e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [117][300/391]\tTime  0.175 ( 0.175)\tLoss 6.2937e-03 (9.0554e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [117][330/391]\tTime  0.175 ( 0.175)\tLoss 1.0799e-02 (9.0900e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [117][360/391]\tTime  0.174 ( 0.175)\tLoss 4.9095e-03 (9.1223e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [117][390/391]\tTime  0.160 ( 0.175)\tLoss 1.4759e-02 (9.2522e-03)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.928 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 76.900 || Acc@5 93.820\n","==> 72.85 seconds to train this epoch\n","\n","\n","----- epoch: 118, lr: 0.004000000000000001 -----\n","Epoch: [118][  0/391]\tTime  0.247 ( 0.247)\tLoss 7.7535e-03 (7.7535e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [118][ 30/391]\tTime  0.175 ( 0.177)\tLoss 7.0929e-03 (8.2613e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [118][ 60/391]\tTime  0.174 ( 0.176)\tLoss 6.7959e-03 (8.2175e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [118][ 90/391]\tTime  0.174 ( 0.176)\tLoss 6.1682e-03 (8.7914e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [118][120/391]\tTime  0.175 ( 0.175)\tLoss 7.0375e-03 (8.6059e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [118][150/391]\tTime  0.175 ( 0.175)\tLoss 1.3943e-02 (8.6924e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [118][180/391]\tTime  0.176 ( 0.175)\tLoss 1.2465e-02 (8.6523e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [118][210/391]\tTime  0.176 ( 0.175)\tLoss 8.7430e-03 (8.6036e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [118][240/391]\tTime  0.175 ( 0.175)\tLoss 8.4642e-03 (8.5767e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [118][270/391]\tTime  0.174 ( 0.175)\tLoss 8.8525e-03 (8.6476e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [118][300/391]\tTime  0.176 ( 0.175)\tLoss 6.1168e-03 (8.6558e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [118][330/391]\tTime  0.175 ( 0.175)\tLoss 1.0494e-02 (8.7029e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [118][360/391]\tTime  0.176 ( 0.175)\tLoss 1.6531e-02 (8.8026e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [118][390/391]\tTime  0.156 ( 0.175)\tLoss 1.5134e-02 (8.8312e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.942 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 76.930 || Acc@5 94.080\n","==> 72.80 seconds to train this epoch\n","\n","\n","----- epoch: 119, lr: 0.004000000000000001 -----\n","Epoch: [119][  0/391]\tTime  0.237 ( 0.237)\tLoss 1.4143e-02 (1.4143e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5 100.00 (100.00)\n","Epoch: [119][ 30/391]\tTime  0.175 ( 0.176)\tLoss 8.4326e-03 (8.0460e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [119][ 60/391]\tTime  0.176 ( 0.175)\tLoss 4.3226e-03 (9.5382e-03)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n","Epoch: [119][ 90/391]\tTime  0.175 ( 0.175)\tLoss 5.3375e-03 (9.4793e-03)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n","Epoch: [119][120/391]\tTime  0.176 ( 0.175)\tLoss 7.2208e-03 (9.4463e-03)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n","Epoch: [119][150/391]\tTime  0.176 ( 0.175)\tLoss 5.5756e-03 (9.3288e-03)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n","Epoch: [119][180/391]\tTime  0.175 ( 0.175)\tLoss 6.9393e-03 (9.0200e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [119][210/391]\tTime  0.175 ( 0.175)\tLoss 3.1564e-02 (9.0643e-03)\tAcc@1  99.22 ( 99.93)\tAcc@5 100.00 (100.00)\n","Epoch: [119][240/391]\tTime  0.176 ( 0.175)\tLoss 6.8009e-03 (9.0683e-03)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n","Epoch: [119][270/391]\tTime  0.175 ( 0.175)\tLoss 1.1265e-02 (8.9100e-03)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n","Epoch: [119][300/391]\tTime  0.174 ( 0.175)\tLoss 1.0694e-02 (9.0144e-03)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n","Epoch: [119][330/391]\tTime  0.174 ( 0.175)\tLoss 8.3118e-03 (8.9677e-03)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n","Epoch: [119][360/391]\tTime  0.173 ( 0.175)\tLoss 7.8380e-03 (9.1079e-03)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n","Epoch: [119][390/391]\tTime  0.157 ( 0.175)\tLoss 1.0797e-02 (9.1380e-03)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.918 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 77.000 || Acc@5 93.860\n","==> 72.82 seconds to train this epoch\n","\n","\n","----- epoch: 120, lr: 0.0008000000000000003 -----\n","Epoch: [120][  0/391]\tTime  0.236 ( 0.236)\tLoss 9.3905e-03 (9.3905e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [120][ 30/391]\tTime  0.174 ( 0.177)\tLoss 4.1094e-03 (8.5965e-03)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n","Epoch: [120][ 60/391]\tTime  0.176 ( 0.176)\tLoss 9.4032e-03 (8.0239e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [120][ 90/391]\tTime  0.175 ( 0.176)\tLoss 6.9133e-03 (8.1608e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [120][120/391]\tTime  0.175 ( 0.176)\tLoss 9.3122e-03 (8.2241e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [120][150/391]\tTime  0.177 ( 0.176)\tLoss 8.2605e-03 (8.0515e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [120][180/391]\tTime  0.178 ( 0.176)\tLoss 5.4805e-03 (8.3149e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [120][210/391]\tTime  0.173 ( 0.176)\tLoss 7.8497e-03 (8.3716e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [120][240/391]\tTime  0.177 ( 0.176)\tLoss 6.2007e-03 (8.3863e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [120][270/391]\tTime  0.174 ( 0.176)\tLoss 1.9524e-02 (8.3279e-03)\tAcc@1  99.22 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [120][300/391]\tTime  0.175 ( 0.176)\tLoss 6.5562e-03 (8.3714e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [120][330/391]\tTime  0.175 ( 0.176)\tLoss 5.0765e-03 (8.2582e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [120][360/391]\tTime  0.175 ( 0.176)\tLoss 7.2559e-03 (8.1571e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [120][390/391]\tTime  0.159 ( 0.176)\tLoss 2.0443e-02 (8.1579e-03)\tAcc@1  98.75 ( 99.95)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.948 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 77.110 || Acc@5 93.930\n","==> 72.96 seconds to train this epoch\n","\n","\n","----- epoch: 121, lr: 0.0008000000000000003 -----\n","Epoch: [121][  0/391]\tTime  0.231 ( 0.231)\tLoss 6.5641e-03 (6.5641e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [121][ 30/391]\tTime  0.175 ( 0.177)\tLoss 8.1587e-03 (8.1409e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [121][ 60/391]\tTime  0.174 ( 0.176)\tLoss 1.0322e-02 (7.9829e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [121][ 90/391]\tTime  0.176 ( 0.176)\tLoss 5.9136e-03 (8.0024e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [121][120/391]\tTime  0.174 ( 0.176)\tLoss 5.4667e-03 (7.9116e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [121][150/391]\tTime  0.176 ( 0.176)\tLoss 6.0078e-03 (7.8542e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [121][180/391]\tTime  0.177 ( 0.176)\tLoss 7.0928e-03 (7.8689e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [121][210/391]\tTime  0.176 ( 0.175)\tLoss 7.0344e-03 (7.9733e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [121][240/391]\tTime  0.175 ( 0.175)\tLoss 4.5461e-03 (8.2250e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [121][270/391]\tTime  0.176 ( 0.175)\tLoss 5.5916e-03 (8.1386e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [121][300/391]\tTime  0.176 ( 0.175)\tLoss 6.3656e-03 (8.2045e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [121][330/391]\tTime  0.175 ( 0.175)\tLoss 1.6043e-02 (8.1758e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [121][360/391]\tTime  0.173 ( 0.175)\tLoss 5.5289e-03 (8.1042e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [121][390/391]\tTime  0.157 ( 0.175)\tLoss 9.7856e-03 (8.0992e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.954 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 77.090 || Acc@5 93.940\n","==> 72.88 seconds to train this epoch\n","\n","\n","----- epoch: 122, lr: 0.0008000000000000003 -----\n","Epoch: [122][  0/391]\tTime  0.228 ( 0.228)\tLoss 9.1989e-03 (9.1989e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [122][ 30/391]\tTime  0.174 ( 0.177)\tLoss 7.2301e-03 (8.4105e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [122][ 60/391]\tTime  0.175 ( 0.176)\tLoss 8.0083e-03 (8.0116e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [122][ 90/391]\tTime  0.175 ( 0.176)\tLoss 2.2746e-02 (7.9414e-03)\tAcc@1  99.22 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [122][120/391]\tTime  0.175 ( 0.175)\tLoss 7.2649e-03 (8.1429e-03)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n","Epoch: [122][150/391]\tTime  0.175 ( 0.175)\tLoss 8.0638e-03 (8.2587e-03)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n","Epoch: [122][180/391]\tTime  0.176 ( 0.175)\tLoss 7.7467e-03 (8.2198e-03)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n","Epoch: [122][210/391]\tTime  0.175 ( 0.175)\tLoss 1.0376e-02 (8.2145e-03)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n","Epoch: [122][240/391]\tTime  0.176 ( 0.175)\tLoss 3.7558e-03 (8.1340e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [122][270/391]\tTime  0.178 ( 0.175)\tLoss 8.3014e-03 (8.1406e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [122][300/391]\tTime  0.170 ( 0.175)\tLoss 9.2431e-03 (8.0749e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [122][330/391]\tTime  0.175 ( 0.175)\tLoss 6.0877e-03 (7.9678e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [122][360/391]\tTime  0.176 ( 0.175)\tLoss 5.7984e-03 (7.9701e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [122][390/391]\tTime  0.159 ( 0.175)\tLoss 1.4794e-02 (8.0524e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.946 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 77.090 || Acc@5 93.990\n","==> 72.83 seconds to train this epoch\n","\n","\n","----- epoch: 123, lr: 0.0008000000000000003 -----\n","Epoch: [123][  0/391]\tTime  0.236 ( 0.236)\tLoss 7.8268e-03 (7.8268e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [123][ 30/391]\tTime  0.175 ( 0.177)\tLoss 9.1960e-03 (8.6133e-03)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n","Epoch: [123][ 60/391]\tTime  0.173 ( 0.176)\tLoss 7.6857e-03 (7.8686e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [123][ 90/391]\tTime  0.176 ( 0.176)\tLoss 4.4459e-03 (8.1815e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [123][120/391]\tTime  0.176 ( 0.175)\tLoss 5.3847e-03 (7.8973e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [123][150/391]\tTime  0.176 ( 0.175)\tLoss 4.4785e-03 (7.7843e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [123][180/391]\tTime  0.170 ( 0.175)\tLoss 4.5003e-03 (7.8739e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [123][210/391]\tTime  0.176 ( 0.175)\tLoss 8.8379e-03 (7.8837e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [123][240/391]\tTime  0.174 ( 0.175)\tLoss 8.6381e-03 (7.8674e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [123][270/391]\tTime  0.176 ( 0.175)\tLoss 5.6454e-03 (7.8081e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [123][300/391]\tTime  0.176 ( 0.175)\tLoss 5.9630e-03 (7.8371e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [123][330/391]\tTime  0.176 ( 0.175)\tLoss 6.7248e-03 (7.7458e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [123][360/391]\tTime  0.170 ( 0.175)\tLoss 6.8282e-03 (7.8569e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [123][390/391]\tTime  0.158 ( 0.175)\tLoss 1.3545e-02 (7.7974e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.958 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 77.070 || Acc@5 94.000\n","==> 72.80 seconds to train this epoch\n","\n","\n","----- epoch: 124, lr: 0.0008000000000000003 -----\n","Epoch: [124][  0/391]\tTime  0.243 ( 0.243)\tLoss 5.5649e-03 (5.5649e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [124][ 30/391]\tTime  0.179 ( 0.177)\tLoss 1.1603e-02 (8.6820e-03)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n","Epoch: [124][ 60/391]\tTime  0.175 ( 0.176)\tLoss 6.6711e-03 (8.1504e-03)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n","Epoch: [124][ 90/391]\tTime  0.176 ( 0.176)\tLoss 9.1881e-03 (8.0925e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [124][120/391]\tTime  0.175 ( 0.175)\tLoss 1.1249e-02 (8.0698e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [124][150/391]\tTime  0.176 ( 0.175)\tLoss 1.3315e-02 (8.0715e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [124][180/391]\tTime  0.176 ( 0.175)\tLoss 1.0407e-02 (8.0134e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [124][210/391]\tTime  0.180 ( 0.175)\tLoss 7.8563e-03 (8.0714e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [124][240/391]\tTime  0.171 ( 0.175)\tLoss 7.2105e-03 (7.9572e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [124][270/391]\tTime  0.175 ( 0.175)\tLoss 6.7934e-03 (7.9552e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [124][300/391]\tTime  0.174 ( 0.175)\tLoss 9.7399e-03 (7.8806e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [124][330/391]\tTime  0.176 ( 0.175)\tLoss 6.1357e-03 (7.7512e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [124][360/391]\tTime  0.176 ( 0.175)\tLoss 4.5882e-03 (7.7610e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [124][390/391]\tTime  0.157 ( 0.175)\tLoss 1.7442e-02 (7.7407e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.960 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 77.010 || Acc@5 93.960\n","==> 72.82 seconds to train this epoch\n","\n","\n","----- epoch: 125, lr: 0.0008000000000000003 -----\n","Epoch: [125][  0/391]\tTime  0.239 ( 0.239)\tLoss 8.7371e-03 (8.7371e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [125][ 30/391]\tTime  0.176 ( 0.177)\tLoss 7.3482e-03 (7.9472e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [125][ 60/391]\tTime  0.174 ( 0.176)\tLoss 6.9380e-03 (7.9439e-03)\tAcc@1 100.00 ( 99.99)\tAcc@5 100.00 (100.00)\n","Epoch: [125][ 90/391]\tTime  0.175 ( 0.176)\tLoss 6.3008e-03 (7.6314e-03)\tAcc@1 100.00 ( 99.99)\tAcc@5 100.00 (100.00)\n","Epoch: [125][120/391]\tTime  0.174 ( 0.176)\tLoss 5.1129e-03 (7.8751e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [125][150/391]\tTime  0.176 ( 0.175)\tLoss 6.1703e-03 (7.9504e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [125][180/391]\tTime  0.175 ( 0.175)\tLoss 5.6261e-03 (7.9169e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [125][210/391]\tTime  0.177 ( 0.175)\tLoss 7.4094e-03 (7.7861e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [125][240/391]\tTime  0.175 ( 0.175)\tLoss 1.0043e-02 (7.7139e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [125][270/391]\tTime  0.177 ( 0.175)\tLoss 9.9058e-03 (7.6618e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [125][300/391]\tTime  0.174 ( 0.175)\tLoss 6.7101e-03 (7.6160e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [125][330/391]\tTime  0.175 ( 0.175)\tLoss 5.2567e-03 (7.6634e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [125][360/391]\tTime  0.175 ( 0.175)\tLoss 8.2826e-03 (7.6728e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [125][390/391]\tTime  0.157 ( 0.175)\tLoss 6.2513e-03 (7.5925e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.968 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 77.060 || Acc@5 94.070\n","==> 72.82 seconds to train this epoch\n","\n","\n","----- epoch: 126, lr: 0.0008000000000000003 -----\n","Epoch: [126][  0/391]\tTime  0.237 ( 0.237)\tLoss 5.1117e-03 (5.1117e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [126][ 30/391]\tTime  0.173 ( 0.176)\tLoss 8.8448e-03 (8.0544e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [126][ 60/391]\tTime  0.175 ( 0.176)\tLoss 8.7228e-03 (8.0357e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [126][ 90/391]\tTime  0.175 ( 0.176)\tLoss 9.3331e-03 (8.1802e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [126][120/391]\tTime  0.176 ( 0.175)\tLoss 1.1573e-02 (8.1365e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [126][150/391]\tTime  0.176 ( 0.175)\tLoss 5.1678e-03 (7.8302e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [126][180/391]\tTime  0.177 ( 0.175)\tLoss 7.2653e-03 (7.8317e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [126][210/391]\tTime  0.172 ( 0.175)\tLoss 2.3084e-02 (7.7802e-03)\tAcc@1  99.22 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [126][240/391]\tTime  0.175 ( 0.175)\tLoss 5.7992e-03 (7.7348e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [126][270/391]\tTime  0.175 ( 0.175)\tLoss 1.0715e-02 (7.6753e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [126][300/391]\tTime  0.176 ( 0.175)\tLoss 7.0404e-03 (7.6608e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [126][330/391]\tTime  0.177 ( 0.175)\tLoss 5.1521e-03 (7.5968e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [126][360/391]\tTime  0.176 ( 0.175)\tLoss 5.7632e-03 (7.6492e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [126][390/391]\tTime  0.159 ( 0.175)\tLoss 7.2530e-03 (7.7419e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.962 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 77.190 || Acc@5 93.920\n","==> 72.91 seconds to train this epoch\n","\n","\n","----- epoch: 127, lr: 0.0008000000000000003 -----\n","Epoch: [127][  0/391]\tTime  0.247 ( 0.247)\tLoss 8.1456e-03 (8.1456e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [127][ 30/391]\tTime  0.174 ( 0.177)\tLoss 6.8214e-03 (7.6390e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [127][ 60/391]\tTime  0.173 ( 0.176)\tLoss 6.0530e-03 (8.4597e-03)\tAcc@1 100.00 ( 99.91)\tAcc@5 100.00 (100.00)\n","Epoch: [127][ 90/391]\tTime  0.175 ( 0.176)\tLoss 4.1768e-03 (7.9197e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [127][120/391]\tTime  0.176 ( 0.176)\tLoss 4.3881e-03 (7.8409e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [127][150/391]\tTime  0.175 ( 0.176)\tLoss 4.2998e-03 (7.7255e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [127][180/391]\tTime  0.177 ( 0.176)\tLoss 5.6040e-03 (7.6106e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [127][210/391]\tTime  0.174 ( 0.176)\tLoss 1.1564e-02 (7.6023e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [127][240/391]\tTime  0.175 ( 0.176)\tLoss 8.3711e-03 (7.7356e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [127][270/391]\tTime  0.175 ( 0.175)\tLoss 6.3224e-03 (7.6870e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [127][300/391]\tTime  0.175 ( 0.175)\tLoss 9.3884e-03 (7.6135e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [127][330/391]\tTime  0.177 ( 0.175)\tLoss 5.9846e-03 (7.6658e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [127][360/391]\tTime  0.176 ( 0.175)\tLoss 6.1068e-03 (7.6631e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [127][390/391]\tTime  0.157 ( 0.175)\tLoss 7.6348e-03 (7.7497e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.948 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 77.160 || Acc@5 93.900\n","==> 72.90 seconds to train this epoch\n","\n","\n","----- epoch: 128, lr: 0.0008000000000000003 -----\n","Epoch: [128][  0/391]\tTime  0.241 ( 0.241)\tLoss 5.5196e-03 (5.5196e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [128][ 30/391]\tTime  0.176 ( 0.177)\tLoss 6.0470e-03 (6.8711e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [128][ 60/391]\tTime  0.174 ( 0.176)\tLoss 4.8083e-03 (7.1389e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [128][ 90/391]\tTime  0.174 ( 0.176)\tLoss 6.8035e-03 (7.5763e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [128][120/391]\tTime  0.175 ( 0.176)\tLoss 6.8353e-03 (7.5285e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [128][150/391]\tTime  0.175 ( 0.175)\tLoss 6.5033e-03 (7.5093e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [128][180/391]\tTime  0.175 ( 0.175)\tLoss 4.9765e-03 (7.4510e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [128][210/391]\tTime  0.174 ( 0.175)\tLoss 6.5605e-03 (7.4708e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [128][240/391]\tTime  0.173 ( 0.175)\tLoss 6.6285e-03 (7.5246e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [128][270/391]\tTime  0.175 ( 0.175)\tLoss 6.3135e-03 (7.6464e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [128][300/391]\tTime  0.175 ( 0.175)\tLoss 4.0716e-03 (7.5333e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [128][330/391]\tTime  0.175 ( 0.175)\tLoss 8.1795e-03 (7.5674e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [128][360/391]\tTime  0.175 ( 0.175)\tLoss 1.0152e-02 (7.6068e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [128][390/391]\tTime  0.158 ( 0.175)\tLoss 8.4037e-03 (7.5693e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.958 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 77.350 || Acc@5 93.990\n","==> 72.81 seconds to train this epoch\n","\n","\n","----- epoch: 129, lr: 0.0008000000000000003 -----\n","Epoch: [129][  0/391]\tTime  0.245 ( 0.245)\tLoss 1.3223e-02 (1.3223e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [129][ 30/391]\tTime  0.174 ( 0.177)\tLoss 5.3439e-03 (9.1321e-03)\tAcc@1 100.00 ( 99.90)\tAcc@5 100.00 (100.00)\n","Epoch: [129][ 60/391]\tTime  0.174 ( 0.176)\tLoss 6.7480e-03 (8.9349e-03)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n","Epoch: [129][ 90/391]\tTime  0.175 ( 0.176)\tLoss 5.1123e-03 (8.3852e-03)\tAcc@1 100.00 ( 99.93)\tAcc@5 100.00 (100.00)\n","Epoch: [129][120/391]\tTime  0.174 ( 0.175)\tLoss 1.0259e-02 (8.3500e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [129][150/391]\tTime  0.176 ( 0.175)\tLoss 1.1185e-02 (8.1300e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [129][180/391]\tTime  0.175 ( 0.175)\tLoss 8.7154e-03 (8.0666e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [129][210/391]\tTime  0.175 ( 0.175)\tLoss 5.5277e-03 (8.1964e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [129][240/391]\tTime  0.176 ( 0.175)\tLoss 6.5477e-03 (8.1244e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [129][270/391]\tTime  0.174 ( 0.175)\tLoss 4.4719e-03 (8.1259e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [129][300/391]\tTime  0.175 ( 0.175)\tLoss 6.1713e-03 (8.0819e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [129][330/391]\tTime  0.174 ( 0.175)\tLoss 6.4319e-03 (8.0711e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [129][360/391]\tTime  0.176 ( 0.175)\tLoss 4.7482e-03 (7.9308e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [129][390/391]\tTime  0.157 ( 0.175)\tLoss 7.1611e-03 (7.8820e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.948 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 77.230 || Acc@5 94.040\n","==> 72.81 seconds to train this epoch\n","\n","\n","----- epoch: 130, lr: 0.0008000000000000003 -----\n","Epoch: [130][  0/391]\tTime  0.236 ( 0.236)\tLoss 1.0861e-02 (1.0861e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [130][ 30/391]\tTime  0.175 ( 0.176)\tLoss 5.6396e-03 (6.7660e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [130][ 60/391]\tTime  0.175 ( 0.176)\tLoss 6.2028e-03 (7.0747e-03)\tAcc@1 100.00 ( 99.99)\tAcc@5 100.00 (100.00)\n","Epoch: [130][ 90/391]\tTime  0.174 ( 0.175)\tLoss 4.1995e-03 (6.9258e-03)\tAcc@1 100.00 ( 99.99)\tAcc@5 100.00 (100.00)\n","Epoch: [130][120/391]\tTime  0.176 ( 0.175)\tLoss 1.0946e-02 (7.0749e-03)\tAcc@1 100.00 ( 99.99)\tAcc@5 100.00 (100.00)\n","Epoch: [130][150/391]\tTime  0.175 ( 0.175)\tLoss 7.0486e-03 (7.0429e-03)\tAcc@1 100.00 ( 99.98)\tAcc@5 100.00 (100.00)\n","Epoch: [130][180/391]\tTime  0.173 ( 0.175)\tLoss 5.2593e-03 (7.0815e-03)\tAcc@1 100.00 ( 99.98)\tAcc@5 100.00 (100.00)\n","Epoch: [130][210/391]\tTime  0.175 ( 0.175)\tLoss 1.0709e-02 (7.2007e-03)\tAcc@1 100.00 ( 99.98)\tAcc@5 100.00 (100.00)\n","Epoch: [130][240/391]\tTime  0.175 ( 0.175)\tLoss 5.8805e-03 (7.2134e-03)\tAcc@1 100.00 ( 99.98)\tAcc@5 100.00 (100.00)\n","Epoch: [130][270/391]\tTime  0.174 ( 0.175)\tLoss 7.3067e-03 (7.3150e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [130][300/391]\tTime  0.175 ( 0.175)\tLoss 5.7123e-03 (7.3528e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [130][330/391]\tTime  0.176 ( 0.175)\tLoss 9.1120e-03 (7.4587e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [130][360/391]\tTime  0.175 ( 0.175)\tLoss 7.5607e-03 (7.5107e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [130][390/391]\tTime  0.162 ( 0.175)\tLoss 7.8909e-03 (7.5967e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.960 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 77.210 || Acc@5 94.020\n","==> 72.80 seconds to train this epoch\n","\n","\n","----- epoch: 131, lr: 0.0008000000000000003 -----\n","Epoch: [131][  0/391]\tTime  0.234 ( 0.234)\tLoss 6.7272e-03 (6.7272e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [131][ 30/391]\tTime  0.175 ( 0.176)\tLoss 3.7407e-03 (7.4375e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [131][ 60/391]\tTime  0.175 ( 0.176)\tLoss 5.9194e-03 (7.4764e-03)\tAcc@1 100.00 ( 99.99)\tAcc@5 100.00 (100.00)\n","Epoch: [131][ 90/391]\tTime  0.175 ( 0.176)\tLoss 4.2763e-03 (7.4571e-03)\tAcc@1 100.00 ( 99.98)\tAcc@5 100.00 (100.00)\n","Epoch: [131][120/391]\tTime  0.176 ( 0.175)\tLoss 4.4755e-03 (7.5605e-03)\tAcc@1 100.00 ( 99.98)\tAcc@5 100.00 (100.00)\n","Epoch: [131][150/391]\tTime  0.175 ( 0.175)\tLoss 5.0267e-03 (7.6350e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [131][180/391]\tTime  0.174 ( 0.175)\tLoss 4.7210e-03 (7.5010e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [131][210/391]\tTime  0.175 ( 0.175)\tLoss 7.7815e-03 (7.4613e-03)\tAcc@1 100.00 ( 99.98)\tAcc@5 100.00 (100.00)\n","Epoch: [131][240/391]\tTime  0.175 ( 0.175)\tLoss 8.0618e-03 (7.4889e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [131][270/391]\tTime  0.180 ( 0.175)\tLoss 6.6563e-03 (7.5066e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [131][300/391]\tTime  0.174 ( 0.175)\tLoss 5.1981e-03 (7.5373e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [131][330/391]\tTime  0.176 ( 0.175)\tLoss 6.3260e-03 (7.5885e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [131][360/391]\tTime  0.173 ( 0.175)\tLoss 1.0585e-02 (7.5620e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [131][390/391]\tTime  0.158 ( 0.175)\tLoss 1.3060e-02 (7.5568e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.962 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 77.160 || Acc@5 93.980\n","==> 72.80 seconds to train this epoch\n","\n","\n","----- epoch: 132, lr: 0.0008000000000000003 -----\n","Epoch: [132][  0/391]\tTime  0.235 ( 0.235)\tLoss 1.3640e-02 (1.3640e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [132][ 30/391]\tTime  0.175 ( 0.176)\tLoss 9.6246e-03 (7.0240e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [132][ 60/391]\tTime  0.175 ( 0.176)\tLoss 8.9644e-03 (7.0942e-03)\tAcc@1 100.00 ( 99.99)\tAcc@5 100.00 (100.00)\n","Epoch: [132][ 90/391]\tTime  0.170 ( 0.175)\tLoss 8.5870e-03 (7.3190e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [132][120/391]\tTime  0.178 ( 0.175)\tLoss 5.4215e-03 (7.4519e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [132][150/391]\tTime  0.174 ( 0.175)\tLoss 7.7302e-03 (7.3613e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [132][180/391]\tTime  0.174 ( 0.175)\tLoss 8.7354e-03 (7.4407e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [132][210/391]\tTime  0.174 ( 0.175)\tLoss 1.0907e-02 (7.5977e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [132][240/391]\tTime  0.175 ( 0.175)\tLoss 7.2768e-03 (7.5566e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [132][270/391]\tTime  0.175 ( 0.175)\tLoss 7.3800e-03 (7.5465e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [132][300/391]\tTime  0.174 ( 0.175)\tLoss 9.3777e-03 (7.5326e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [132][330/391]\tTime  0.178 ( 0.175)\tLoss 6.1735e-03 (7.5117e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [132][360/391]\tTime  0.175 ( 0.175)\tLoss 7.5344e-03 (7.4905e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [132][390/391]\tTime  0.158 ( 0.175)\tLoss 1.1923e-02 (7.5226e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.962 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 77.140 || Acc@5 93.910\n","==> 72.78 seconds to train this epoch\n","\n","\n","----- epoch: 133, lr: 0.0008000000000000003 -----\n","Epoch: [133][  0/391]\tTime  0.227 ( 0.227)\tLoss 3.7987e-03 (3.7987e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [133][ 30/391]\tTime  0.176 ( 0.176)\tLoss 8.2713e-03 (6.7558e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [133][ 60/391]\tTime  0.175 ( 0.175)\tLoss 6.3811e-03 (7.2670e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [133][ 90/391]\tTime  0.174 ( 0.175)\tLoss 8.3043e-03 (7.5426e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [133][120/391]\tTime  0.176 ( 0.175)\tLoss 8.2560e-03 (7.4078e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [133][150/391]\tTime  0.175 ( 0.175)\tLoss 1.4663e-02 (7.4587e-03)\tAcc@1  99.22 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [133][180/391]\tTime  0.175 ( 0.175)\tLoss 1.2175e-02 (7.4094e-03)\tAcc@1  99.22 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [133][210/391]\tTime  0.175 ( 0.175)\tLoss 8.4841e-03 (7.4277e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [133][240/391]\tTime  0.175 ( 0.175)\tLoss 4.6670e-03 (7.2987e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [133][270/391]\tTime  0.175 ( 0.175)\tLoss 5.7028e-03 (7.2568e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [133][300/391]\tTime  0.178 ( 0.175)\tLoss 5.5242e-03 (7.2900e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [133][330/391]\tTime  0.177 ( 0.175)\tLoss 2.2707e-02 (7.3558e-03)\tAcc@1  99.22 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [133][360/391]\tTime  0.176 ( 0.175)\tLoss 8.4247e-03 (7.4070e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [133][390/391]\tTime  0.156 ( 0.175)\tLoss 8.2066e-03 (7.3992e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.958 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 77.240 || Acc@5 93.910\n","==> 72.76 seconds to train this epoch\n","\n","\n","----- epoch: 134, lr: 0.0008000000000000003 -----\n","Epoch: [134][  0/391]\tTime  0.235 ( 0.235)\tLoss 9.1265e-03 (9.1265e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [134][ 30/391]\tTime  0.173 ( 0.176)\tLoss 8.7282e-03 (7.6756e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [134][ 60/391]\tTime  0.174 ( 0.176)\tLoss 1.9234e-02 (7.4652e-03)\tAcc@1  99.22 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [134][ 90/391]\tTime  0.175 ( 0.175)\tLoss 6.6668e-03 (7.3859e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [134][120/391]\tTime  0.174 ( 0.175)\tLoss 6.8890e-03 (7.3235e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [134][150/391]\tTime  0.175 ( 0.175)\tLoss 9.3782e-03 (7.3856e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [134][180/391]\tTime  0.176 ( 0.175)\tLoss 6.0967e-03 (7.3878e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [134][210/391]\tTime  0.174 ( 0.175)\tLoss 5.3416e-03 (7.3878e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [134][240/391]\tTime  0.175 ( 0.175)\tLoss 5.1793e-03 (7.3818e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [134][270/391]\tTime  0.179 ( 0.175)\tLoss 3.7957e-03 (7.3613e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [134][300/391]\tTime  0.171 ( 0.175)\tLoss 9.6994e-03 (7.2528e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [134][330/391]\tTime  0.175 ( 0.175)\tLoss 6.0593e-03 (7.2669e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [134][360/391]\tTime  0.173 ( 0.175)\tLoss 5.9524e-03 (7.2766e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [134][390/391]\tTime  0.158 ( 0.175)\tLoss 7.1116e-03 (7.3214e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.964 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 77.390 || Acc@5 94.000\n","==> 72.82 seconds to train this epoch\n","\n","\n","----- epoch: 135, lr: 0.0008000000000000003 -----\n","Epoch: [135][  0/391]\tTime  0.239 ( 0.239)\tLoss 4.3153e-03 (4.3153e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [135][ 30/391]\tTime  0.175 ( 0.177)\tLoss 7.2110e-03 (8.2594e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [135][ 60/391]\tTime  0.172 ( 0.176)\tLoss 8.8478e-03 (7.9442e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [135][ 90/391]\tTime  0.179 ( 0.176)\tLoss 2.1434e-02 (7.9055e-03)\tAcc@1  99.22 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [135][120/391]\tTime  0.175 ( 0.176)\tLoss 8.4163e-03 (7.7419e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [135][150/391]\tTime  0.176 ( 0.175)\tLoss 8.9288e-03 (7.6723e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [135][180/391]\tTime  0.176 ( 0.175)\tLoss 1.0662e-02 (7.5753e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [135][210/391]\tTime  0.177 ( 0.175)\tLoss 5.6146e-03 (7.5534e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [135][240/391]\tTime  0.175 ( 0.175)\tLoss 4.7828e-03 (7.5723e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [135][270/391]\tTime  0.175 ( 0.175)\tLoss 1.0420e-02 (7.5731e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [135][300/391]\tTime  0.176 ( 0.175)\tLoss 1.2239e-02 (7.5616e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [135][330/391]\tTime  0.174 ( 0.175)\tLoss 1.0103e-02 (7.5249e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [135][360/391]\tTime  0.175 ( 0.175)\tLoss 6.2000e-03 (7.5181e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [135][390/391]\tTime  0.159 ( 0.175)\tLoss 7.8791e-03 (7.4770e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.962 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 77.190 || Acc@5 93.930\n","==> 72.90 seconds to train this epoch\n","\n","\n","----- epoch: 136, lr: 0.0008000000000000003 -----\n","Epoch: [136][  0/391]\tTime  0.240 ( 0.240)\tLoss 5.4560e-03 (5.4560e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [136][ 30/391]\tTime  0.175 ( 0.177)\tLoss 6.0987e-03 (6.8465e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [136][ 60/391]\tTime  0.175 ( 0.176)\tLoss 5.2372e-03 (6.9844e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [136][ 90/391]\tTime  0.176 ( 0.176)\tLoss 6.2312e-03 (6.9398e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [136][120/391]\tTime  0.173 ( 0.176)\tLoss 5.3424e-03 (6.8932e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [136][150/391]\tTime  0.176 ( 0.175)\tLoss 6.2752e-03 (7.0215e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [136][180/391]\tTime  0.175 ( 0.175)\tLoss 5.8005e-03 (7.0761e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [136][210/391]\tTime  0.176 ( 0.175)\tLoss 1.4799e-02 (7.1529e-03)\tAcc@1  99.22 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [136][240/391]\tTime  0.174 ( 0.175)\tLoss 5.8244e-03 (7.2573e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [136][270/391]\tTime  0.174 ( 0.175)\tLoss 8.1909e-03 (7.3241e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [136][300/391]\tTime  0.174 ( 0.175)\tLoss 6.4675e-03 (7.3942e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [136][330/391]\tTime  0.177 ( 0.175)\tLoss 8.5577e-03 (7.5084e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [136][360/391]\tTime  0.176 ( 0.175)\tLoss 6.9909e-03 (7.4860e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [136][390/391]\tTime  0.157 ( 0.175)\tLoss 9.3410e-03 (7.4826e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.954 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 77.250 || Acc@5 93.930\n","==> 72.83 seconds to train this epoch\n","\n","\n","----- epoch: 137, lr: 0.0008000000000000003 -----\n","Epoch: [137][  0/391]\tTime  0.238 ( 0.238)\tLoss 7.2793e-03 (7.2793e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [137][ 30/391]\tTime  0.176 ( 0.176)\tLoss 5.8413e-03 (7.1371e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [137][ 60/391]\tTime  0.175 ( 0.176)\tLoss 4.2442e-03 (6.8947e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [137][ 90/391]\tTime  0.175 ( 0.176)\tLoss 4.1524e-03 (6.9770e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [137][120/391]\tTime  0.175 ( 0.175)\tLoss 4.7701e-03 (7.3234e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [137][150/391]\tTime  0.176 ( 0.175)\tLoss 1.5814e-02 (7.3140e-03)\tAcc@1  99.22 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [137][180/391]\tTime  0.180 ( 0.175)\tLoss 4.5959e-03 (7.4078e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [137][210/391]\tTime  0.172 ( 0.175)\tLoss 7.5234e-03 (7.3982e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [137][240/391]\tTime  0.175 ( 0.175)\tLoss 6.5089e-03 (7.4578e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [137][270/391]\tTime  0.177 ( 0.175)\tLoss 7.1507e-03 (7.5661e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [137][300/391]\tTime  0.175 ( 0.175)\tLoss 5.5430e-03 (7.5542e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [137][330/391]\tTime  0.177 ( 0.175)\tLoss 6.1627e-03 (7.5311e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [137][360/391]\tTime  0.173 ( 0.175)\tLoss 4.4025e-03 (7.6370e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [137][390/391]\tTime  0.157 ( 0.175)\tLoss 1.2286e-02 (7.6411e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.954 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 77.250 || Acc@5 94.040\n","==> 72.80 seconds to train this epoch\n","\n","\n","----- epoch: 138, lr: 0.0008000000000000003 -----\n","Epoch: [138][  0/391]\tTime  0.238 ( 0.238)\tLoss 7.1173e-03 (7.1173e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [138][ 30/391]\tTime  0.174 ( 0.176)\tLoss 6.6385e-03 (7.6812e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [138][ 60/391]\tTime  0.177 ( 0.176)\tLoss 6.4470e-03 (7.9103e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [138][ 90/391]\tTime  0.184 ( 0.176)\tLoss 7.5793e-03 (7.7536e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [138][120/391]\tTime  0.174 ( 0.175)\tLoss 9.0123e-03 (7.6069e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [138][150/391]\tTime  0.174 ( 0.175)\tLoss 5.7759e-03 (7.7293e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [138][180/391]\tTime  0.175 ( 0.175)\tLoss 8.4239e-03 (7.6398e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [138][210/391]\tTime  0.174 ( 0.175)\tLoss 7.3731e-03 (7.5087e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [138][240/391]\tTime  0.179 ( 0.175)\tLoss 7.9547e-03 (7.4489e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [138][270/391]\tTime  0.175 ( 0.175)\tLoss 5.7324e-03 (7.4910e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [138][300/391]\tTime  0.175 ( 0.175)\tLoss 8.5505e-03 (7.4062e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [138][330/391]\tTime  0.175 ( 0.175)\tLoss 6.4438e-03 (7.3753e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [138][360/391]\tTime  0.176 ( 0.175)\tLoss 3.2373e-03 (7.4363e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [138][390/391]\tTime  0.159 ( 0.175)\tLoss 7.6611e-03 (7.4578e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.966 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 77.270 || Acc@5 94.070\n","==> 72.79 seconds to train this epoch\n","\n","\n","----- epoch: 139, lr: 0.0008000000000000003 -----\n","Epoch: [139][  0/391]\tTime  0.244 ( 0.244)\tLoss 6.1337e-03 (6.1337e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [139][ 30/391]\tTime  0.175 ( 0.176)\tLoss 6.1486e-03 (6.7561e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [139][ 60/391]\tTime  0.174 ( 0.176)\tLoss 1.1252e-02 (7.4757e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [139][ 90/391]\tTime  0.175 ( 0.175)\tLoss 7.7498e-03 (7.7278e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [139][120/391]\tTime  0.177 ( 0.175)\tLoss 7.6488e-03 (7.5961e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [139][150/391]\tTime  0.175 ( 0.175)\tLoss 6.9773e-03 (7.3909e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [139][180/391]\tTime  0.175 ( 0.175)\tLoss 5.7946e-03 (7.4343e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [139][210/391]\tTime  0.175 ( 0.175)\tLoss 4.9820e-03 (7.6664e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [139][240/391]\tTime  0.176 ( 0.175)\tLoss 8.6041e-03 (7.6973e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [139][270/391]\tTime  0.175 ( 0.175)\tLoss 6.5727e-03 (7.6660e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [139][300/391]\tTime  0.175 ( 0.175)\tLoss 8.7156e-03 (7.6239e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [139][330/391]\tTime  0.173 ( 0.175)\tLoss 8.0221e-03 (7.5421e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [139][360/391]\tTime  0.175 ( 0.175)\tLoss 8.4820e-03 (7.5146e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [139][390/391]\tTime  0.158 ( 0.175)\tLoss 8.1125e-03 (7.5330e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.950 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 77.130 || Acc@5 94.030\n","==> 72.77 seconds to train this epoch\n","\n","\n","----- epoch: 140, lr: 0.0008000000000000003 -----\n","Epoch: [140][  0/391]\tTime  0.232 ( 0.232)\tLoss 7.8154e-03 (7.8154e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [140][ 30/391]\tTime  0.176 ( 0.176)\tLoss 5.1903e-03 (6.8110e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [140][ 60/391]\tTime  0.174 ( 0.176)\tLoss 4.7027e-03 (7.2480e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [140][ 90/391]\tTime  0.175 ( 0.175)\tLoss 8.3505e-03 (7.6148e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [140][120/391]\tTime  0.175 ( 0.175)\tLoss 8.3090e-03 (7.5738e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [140][150/391]\tTime  0.174 ( 0.175)\tLoss 6.0310e-03 (7.4994e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [140][180/391]\tTime  0.176 ( 0.175)\tLoss 5.5264e-03 (7.4245e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [140][210/391]\tTime  0.173 ( 0.175)\tLoss 8.5441e-03 (7.3990e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [140][240/391]\tTime  0.174 ( 0.175)\tLoss 6.4875e-03 (7.4086e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [140][270/391]\tTime  0.175 ( 0.175)\tLoss 6.1638e-03 (7.4691e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [140][300/391]\tTime  0.175 ( 0.175)\tLoss 4.4962e-03 (7.5913e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [140][330/391]\tTime  0.175 ( 0.175)\tLoss 6.9222e-03 (7.6039e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [140][360/391]\tTime  0.175 ( 0.175)\tLoss 5.9988e-03 (7.5636e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [140][390/391]\tTime  0.161 ( 0.175)\tLoss 2.0357e-02 (7.5780e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.954 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 77.260 || Acc@5 94.100\n","==> 72.76 seconds to train this epoch\n","\n","\n","----- epoch: 141, lr: 0.0008000000000000003 -----\n","Epoch: [141][  0/391]\tTime  0.229 ( 0.229)\tLoss 7.1504e-03 (7.1504e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [141][ 30/391]\tTime  0.176 ( 0.176)\tLoss 6.1360e-03 (8.0892e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [141][ 60/391]\tTime  0.179 ( 0.176)\tLoss 8.9640e-03 (7.5316e-03)\tAcc@1 100.00 ( 99.99)\tAcc@5 100.00 (100.00)\n","Epoch: [141][ 90/391]\tTime  0.171 ( 0.175)\tLoss 8.0814e-03 (7.5498e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [141][120/391]\tTime  0.177 ( 0.175)\tLoss 1.1877e-02 (7.4828e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [141][150/391]\tTime  0.174 ( 0.175)\tLoss 5.4875e-03 (7.4487e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [141][180/391]\tTime  0.175 ( 0.175)\tLoss 4.4893e-03 (7.3082e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [141][210/391]\tTime  0.175 ( 0.175)\tLoss 6.5591e-03 (7.2947e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [141][240/391]\tTime  0.179 ( 0.175)\tLoss 5.3271e-03 (7.3874e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [141][270/391]\tTime  0.173 ( 0.175)\tLoss 1.0073e-02 (7.4177e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [141][300/391]\tTime  0.175 ( 0.175)\tLoss 2.0755e-02 (7.4812e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [141][330/391]\tTime  0.176 ( 0.175)\tLoss 8.4541e-03 (7.4717e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [141][360/391]\tTime  0.175 ( 0.175)\tLoss 6.3984e-03 (7.4430e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [141][390/391]\tTime  0.157 ( 0.175)\tLoss 8.5987e-03 (7.4789e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.968 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 77.340 || Acc@5 93.910\n","==> 72.72 seconds to train this epoch\n","\n","\n","----- epoch: 142, lr: 0.0008000000000000003 -----\n","Epoch: [142][  0/391]\tTime  0.239 ( 0.239)\tLoss 1.8917e-02 (1.8917e-02)\tAcc@1  99.22 ( 99.22)\tAcc@5 100.00 (100.00)\n","Epoch: [142][ 30/391]\tTime  0.177 ( 0.176)\tLoss 8.0628e-03 (7.9263e-03)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n","Epoch: [142][ 60/391]\tTime  0.175 ( 0.176)\tLoss 4.8590e-03 (7.6911e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [142][ 90/391]\tTime  0.175 ( 0.175)\tLoss 4.6400e-03 (7.7357e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [142][120/391]\tTime  0.181 ( 0.175)\tLoss 5.2173e-03 (7.5930e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [142][150/391]\tTime  0.172 ( 0.175)\tLoss 8.3019e-03 (7.5150e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [142][180/391]\tTime  0.175 ( 0.175)\tLoss 7.0242e-03 (7.5488e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [142][210/391]\tTime  0.175 ( 0.175)\tLoss 1.0527e-02 (7.4634e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [142][240/391]\tTime  0.174 ( 0.175)\tLoss 6.6741e-03 (7.3647e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [142][270/391]\tTime  0.177 ( 0.175)\tLoss 5.6692e-03 (7.3272e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [142][300/391]\tTime  0.174 ( 0.175)\tLoss 9.5188e-03 (7.3158e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [142][330/391]\tTime  0.174 ( 0.175)\tLoss 5.6721e-03 (7.3009e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [142][360/391]\tTime  0.176 ( 0.175)\tLoss 1.2639e-02 (7.3115e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [142][390/391]\tTime  0.158 ( 0.175)\tLoss 8.5666e-03 (7.3140e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.956 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 77.180 || Acc@5 94.060\n","==> 72.79 seconds to train this epoch\n","\n","\n","----- epoch: 143, lr: 0.0008000000000000003 -----\n","Epoch: [143][  0/391]\tTime  0.238 ( 0.238)\tLoss 9.1274e-03 (9.1274e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [143][ 30/391]\tTime  0.171 ( 0.177)\tLoss 5.2548e-03 (7.7996e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [143][ 60/391]\tTime  0.175 ( 0.176)\tLoss 6.0258e-03 (8.5189e-03)\tAcc@1 100.00 ( 99.92)\tAcc@5 100.00 (100.00)\n","Epoch: [143][ 90/391]\tTime  0.174 ( 0.176)\tLoss 5.9285e-03 (8.2594e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [143][120/391]\tTime  0.175 ( 0.176)\tLoss 7.3733e-03 (7.9490e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [143][150/391]\tTime  0.176 ( 0.175)\tLoss 8.3560e-03 (7.9280e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [143][180/391]\tTime  0.177 ( 0.175)\tLoss 6.7363e-03 (7.9728e-03)\tAcc@1 100.00 ( 99.94)\tAcc@5 100.00 (100.00)\n","Epoch: [143][210/391]\tTime  0.175 ( 0.175)\tLoss 4.9389e-03 (7.7248e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [143][240/391]\tTime  0.175 ( 0.175)\tLoss 8.1806e-03 (7.6109e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [143][270/391]\tTime  0.177 ( 0.175)\tLoss 6.5582e-03 (7.6234e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [143][300/391]\tTime  0.175 ( 0.175)\tLoss 6.0099e-03 (7.5799e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [143][330/391]\tTime  0.175 ( 0.175)\tLoss 7.9206e-03 (7.5735e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [143][360/391]\tTime  0.176 ( 0.175)\tLoss 1.3979e-02 (7.5223e-03)\tAcc@1  99.22 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [143][390/391]\tTime  0.156 ( 0.175)\tLoss 1.1643e-02 (7.4612e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.956 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 77.300 || Acc@5 94.060\n","==> 72.86 seconds to train this epoch\n","\n","\n","----- epoch: 144, lr: 0.0008000000000000003 -----\n","Epoch: [144][  0/391]\tTime  0.225 ( 0.225)\tLoss 5.6330e-03 (5.6330e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [144][ 30/391]\tTime  0.175 ( 0.176)\tLoss 6.0772e-03 (7.1437e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [144][ 60/391]\tTime  0.175 ( 0.176)\tLoss 5.0997e-03 (6.9373e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [144][ 90/391]\tTime  0.176 ( 0.175)\tLoss 6.7346e-03 (7.3733e-03)\tAcc@1 100.00 ( 99.98)\tAcc@5 100.00 (100.00)\n","Epoch: [144][120/391]\tTime  0.174 ( 0.175)\tLoss 5.8740e-03 (7.4420e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [144][150/391]\tTime  0.174 ( 0.175)\tLoss 8.9047e-03 (7.4115e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [144][180/391]\tTime  0.173 ( 0.175)\tLoss 5.4590e-03 (7.3460e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [144][210/391]\tTime  0.175 ( 0.175)\tLoss 5.2698e-03 (7.4530e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [144][240/391]\tTime  0.174 ( 0.175)\tLoss 7.2653e-03 (7.3809e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [144][270/391]\tTime  0.173 ( 0.175)\tLoss 6.4124e-03 (7.4148e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [144][300/391]\tTime  0.175 ( 0.175)\tLoss 7.6450e-03 (7.4213e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [144][330/391]\tTime  0.176 ( 0.175)\tLoss 1.3720e-02 (7.4303e-03)\tAcc@1  99.22 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [144][360/391]\tTime  0.176 ( 0.175)\tLoss 7.0461e-03 (7.3671e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [144][390/391]\tTime  0.159 ( 0.175)\tLoss 5.9454e-03 (7.3626e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.966 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 77.270 || Acc@5 93.990\n","==> 72.88 seconds to train this epoch\n","\n","\n","----- epoch: 145, lr: 0.0008000000000000003 -----\n","Epoch: [145][  0/391]\tTime  0.241 ( 0.241)\tLoss 1.2364e-02 (1.2364e-02)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n","Epoch: [145][ 30/391]\tTime  0.175 ( 0.177)\tLoss 7.9350e-03 (7.8446e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [145][ 60/391]\tTime  0.175 ( 0.176)\tLoss 7.4471e-03 (8.0965e-03)\tAcc@1 100.00 ( 99.95)\tAcc@5 100.00 (100.00)\n","Epoch: [145][ 90/391]\tTime  0.175 ( 0.176)\tLoss 8.6819e-03 (7.7425e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [145][120/391]\tTime  0.175 ( 0.176)\tLoss 5.2211e-03 (7.5669e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [145][150/391]\tTime  0.177 ( 0.176)\tLoss 6.0300e-03 (7.4597e-03)\tAcc@1 100.00 ( 99.96)\tAcc@5 100.00 (100.00)\n","Epoch: [145][180/391]\tTime  0.176 ( 0.176)\tLoss 5.7368e-03 (7.3507e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [145][210/391]\tTime  0.174 ( 0.176)\tLoss 9.6917e-03 (7.4328e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [145][240/391]\tTime  0.174 ( 0.176)\tLoss 1.0459e-02 (7.3809e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [145][270/391]\tTime  0.174 ( 0.176)\tLoss 2.3569e-02 (7.4120e-03)\tAcc@1  99.22 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [145][300/391]\tTime  0.176 ( 0.176)\tLoss 7.3966e-03 (7.4420e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [145][330/391]\tTime  0.176 ( 0.176)\tLoss 9.2580e-03 (7.4620e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [145][360/391]\tTime  0.174 ( 0.176)\tLoss 6.6257e-03 (7.4256e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","Epoch: [145][390/391]\tTime  0.158 ( 0.175)\tLoss 7.3898e-03 (7.3600e-03)\tAcc@1 100.00 ( 99.97)\tAcc@5 100.00 (100.00)\n","==> Train Accuracy: Acc@1 99.974 || Acc@5 100.000\n","==> Test Accuracy:  Acc@1 77.340 || Acc@5 93.990\n","==> 72.97 seconds to train this epoch\n","\n","\n","----- epoch: 146, lr: 0.0008000000000000003 -----\n","Epoch: [146][  0/391]\tTime  0.230 ( 0.230)\tLoss 6.1067e-03 (6.1067e-03)\tAcc@1 100.00 (100.00)\tAcc@5 100.00 (100.00)\n"],"name":"stdout"}]}]}